{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "510d3232",
        "eee98349",
        "f7719f7c",
        "de59669d",
        "b4461951",
        "bf9f79c3",
        "6e408a53",
        "ikS2_ccU5RG0",
        "b36c67a2",
        "9c2f2a1a",
        "d7e3249e",
        "Ac1QcYIbdUAu",
        "jdE2ZbYedna7",
        "xQ8MfSYylxDX",
        "YUTiYchBx_Ix",
        "Eyqq404T03F2",
        "IC2cmvzS24cn",
        "LsQMWfRiK0oZ",
        "Ft-2gW5_Tk_c"
      ],
      "authorship_tag": "ABX9TyPoQnUfUwX1GgqY1w7uDqBS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KunalSachdev2005/Data_Science_Intern_at_Info_Origin/blob/main/BBC_News_Articles_Classification_Goolge_nnlm_%26_Fine_Tuning_Hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510d3232"
      },
      "source": [
        "## Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "227c04f4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dkgxDx8CoHOt",
        "outputId": "91d75f8a-2784-4f8c-b67a-b8f8a966b16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.5.0-py3-none-any.whl (28 kB)\n",
            "Collecting colorama<0.5.0,>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (3.5.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.5.0 colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eee98349"
      },
      "source": [
        "## Loading Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Source - https://www.kaggle.com/datasets/moazeldsokyx/bbc-news/data"
      ],
      "metadata": {
        "id": "Kb8MNdN-1tOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # mounting notebook on google drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Sbsk3W9qdy",
        "outputId": "50dbe56b-d296-48b1-8132-03eef5d2a310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/BBC_News_Articles_Classification_with_RoBERTa/bbc-text.csv')"
      ],
      "metadata": {
        "id": "h3_B-881-AyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "c9RNwHs6-KcL",
        "outputId": "ba51f5b4-a70f-4ec5-9131-139c10027245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        category                                               text\n",
              "0           tech  tv future in the hands of viewers with home th...\n",
              "1       business  worldcom boss  left books alone  former worldc...\n",
              "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3          sport  yeading face newcastle in fa cup premiership s...\n",
              "4  entertainment  ocean s twelve raids box office ocean s twelve..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec163e0e-8561-4689-aab0-b64d8b012e0c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec163e0e-8561-4689-aab0-b64d8b012e0c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec163e0e-8561-4689-aab0-b64d8b012e0c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec163e0e-8561-4689-aab0-b64d8b012e0c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d1d900d7-de6f-464c-8a0b-a6c1894b79f9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d1d900d7-de6f-464c-8a0b-a6c1894b79f9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d1d900d7-de6f-464c-8a0b-a6c1894b79f9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2225,\n  \"fields\": [\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"business\",\n          \"politics\",\n          \"sport\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2126,\n        \"samples\": [\n          \"plan to give elderly care control elderly and disabled people would choose how their own budget for personal care was spent and organised under government plans.  ministers say elderly and disabled people themselves  not social workers  should be able to decide on their care and stay in their own homes. they also plan a supremo for adult services in each english area to get different agencies working together. but the government shunned opponents  calls for free long-term care.  there are 1.7m people needing care in england and ministers suggest the number could quadruple by 2050. monday s consultation paper on social care for adults in england is aimed at ending a system which generates dependency. health minister stephen ladyman said:  this document is the antithesis of the nanny state.   it s about taking power away from the state and giving it to individuals and saying that we will help you make these decisions but we are not going to make them for you any more.  the government has already allowed local councils to give people money so they can pay for their services directly but take-up of the scheme has been  disappointing .  ministers say the new plans would make direct payments simpler and try to counter reluctance in some local councils to use the payments. they also want to set up a new  half-way house  where social workers tell people how much money is available for their care and help them choose how to spend that  individual budget . the scheme will be funded on existing budgets set until 2008. but mr ladyman said the plans could deliver savings in some areas  such as freeing up nhs beds and preventing illnesses. he ruled out free personal care in england - which is on offer in scotland and wales  saying it was  unsustainable .  david rogers  from the local government association  said agencies were working together on the kind of innovation proposed by the government. and tony hunter  president of the association of directors of social services  said the plans could improve dignity and well-being for thousands of people. but age concern argued social care was chronically under-funded and older people were being offered choice in principle  but not in practice. its director general  gordon lishman  said:  direct payments will not work if there are no services for people to choose from locally.   the tories say people who pay for three years  long-term care directly or through insurance should be guaranteed free care for the rest of their lives. tory spokesman simon burns said more than 80 000 long term care places had been lost since 1997.  after eight years of persistent change  dogmatic enforcement of regulation  and overbearing government initiatives - we need action  not a vision   said mr burns. the lib dems say they would fund free personal care by a new 50% tax rate on incomes over \\u00a3100 000. health spokesman paul burstow said:  promoting independence sounds good and helping people to live in their own homes is a goal we share.  but the risk is that independence can turn into isolation if the right support and care is not available.\",\n          \"beer giant swallows russian firm brewing giant inbev has agreed to buy alfa-eco s stake in sun interbrew  russia s second-largest brewer  for up to 259.7m euros ($353.3m; \\u00a3183.75m).  alfa-eco  the venture capital arm of russian conglomerate alfa group  has a one-fifth stake in sun interbrew. the deal gives inbev  the world s biggest beermaker  near-total control over the russian brewer. inbev bought out another partner in august 2004. inbev brands include bass  stella artois  hoegaarden and staropramen. it employs 77 000 people  running operations in over 30 countries across the americas  europe and asia pacific.  the leuven-based brewery said it would own 97.3% of the voting shares and 98.8% of the non-voting shares of sun interbrew. the deal is expected to be completed in the first quarter of 2005. inbev was formed in august 2004 when belgium s interbrew bought brazilian brewer ambev. sun interbrew  which employs 8 000 staff  owns breweries in eight russian cities - klin  ivanovo  saransk  kursk  volzhsky  omsk  perm and novocheboksarsk. there are also three breweries in ukraine  in the cities of chernigov  nikolaev and kharkov.\",\n          \"athens memories soar above lows well  it s goodbye to another olympic year and as usual there were plenty of highs and lows in athens.  obviously  there s no getting away from the differing fortunes of kelly holmes and paula radcliffe. but i want to remind you of a few more events that made 2004 another year to remember - or forget - for athletics.      one of my favourite olympic moments was kelly s success in the 800m.  winning that race was the key to her success because if she won that then the 1500m would be a bit of a formality. kelly had been full of  should i  shouldn t i   thoughts about going for the double in athens. i thought why wouldn t you do the 800m  it s your best event  it was such good fun to commentate on her 1500m and it was nice to be able to be part of her athens story.      the victory for the british men s 4x100m relay team was a bit of a surprise but a great climax to the games. i think the four of them - jason gardener  darren campbell  marlon devonish and mark lewis-francis - knew deep down that it was their best chance of a medal. the lads had run poorly in the individual sprints so maybe they did lift their game when they knew something was really at stake.      hicham el guerrouj s olympic double is a much bigger achievement than kelly s on a global scale.  he was the first man since for 80 years to win both the 1500m and 5 000m titles. as soon as he had added the 5 000m crown and i had finished commentating  i jumped up  ran down the stairs  pushed everyone out the way and just gave him a big hug. he is one of the few african runners who has embraced the tradition of the mile and he loves to hear all the roger bannister stories. hicham is someone i enjoy having a bit of time with  even though my french and his english are not very good.      what happened to paula in athens this year is the obvious low on a personal level and for the expectations of the nation as well. there were a set of circumstances around athens that conspired to produce a very dramatic ending which i think has been greatly misunderstood. dropping out of the marathon was the right thing to do but starting in the 10 000m five days later was not wise. that was her heart and not her head reacting. paula had a lot of little things going wrong in her preparation and on the day.  things like niggling injuries  not being able to do all her running sessions and feeling the pressure of the race looming ahead of her. i think she came to the start line in athens physically and emotionally drained. and if even the smallest thing doesn t feel right when you are preparing to race a marathon  10 miles down the road it will hit you like a brick wall. the positive thing to take from paula s olympics it that she will have learned a lot from it and so will a lot of people - including me.      purely as a race  paula s victory in the new york marathon has to go down as one of the most thrilling. it was so nip-and-tuck between her and kenya s susan chepkemei and you don t usually get that kind of excitement in marathons. it was also a real delight for all athletics fans because  to use one of my favourite words  paula showed real  bouncebackability . and it was a bit of a rarity for me too because i genuinely did not have an inkling how the race was going to pan out.      kelly and the 4x100m boys  victories papered over the cracks in the general performance of the british team. we should be concerned that we re not producing enough people who are capable of reaching finals at senior level.  the only individual men s finalist on the track was michael east in the 1500m. i am beginning to look down and wonder where are the new breed  and that s where things begin to look even gloomier for british athletics as we did not win any medals at the world junior championships in italy. dani barnes came fourth in the 1500m and she was the highest finisher for team gb. the thing is if we don t have athletes getting into the finals at junior level then it really doesn t look good for the beijing olympics and beyond.      i tell you what i really enjoyed this year  benita johnson winning the world cross country championships back in march. in the absence of paula  we tend to think of the event as something of an african preserve. so to have an australian come up and deliver such a surprise was something special.      to be honest  i m getting bored with all the drug scandals  especially balco. i just wish the whole thing would come to a head so we can move on.  having said that  i m always pleased when drugs cheats are caught because it shows the sport is standing up to it and not turning a blind eye anymore. and one of the positive things to come out of balco is people are starting to blow the whistle. we need more people to come forward and help the authorities kick out the cheats. as regards the case against greek sprinters kostas kenteris and katerina thanou  well suspicions have been hanging over kenteris for a while. the bottom line is we cannot keep letting drugs damage the sport because if we do then it stops everyone enjoying it.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "242a5cec",
        "outputId": "0f94fadc-4be6-430c-ccc9-d12298abeabd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category\n",
              "sport            511\n",
              "business         510\n",
              "politics         417\n",
              "tech             401\n",
              "entertainment    386\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# class distribution of bbc news articles\n",
        "\n",
        "df['category'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7719f7c"
      },
      "source": [
        "## Encode Labels, Train_Test_Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de59669d"
      },
      "source": [
        "### Encode Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b54dc0f"
      },
      "outputs": [],
      "source": [
        "# converting news categories to numerical values\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['encoded_labels'] = label_encoder.fit_transform(df['category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "95ee0136",
        "outputId": "089b7902-4f75-4ee7-e948-595db2f2310e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        category                                               text  \\\n",
              "0           tech  tv future in the hands of viewers with home th...   \n",
              "1       business  worldcom boss  left books alone  former worldc...   \n",
              "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
              "3          sport  yeading face newcastle in fa cup premiership s...   \n",
              "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
              "\n",
              "   encoded_labels  \n",
              "0               4  \n",
              "1               0  \n",
              "2               3  \n",
              "3               3  \n",
              "4               1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-024558ce-e083-4ebc-b008-83344b87c28e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>encoded_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-024558ce-e083-4ebc-b008-83344b87c28e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-024558ce-e083-4ebc-b008-83344b87c28e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-024558ce-e083-4ebc-b008-83344b87c28e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-76527a20-f1e9-4482-878e-f1114eae259b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-76527a20-f1e9-4482-878e-f1114eae259b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-76527a20-f1e9-4482-878e-f1114eae259b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2225,\n  \"fields\": [\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"business\",\n          \"politics\",\n          \"sport\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2126,\n        \"samples\": [\n          \"plan to give elderly care control elderly and disabled people would choose how their own budget for personal care was spent and organised under government plans.  ministers say elderly and disabled people themselves  not social workers  should be able to decide on their care and stay in their own homes. they also plan a supremo for adult services in each english area to get different agencies working together. but the government shunned opponents  calls for free long-term care.  there are 1.7m people needing care in england and ministers suggest the number could quadruple by 2050. monday s consultation paper on social care for adults in england is aimed at ending a system which generates dependency. health minister stephen ladyman said:  this document is the antithesis of the nanny state.   it s about taking power away from the state and giving it to individuals and saying that we will help you make these decisions but we are not going to make them for you any more.  the government has already allowed local councils to give people money so they can pay for their services directly but take-up of the scheme has been  disappointing .  ministers say the new plans would make direct payments simpler and try to counter reluctance in some local councils to use the payments. they also want to set up a new  half-way house  where social workers tell people how much money is available for their care and help them choose how to spend that  individual budget . the scheme will be funded on existing budgets set until 2008. but mr ladyman said the plans could deliver savings in some areas  such as freeing up nhs beds and preventing illnesses. he ruled out free personal care in england - which is on offer in scotland and wales  saying it was  unsustainable .  david rogers  from the local government association  said agencies were working together on the kind of innovation proposed by the government. and tony hunter  president of the association of directors of social services  said the plans could improve dignity and well-being for thousands of people. but age concern argued social care was chronically under-funded and older people were being offered choice in principle  but not in practice. its director general  gordon lishman  said:  direct payments will not work if there are no services for people to choose from locally.   the tories say people who pay for three years  long-term care directly or through insurance should be guaranteed free care for the rest of their lives. tory spokesman simon burns said more than 80 000 long term care places had been lost since 1997.  after eight years of persistent change  dogmatic enforcement of regulation  and overbearing government initiatives - we need action  not a vision   said mr burns. the lib dems say they would fund free personal care by a new 50% tax rate on incomes over \\u00a3100 000. health spokesman paul burstow said:  promoting independence sounds good and helping people to live in their own homes is a goal we share.  but the risk is that independence can turn into isolation if the right support and care is not available.\",\n          \"beer giant swallows russian firm brewing giant inbev has agreed to buy alfa-eco s stake in sun interbrew  russia s second-largest brewer  for up to 259.7m euros ($353.3m; \\u00a3183.75m).  alfa-eco  the venture capital arm of russian conglomerate alfa group  has a one-fifth stake in sun interbrew. the deal gives inbev  the world s biggest beermaker  near-total control over the russian brewer. inbev bought out another partner in august 2004. inbev brands include bass  stella artois  hoegaarden and staropramen. it employs 77 000 people  running operations in over 30 countries across the americas  europe and asia pacific.  the leuven-based brewery said it would own 97.3% of the voting shares and 98.8% of the non-voting shares of sun interbrew. the deal is expected to be completed in the first quarter of 2005. inbev was formed in august 2004 when belgium s interbrew bought brazilian brewer ambev. sun interbrew  which employs 8 000 staff  owns breweries in eight russian cities - klin  ivanovo  saransk  kursk  volzhsky  omsk  perm and novocheboksarsk. there are also three breweries in ukraine  in the cities of chernigov  nikolaev and kharkov.\",\n          \"athens memories soar above lows well  it s goodbye to another olympic year and as usual there were plenty of highs and lows in athens.  obviously  there s no getting away from the differing fortunes of kelly holmes and paula radcliffe. but i want to remind you of a few more events that made 2004 another year to remember - or forget - for athletics.      one of my favourite olympic moments was kelly s success in the 800m.  winning that race was the key to her success because if she won that then the 1500m would be a bit of a formality. kelly had been full of  should i  shouldn t i   thoughts about going for the double in athens. i thought why wouldn t you do the 800m  it s your best event  it was such good fun to commentate on her 1500m and it was nice to be able to be part of her athens story.      the victory for the british men s 4x100m relay team was a bit of a surprise but a great climax to the games. i think the four of them - jason gardener  darren campbell  marlon devonish and mark lewis-francis - knew deep down that it was their best chance of a medal. the lads had run poorly in the individual sprints so maybe they did lift their game when they knew something was really at stake.      hicham el guerrouj s olympic double is a much bigger achievement than kelly s on a global scale.  he was the first man since for 80 years to win both the 1500m and 5 000m titles. as soon as he had added the 5 000m crown and i had finished commentating  i jumped up  ran down the stairs  pushed everyone out the way and just gave him a big hug. he is one of the few african runners who has embraced the tradition of the mile and he loves to hear all the roger bannister stories. hicham is someone i enjoy having a bit of time with  even though my french and his english are not very good.      what happened to paula in athens this year is the obvious low on a personal level and for the expectations of the nation as well. there were a set of circumstances around athens that conspired to produce a very dramatic ending which i think has been greatly misunderstood. dropping out of the marathon was the right thing to do but starting in the 10 000m five days later was not wise. that was her heart and not her head reacting. paula had a lot of little things going wrong in her preparation and on the day.  things like niggling injuries  not being able to do all her running sessions and feeling the pressure of the race looming ahead of her. i think she came to the start line in athens physically and emotionally drained. and if even the smallest thing doesn t feel right when you are preparing to race a marathon  10 miles down the road it will hit you like a brick wall. the positive thing to take from paula s olympics it that she will have learned a lot from it and so will a lot of people - including me.      purely as a race  paula s victory in the new york marathon has to go down as one of the most thrilling. it was so nip-and-tuck between her and kenya s susan chepkemei and you don t usually get that kind of excitement in marathons. it was also a real delight for all athletics fans because  to use one of my favourite words  paula showed real  bouncebackability . and it was a bit of a rarity for me too because i genuinely did not have an inkling how the race was going to pan out.      kelly and the 4x100m boys  victories papered over the cracks in the general performance of the british team. we should be concerned that we re not producing enough people who are capable of reaching finals at senior level.  the only individual men s finalist on the track was michael east in the 1500m. i am beginning to look down and wonder where are the new breed  and that s where things begin to look even gloomier for british athletics as we did not win any medals at the world junior championships in italy. dani barnes came fourth in the 1500m and she was the highest finisher for team gb. the thing is if we don t have athletes getting into the finals at junior level then it really doesn t look good for the beijing olympics and beyond.      i tell you what i really enjoyed this year  benita johnson winning the world cross country championships back in march. in the absence of paula  we tend to think of the event as something of an african preserve. so to have an australian come up and deliver such a surprise was something special.      to be honest  i m getting bored with all the drug scandals  especially balco. i just wish the whole thing would come to a head so we can move on.  having said that  i m always pleased when drugs cheats are caught because it shows the sport is standing up to it and not turning a blind eye anymore. and one of the positive things to come out of balco is people are starting to blow the whistle. we need more people to come forward and help the authorities kick out the cheats. as regards the case against greek sprinters kostas kenteris and katerina thanou  well suspicions have been hanging over kenteris for a while. the bottom line is we cannot keep letting drugs damage the sport because if we do then it stops everyone enjoying it.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"encoded_labels\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44f10c6c",
        "outputId": "84f421f5-4d59-45d9-d516-8d5ffe806aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories: ['business' 'entertainment' 'politics' 'sport' 'tech']\n"
          ]
        }
      ],
      "source": [
        "print(\"Categories:\", label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4461951"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac105c74"
      },
      "outputs": [],
      "source": [
        "# Splitting the data into training & testing sets: 80-20 respectively\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['encoded_labels'], test_size = 0.2,\n",
        "                                                    random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf9f79c3"
      },
      "source": [
        "## Text Embeddings, Defining PyTorch Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Google's NNLM model for text embeddings from TensorFlow Hub\n",
        "# The embedding model will convert each news article into a vector of 128 dimensions\n",
        "\n",
        "embedding_model = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128/2\")"
      ],
      "metadata": {
        "id": "9oFjN7Sft4aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text(texts):\n",
        "    return embedding_model(texts).numpy()\n",
        "\n",
        "# Example: embedding a list of texts\n",
        "sample_texts = [\"This is a sample text.\", \"Another example text.\"]\n",
        "sample_embeddings = embed_text(sample_texts)\n",
        "print(sample_embeddings.shape)  # Should output (2, 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N85ILFHtuPyx",
        "outputId": "9be6e621-8b01-4e99-aafb-5d74c477f183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vL8kV8ISsnQA",
        "outputId": "45d074de-eb04-4717-a7bc-18701608b7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.17659247, -0.04193331, -0.0037256 ,  0.13469452, -0.04602445,\n",
              "        -0.12868825, -0.08918834,  0.0492555 ,  0.0809811 ,  0.06132634,\n",
              "        -0.00908461, -0.2556114 , -0.1531524 ,  0.06502627, -0.07846863,\n",
              "        -0.0997012 , -0.00760367, -0.11002098, -0.11830123,  0.23778622,\n",
              "         0.02041573,  0.04113508,  0.12891719, -0.1450891 ,  0.0311806 ,\n",
              "         0.11851817, -0.08572798, -0.13035978, -0.03883693,  0.03471064,\n",
              "        -0.07909673, -0.09681756, -0.03693242,  0.14675713,  0.11979764,\n",
              "         0.05986909, -0.17420802, -0.06956328, -0.04668118, -0.09656622,\n",
              "        -0.0613942 , -0.04804664, -0.03568114, -0.10295977, -0.01576998,\n",
              "        -0.07141721, -0.08013295, -0.22305976, -0.0662103 , -0.0155965 ,\n",
              "        -0.04948296, -0.0937605 , -0.10211378,  0.05183063,  0.03943651,\n",
              "         0.01829274,  0.18880837, -0.01140689,  0.02464673, -0.1042655 ,\n",
              "         0.07531073, -0.09366877,  0.03305195,  0.0300911 ,  0.04872525,\n",
              "        -0.09421663,  0.022035  , -0.200644  ,  0.07449422,  0.14987463,\n",
              "         0.10846677, -0.02149267, -0.13197733, -0.25255528,  0.04166801,\n",
              "        -0.08103456, -0.14834367, -0.11855887, -0.02176448,  0.00158472,\n",
              "        -0.07600064,  0.09278991, -0.133456  , -0.09985868, -0.00358644,\n",
              "         0.13731463,  0.05703225,  0.02087064,  0.15086961,  0.14009938,\n",
              "        -0.015642  , -0.13155791,  0.05045215, -0.16475743, -0.09466076,\n",
              "         0.02036789, -0.031654  ,  0.0695172 , -0.03915256,  0.02301325,\n",
              "         0.03450361,  0.03523849, -0.06981788,  0.10928387,  0.03349016,\n",
              "         0.02946332, -0.16803879,  0.04460763, -0.06068563, -0.01122629,\n",
              "        -0.0202169 ,  0.03090765,  0.04403535, -0.01951889,  0.13445807,\n",
              "        -0.06297068, -0.00859815, -0.08450618,  0.04522959,  0.0080579 ,\n",
              "         0.01194727, -0.05313307, -0.00850712, -0.19769879, -0.04385172,\n",
              "         0.09090956,  0.02288605, -0.11083546],\n",
              "       [ 0.04850918, -0.08894073,  0.12454566,  0.00903469, -0.05269296,\n",
              "        -0.10605928,  0.06314846,  0.02912748,  0.10139645,  0.0145708 ,\n",
              "         0.06111839, -0.1350334 , -0.14828664,  0.07333837,  0.02448969,\n",
              "        -0.02904256, -0.18329273, -0.20083381,  0.01773142,  0.13984892,\n",
              "        -0.02715302, -0.07147812,  0.21626331, -0.16616268, -0.00530566,\n",
              "         0.0382948 , -0.15843076, -0.15033446,  0.0715511 ,  0.06690284,\n",
              "        -0.00187246, -0.05476566, -0.03261297,  0.01361276,  0.10251402,\n",
              "        -0.00173365, -0.16718873, -0.01260774, -0.01061041, -0.07883717,\n",
              "        -0.01785343, -0.0215304 ,  0.0312744 , -0.16530457, -0.15471001,\n",
              "        -0.14742309, -0.09123255, -0.1944541 , -0.16149051,  0.05995926,\n",
              "        -0.04258087, -0.13187854, -0.05695201, -0.06496836, -0.05041596,\n",
              "         0.02970529,  0.05195773,  0.01842512,  0.07061294, -0.10675638,\n",
              "         0.14492482, -0.02835457,  0.05233018, -0.04388488,  0.14694719,\n",
              "        -0.01651018,  0.0032618 , -0.04932599,  0.08572464,  0.1239419 ,\n",
              "         0.10482621,  0.00735864, -0.22647442, -0.074681  ,  0.01459165,\n",
              "         0.0034688 , -0.21300322, -0.22596633, -0.05149909,  0.07012887,\n",
              "         0.05161146,  0.03580983, -0.025322  , -0.23623964,  0.02880932,\n",
              "         0.1209349 ,  0.12384982,  0.1507533 , -0.08150952,  0.08188976,\n",
              "         0.14171149, -0.20459883,  0.0819529 , -0.0711287 , -0.1016211 ,\n",
              "        -0.07304139, -0.12934783, -0.09145928, -0.03754443, -0.03260307,\n",
              "         0.0525603 , -0.098433  , -0.10362957,  0.00491092, -0.02164995,\n",
              "        -0.03274795, -0.05293576,  0.17380644,  0.04720854, -0.09468248,\n",
              "        -0.08079156,  0.16541396, -0.13368557, -0.12790586, -0.08257448,\n",
              "        -0.0511529 , -0.04452455,  0.03125302,  0.01898028, -0.0059138 ,\n",
              "         0.04336157,  0.05731915,  0.10351605, -0.08899395, -0.02508098,\n",
              "        -0.01849375,  0.02867511, -0.02265471]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to convert list of news articles to their embeddings in batches\n",
        "\n",
        "def texts_to_embeddings(texts):\n",
        "    embeddings = []\n",
        "    batch_size = 64\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        batch_embeddings = embed_text(batch_texts)\n",
        "        embeddings.append(batch_embeddings)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "    # embed_text fn return an array of embeddings for a batch of news articles\n",
        "    # 'embeddings' is a list where each element is an array of embeddings for a batch of texts\n",
        "    # np.vstack stacks arrays in a sequence vertically (row-wise)\n",
        "    # np.vstack will take a list of batch arrays and stack them vertically to create a single array"
      ],
      "metadata": {
        "id": "klMyhw62uk0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_embeddings = texts_to_embeddings(X_train) # converting training and testing data to embeddings\n",
        "X_test_embeddings = texts_to_embeddings(X_test)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_embeddings, dtype=torch.float32) # Converting the embeddings to PyTorch tensors\n",
        "X_test_tensor = torch.tensor(X_test_embeddings, dtype=torch.float32)\n",
        "\n",
        "y_train = y_train.to_numpy() # Converting train & test labels to numpy arrays and then to PyTorch tensors\n",
        "y_test = y_test.to_numpy()\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
      ],
      "metadata": {
        "id": "Ty61SbGNvcu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definign training and testing TensorDatasets and respective Data Loaders\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 64 # Batch size for data loader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
      ],
      "metadata": {
        "id": "F0iTMO9Gz7Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "544bdc05",
        "outputId": "43f52781-067e-4ae9-afb7-98b2ba79f35f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "Sequences:\n",
            "tensor([[ 2.2977e+00,  6.1617e-01,  1.5332e-01,  ..., -2.9193e-01,\n",
            "          1.9858e-01,  1.7425e-01],\n",
            "        [ 1.2482e+00, -6.2437e-02,  3.4795e-02,  ..., -2.0527e-02,\n",
            "          1.3849e-01, -2.9890e-01],\n",
            "        [ 1.8288e+00, -1.3499e-02,  5.7941e-02,  ...,  1.8327e-01,\n",
            "          1.5872e-01, -2.2271e-01],\n",
            "        ...,\n",
            "        [ 2.4075e+00,  2.0290e-01,  2.8984e-01,  ..., -1.1183e-01,\n",
            "         -1.2984e-01, -3.6770e-02],\n",
            "        [ 1.3605e+00,  1.5177e-03,  3.8511e-01,  ...,  1.4066e-01,\n",
            "          1.9235e-02, -7.4891e-02],\n",
            "        [ 2.5173e+00,  1.7301e-01,  2.6774e-01,  ..., -1.8476e-01,\n",
            "          9.4326e-02, -1.8494e-01]])\n",
            "<class 'torch.Tensor'>\n",
            "Labels:\n",
            "tensor([2, 1, 3, 1, 3, 1, 4, 0, 0, 3, 3, 3, 0, 3, 0, 2, 2, 0, 3, 0, 3, 4, 2, 3,\n",
            "        3, 1, 3, 0, 2, 0, 3, 3, 3, 3, 0, 1, 2, 3, 0, 1, 1, 3, 3, 0, 2, 4, 0, 1,\n",
            "        2, 4, 3, 0, 0, 0, 3, 0, 1, 4, 2, 4, 0, 4, 0, 2])\n",
            "Shape of Sequences: torch.Size([64, 128])\n",
            "Shape of Labels: torch.Size([64])\n",
            "Dimensions of Sequences: 2\n",
            "Dimensions of Labels: 1\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "# Iterate over train_loader to see its contents\n",
        "# Each batch has 64 news article embeddings: (64, 128)\n",
        "for batch_idx, (sequences, labels) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(\"Sequences:\")\n",
        "    print(sequences)\n",
        "    print(type(sequences))\n",
        "    print(\"Labels:\")\n",
        "    print(labels)\n",
        "    print(\"Shape of Sequences:\", sequences.shape)\n",
        "    print(\"Shape of Labels:\", labels.shape)\n",
        "    print(\"Dimensions of Sequences:\", sequences.dim())\n",
        "    print(\"Dimensions of Labels:\", labels.dim())\n",
        "    print(\"-------------------\")\n",
        "\n",
        "    # Only print the first batch to just see the contents\n",
        "    if batch_idx == 0:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e408a53"
      },
      "source": [
        "## Defining Neural Network, Training, Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Model"
      ],
      "metadata": {
        "id": "ikS2_ccU5RG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining neural network architecture and forward pass\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, output_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim1) # First hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2) # Second hidden layer\n",
        "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3) # Thrid hidden layer\n",
        "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4) # Fourth hidden layer\n",
        "        self.fc_output = nn.Linear(hidden_dim4, output_dim) # Output Layer\n",
        "        self.dropout = nn.Dropout(0.3) # Dropout layer for regularization\n",
        "        self.relu = nn.ReLU() # ReLU activation function\n",
        "\n",
        "        #self.sigmoid = nn.Sigmoid() # Commented out because we are Using cross entropy loss which already uses softmax\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "\n",
        "        # input: (batch size, embedding dimension): (64, 128)\n",
        "        # hidden_dim1 = 128\n",
        "        seq = self.fc1(embeddings) # (64, 128) -> (64, 128)\n",
        "        seq = self.relu(seq) # Same shape\n",
        "        seq = self.dropout(seq) # Same shape\n",
        "\n",
        "        # hidden_dim2 = 128\n",
        "        seq = self.fc2(seq) # (64, 128) -> (64, 128)\n",
        "        seq = self.relu(seq)\n",
        "        seq = self.dropout(seq)\n",
        "\n",
        "        # hidden_dim3 = 64\n",
        "        seq = self.fc3(seq) # (64, 128) -> (64, 64)\n",
        "        seq = self.relu(seq)\n",
        "        seq = self.dropout(seq)\n",
        "\n",
        "        # hidden_dim4 = 32\n",
        "        seq = self.fc4(seq) # (64, 64) -> (64, 32)\n",
        "        seq = self.relu(seq)\n",
        "        seq = self.dropout(seq)\n",
        "\n",
        "        # output_dim = 5\n",
        "        output = self.fc_output(seq) # (64, 32) -> (64, 5)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "hLDTULsD1JMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92cfc135"
      },
      "outputs": [],
      "source": [
        "# Defining Hyperparameters\n",
        "embedding_dim = 128\n",
        "hidden_dim1 = 128\n",
        "hidden_dim2 = 128\n",
        "hidden_dim3 = 64\n",
        "hidden_dim4 = 32\n",
        "output_dim = 5\n",
        "\n",
        "# Initializing model\n",
        "model = SimpleNN(embedding_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbf4kJmb478K",
        "outputId": "406fa3fa-44e4-481d-ead5-0804a858cd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleNN(\n",
            "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc_output): Linear(in_features=32, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b36c67a2"
      },
      "source": [
        "### Defining Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40107679"
      },
      "outputs": [],
      "source": [
        "loss_type = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bef42df",
        "outputId": "7616cf9e-9b5b-4120-9509-c117662846ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter name: fc1.weight\n",
            "Parameter containing:\n",
            "tensor([[ 0.0881, -0.0519,  0.0560,  ..., -0.0172,  0.0794, -0.0696],\n",
            "        [-0.0347, -0.0225, -0.0066,  ..., -0.0079, -0.0146, -0.0036],\n",
            "        [-0.0195, -0.0451, -0.0302,  ..., -0.0008, -0.0811, -0.0315],\n",
            "        ...,\n",
            "        [ 0.0768,  0.0599, -0.0265,  ..., -0.0575,  0.0106,  0.0112],\n",
            "        [ 0.0368,  0.0060, -0.0127,  ..., -0.0751, -0.0474,  0.0134],\n",
            "        [-0.0453, -0.0601,  0.0146,  ...,  0.0255, -0.0695,  0.0162]],\n",
            "       requires_grad=True)\n",
            "Parameter name: fc1.bias\n",
            "Parameter containing:\n",
            "tensor([ 0.0526,  0.0442, -0.0535, -0.0530,  0.0763,  0.0009,  0.0159,  0.0305,\n",
            "        -0.0005, -0.0770,  0.0499,  0.0775, -0.0300, -0.0862,  0.0778, -0.0490,\n",
            "        -0.0059,  0.0057,  0.0165,  0.0222, -0.0394,  0.0526, -0.0282, -0.0824,\n",
            "        -0.0837,  0.0047, -0.0148,  0.0717,  0.0737,  0.0151, -0.0508,  0.0693,\n",
            "        -0.0387,  0.0634,  0.0486,  0.0524, -0.0381,  0.0351, -0.0776,  0.0060,\n",
            "        -0.0231,  0.0824, -0.0804,  0.0217, -0.0684, -0.0837, -0.0117, -0.0644,\n",
            "         0.0064,  0.0725, -0.0207, -0.0813, -0.0632,  0.0385,  0.0609,  0.0239,\n",
            "         0.0643,  0.0593, -0.0360,  0.0129,  0.0514, -0.0739, -0.0805,  0.0302,\n",
            "         0.0865, -0.0395, -0.0279, -0.0422, -0.0799,  0.0879,  0.0807, -0.0744,\n",
            "         0.0395, -0.0716, -0.0344,  0.0065,  0.0334,  0.0103, -0.0818,  0.0717,\n",
            "        -0.0689, -0.0637,  0.0018, -0.0197,  0.0130, -0.0071,  0.0051, -0.0012,\n",
            "         0.0869, -0.0112, -0.0380,  0.0106, -0.0278,  0.0610,  0.0818,  0.0634,\n",
            "        -0.0770,  0.0517,  0.0418,  0.0676, -0.0421, -0.0661,  0.0167, -0.0696,\n",
            "        -0.0615,  0.0034, -0.0259, -0.0623,  0.0852, -0.0052,  0.0365,  0.0611,\n",
            "        -0.0419,  0.0293,  0.0650,  0.0667,  0.0491,  0.0573,  0.0061, -0.0766,\n",
            "         0.0856, -0.0291,  0.0158,  0.0609,  0.0039, -0.0186,  0.0871,  0.0043],\n",
            "       requires_grad=True)\n",
            "Parameter name: fc2.weight\n",
            "Parameter containing:\n",
            "tensor([[-0.0005, -0.0111, -0.0595,  ...,  0.0457, -0.0848,  0.0796],\n",
            "        [-0.0241, -0.0368, -0.0698,  ..., -0.0201,  0.0334,  0.0384],\n",
            "        [-0.0486,  0.0600,  0.0767,  ...,  0.0443,  0.0433,  0.0186],\n",
            "        ...,\n",
            "        [-0.0865,  0.0837, -0.0818,  ...,  0.0603,  0.0573,  0.0380],\n",
            "        [ 0.0198, -0.0445, -0.0308,  ..., -0.0105, -0.0660, -0.0562],\n",
            "        [ 0.0006, -0.0035, -0.0400,  ..., -0.0653, -0.0394,  0.0764]],\n",
            "       requires_grad=True)\n",
            "Parameter name: fc2.bias\n",
            "Parameter containing:\n",
            "tensor([ 0.0496,  0.0875, -0.0580,  0.0629, -0.0846,  0.0648,  0.0008, -0.0427,\n",
            "         0.0234, -0.0522, -0.0004,  0.0820, -0.0363,  0.0240, -0.0233, -0.0124,\n",
            "        -0.0010, -0.0169,  0.0534,  0.0355,  0.0719,  0.0450, -0.0075,  0.0793,\n",
            "        -0.0725, -0.0357,  0.0365,  0.0439, -0.0849, -0.0621,  0.0086,  0.0094,\n",
            "        -0.0568, -0.0099,  0.0835, -0.0327, -0.0684,  0.0407,  0.0325, -0.0415,\n",
            "        -0.0455,  0.0006,  0.0802,  0.0325, -0.0134, -0.0677, -0.0713, -0.0838,\n",
            "        -0.0063,  0.0654,  0.0461,  0.0402,  0.0467, -0.0618, -0.0485, -0.0312,\n",
            "         0.0082,  0.0328,  0.0428, -0.0791,  0.0245, -0.0513,  0.0494, -0.0283,\n",
            "        -0.0291, -0.0771, -0.0305, -0.0160, -0.0079, -0.0633, -0.0100, -0.0172,\n",
            "         0.0872, -0.0334, -0.0744,  0.0260,  0.0327,  0.0048,  0.0416,  0.0613,\n",
            "        -0.0328, -0.0772, -0.0351, -0.0689, -0.0507, -0.0417, -0.0364,  0.0492,\n",
            "         0.0366, -0.0671, -0.0244, -0.0238,  0.0759,  0.0081,  0.0023, -0.0862,\n",
            "        -0.0219,  0.0237,  0.0246, -0.0228, -0.0470,  0.0371,  0.0181, -0.0876,\n",
            "        -0.0778, -0.0096,  0.0651, -0.0605, -0.0388, -0.0555,  0.0127,  0.0488,\n",
            "        -0.0130,  0.0712, -0.0776, -0.0026, -0.0860,  0.0303, -0.0609, -0.0130,\n",
            "        -0.0326,  0.0853,  0.0413,  0.0373,  0.0247, -0.0840,  0.0061,  0.0670],\n",
            "       requires_grad=True)\n",
            "Parameter name: fc3.weight\n",
            "Parameter containing:\n",
            "tensor([[-0.0445,  0.0655,  0.0660,  ..., -0.0018,  0.0553, -0.0153],\n",
            "        [ 0.0032, -0.0840, -0.0040,  ..., -0.0810, -0.0504,  0.0335],\n",
            "        [ 0.0137,  0.0453, -0.0245,  ..., -0.0449,  0.0647, -0.0693],\n",
            "        ...,\n",
            "        [ 0.0386,  0.0215, -0.0472,  ..., -0.0522,  0.0696, -0.0016],\n",
            "        [ 0.0014, -0.0419,  0.0848,  ..., -0.0285, -0.0645,  0.0246],\n",
            "        [ 0.0290, -0.0168, -0.0426,  ...,  0.0080, -0.0175, -0.0392]],\n",
            "       requires_grad=True)\n",
            "Parameter name: fc3.bias\n",
            "Parameter containing:\n",
            "tensor([-0.0043, -0.0767,  0.0288, -0.0483,  0.0151, -0.0495, -0.0418, -0.0362,\n",
            "         0.0431, -0.0202, -0.0164,  0.0117,  0.0176,  0.0540, -0.0384, -0.0052,\n",
            "         0.0100, -0.0440,  0.0423, -0.0715, -0.0798, -0.0395,  0.0097,  0.0018,\n",
            "        -0.0381, -0.0711, -0.0204, -0.0226,  0.0553,  0.0351, -0.0508,  0.0681,\n",
            "        -0.0601,  0.0611,  0.0696,  0.0514, -0.0820, -0.0637, -0.0411, -0.0494,\n",
            "         0.0564,  0.0089, -0.0465, -0.0247, -0.0493, -0.0390, -0.0507, -0.0476,\n",
            "        -0.0855,  0.0648, -0.0052,  0.0811, -0.0476,  0.0790, -0.0105,  0.0249,\n",
            "         0.0014, -0.0119,  0.0455, -0.0274, -0.0573, -0.0248, -0.0714,  0.0193],\n",
            "       requires_grad=True)\n",
            "Parameter name: fc4.weight\n",
            "Parameter containing:\n",
            "tensor([[-0.1233,  0.0303, -0.0598,  ...,  0.0014, -0.0816,  0.0541],\n",
            "        [-0.1209, -0.0444, -0.0251,  ...,  0.1005,  0.1054, -0.0264],\n",
            "        [-0.0095, -0.0671, -0.0771,  ...,  0.0555,  0.0325,  0.0338],\n",
            "        ...,\n",
            "        [-0.0987, -0.0222,  0.0900,  ...,  0.1143, -0.0665,  0.0934],\n",
            "        [-0.1073, -0.0968, -0.0801,  ...,  0.1158, -0.1166, -0.0399],\n",
            "        [-0.0841, -0.0320,  0.0472,  ...,  0.1002,  0.0818, -0.0563]],\n",
            "       requires_grad=True)\n",
            "Parameter name: fc4.bias\n",
            "Parameter containing:\n",
            "tensor([ 0.1150,  0.0071, -0.0490, -0.0915, -0.1130, -0.0550,  0.0514,  0.0232,\n",
            "        -0.1196, -0.0416,  0.1121,  0.0275, -0.0499, -0.0748, -0.0578, -0.0902,\n",
            "        -0.0175, -0.0710, -0.1009,  0.0869, -0.1120, -0.0192,  0.1224, -0.0638,\n",
            "         0.0460, -0.1217, -0.0519,  0.0776, -0.0844,  0.0397,  0.1159, -0.0613],\n",
            "       requires_grad=True)\n",
            "Parameter name: fc_output.weight\n",
            "Parameter containing:\n",
            "tensor([[-0.1009, -0.1074, -0.0551,  0.1450, -0.1045, -0.0642,  0.0368,  0.1511,\n",
            "          0.0093, -0.0382, -0.0496,  0.1311, -0.0138,  0.1019, -0.0547, -0.0273,\n",
            "         -0.0495, -0.0868, -0.0796, -0.1601,  0.1161,  0.1047,  0.0979,  0.0085,\n",
            "          0.1227, -0.1726, -0.1010, -0.0264,  0.0238, -0.0864,  0.1764, -0.0452],\n",
            "        [ 0.0908, -0.1387, -0.0841, -0.1215,  0.0950,  0.0691,  0.0690,  0.0713,\n",
            "          0.0992,  0.0202,  0.0956,  0.0462, -0.0498, -0.0243, -0.0053, -0.0306,\n",
            "         -0.1399,  0.0845, -0.0482, -0.0849, -0.1229,  0.0507, -0.1579,  0.1709,\n",
            "          0.0971,  0.0329, -0.0754, -0.0681, -0.0739, -0.1360,  0.1121, -0.1685],\n",
            "        [ 0.1067, -0.1669,  0.1562,  0.1236,  0.1119, -0.1027, -0.0285,  0.1231,\n",
            "         -0.0130,  0.0212, -0.0392, -0.1415, -0.0052,  0.1575, -0.0681, -0.0493,\n",
            "          0.0868, -0.0783,  0.1628,  0.0665,  0.1575,  0.0280,  0.0618, -0.0913,\n",
            "         -0.1646,  0.0368, -0.1250,  0.0238,  0.0075, -0.1591, -0.0923, -0.0230],\n",
            "        [-0.0289, -0.1732, -0.1615,  0.1300, -0.0128, -0.0570, -0.1070, -0.0684,\n",
            "         -0.1716, -0.0616, -0.0553,  0.0657, -0.0984, -0.1644,  0.0057,  0.1236,\n",
            "          0.0914, -0.0673,  0.0131,  0.1523, -0.0253, -0.1413,  0.0380,  0.1422,\n",
            "          0.0173, -0.0557, -0.1664, -0.0083, -0.1550,  0.0860,  0.1129, -0.0340],\n",
            "        [ 0.0123,  0.1365, -0.1718,  0.1155,  0.1289, -0.0130, -0.1578,  0.0141,\n",
            "          0.1516, -0.1046,  0.0544,  0.0592, -0.0420,  0.1673, -0.1228,  0.0868,\n",
            "          0.0434, -0.1020, -0.0596,  0.0657,  0.0451, -0.1028, -0.0037,  0.1167,\n",
            "         -0.0913,  0.1178,  0.1516, -0.1097, -0.1587, -0.1398,  0.0623,  0.1351]],\n",
            "       requires_grad=True)\n",
            "Parameter name: fc_output.bias\n",
            "Parameter containing:\n",
            "tensor([ 0.0645,  0.1168,  0.1434, -0.1347, -0.0740], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter name: {name}\")\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c2f2a1a"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea31254c"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, loss_type):\n",
        "    model.train() # training mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Loop through all batches of data in the loader\n",
        "    for sequences, labels in iterator:\n",
        "        optimizer.zero_grad() # clear the gradients of optimized params\n",
        "        predictions = model(sequences) # Forward Pass: get model predictions\n",
        "        loss = loss_type(predictions, labels) # calculate cross entropy loss\n",
        "        loss.backward() # Backpropagation (back pass): compute gradient of loss  with respect to model parameters\n",
        "        optimizer.step() # Update the model params with optimizer\n",
        "        epoch_loss += loss.item() # accumulate the loss over an epoch\n",
        "    return epoch_loss / len(iterator) # return average loss over an epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b5b195e",
        "outputId": "664dbd8b-2261-4357-ae95-ddd1c10e2db7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 1.5963\n",
            "Epoch: 2, Training Loss: 1.2454\n",
            "Epoch: 3, Training Loss: 0.6613\n",
            "Epoch: 4, Training Loss: 0.4162\n",
            "Epoch: 5, Training Loss: 0.2746\n",
            "Epoch: 6, Training Loss: 0.2205\n",
            "Epoch: 7, Training Loss: 0.1963\n",
            "Epoch: 8, Training Loss: 0.1657\n",
            "Epoch: 9, Training Loss: 0.1374\n",
            "Epoch: 10, Training Loss: 0.1308\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, loss_type) # train the model for one epoch; compute average training loss\n",
        "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7e3249e"
      },
      "source": [
        "### Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da253785"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, loss_type):\n",
        "    model.eval() # evaluation mode\n",
        "    epoch_loss = 0\n",
        "    all_preds = [] # predictions\n",
        "    all_labels = [] # true labels\n",
        "\n",
        "    with torch.no_grad(): # disable gradient computation because we are in evaluation mode\n",
        "        for sequences, labels in iterator:\n",
        "            predictions = model(sequences) # get model predictions\n",
        "            loss = loss_type(predictions, labels) # calculate loss\n",
        "            epoch_loss += loss.item() # accumulate epoch loss\n",
        "            _, predicted_labels = torch.max(predictions, 1) # get the predicted labels (index of maximum logit)\n",
        "            # torch.max will give maximum value along dimension 1 of `predictions` tensor\n",
        "            # returns tuple of two tensors - maximum value and corresponding index\n",
        "            # we only want the corresponsing index\n",
        "\n",
        "            all_preds.extend(predicted_labels.numpy()) # .numpy to convert from tensor to np array\n",
        "            all_labels.extend(labels.numpy())\n",
        "    return epoch_loss / len(iterator), all_preds, all_labels # return avg loss over an epoch, and all predictions & true labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "105b241d"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on test data\n",
        "test_loss, test_preds, test_labels = evaluate(model, test_loader, loss_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "807ce5aa",
        "outputId": "e2ed6000-81f1-418b-db24-7293d30258d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1519, Test Accuracy: 0.9528\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = np.mean(np.array(test_preds) == np.array(test_labels))\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = [\"\"\"Islam Makhachev made a third successful defense of his UFC lightweight championship on Saturday night, and his reward in the pound-for-pound rankings is ... nothing. Makhachev (26-1), who submitted Dustin Poirier in Round 5 of their UFC 302 main event, was already No. 1 in the ESPN rankings. There was nowhere for him to rise. However, Makhachev surely solidified his position at the top of the sport in the eyes of our voters and many fans, although not the person whose job it is to promote him. \"I don't think he's the pound-for-pound best fighter in the world,\" UFC CEO Dana White said after Saturday's fights. \"For anyone to call Islam the pound-for-pound best fighter in the world when Jon Jones is still f---ing fighting is nuts and shouldn't be ranking in the pound-for-pound or doing any of the f---ing rankings ever, if that's what you really think.\" One can argue for Jones, for sure, but there's a strong case to be made for Makhachev. Unlike the GOAT discussion, rankings are not a lifetime achievement award. Jones has fought only once in over four years. In that time, Makhachev is 8-0 with seven finishes. And when it comes to ESPN's rankings, our eligibility rules require a fighter to have competed in the past year or have a fight booked. Jones last fought on March 4, 2023 -- exactly 15 months ago, after a three-year absence -- and has no upcoming fight scheduled. He is ineligible. (Interestingly, Jones does appear in the UFC's official rankings -- at No. 2, behind Makhachev.) Eligibility rules account for the one (small) change in these ESPN rankings. One of our voters had Demetrious Johnson in his top 10 the last time we published rankings, but \"Mighty Mouse\" is now ineligible. Removing Johnson, whose last fight was 13 months ago, bumped up Israel Adesanya one spot on that voter's ballot, putting Adesanya in an overall tie with his teammate, Alexander Volkanovski.\"\"\"]\n",
        "input_tensor = torch.tensor(embedding_model(input_text).numpy(), dtype=torch.float32)\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(output) # highest index - 3 - sport - correct :)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY5jyhA39Ox1",
        "outputId": "c1e6ebb4-9331-4489-b6a7-2e52f0b05bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.3041, -2.4238, -1.2287,  5.7125, -3.6491]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles = [\n",
        "    \"\"\"How much is Elon Musk, the mercurial multibillionaire, worth to Tesla, the carmaker he runs? In 2018 the company’s board put in place a plan to award Mr Musk shares over ten years worth $46bn, at their current price, provided the business cleared a series of hurdles. In January a Delaware judge struck down the package, calling it “unfathomable”, after a shareholder sued to have it rescinded. The company has asked its investors to reaffirm their support for the award ahead of an annual general meeting on June 13th. Mr Musk’s monster pay package is worth nearly 300 times what America’s best-paid chief executive, Hock Tan of Broadcom, a chipmaker, made last year. It is also equivalent to 8% of Tesla’s current market value—which is down by roughly a fifth over the past year.\"\"\",\n",
        "    \"\"\"macy s owner buys rival for $11bn us retail giant federated department stores is to buy rival may department stores for $11bn (Â£5.7bn).  the deal will bring together famous stores like macy s  bloomingdale s and marshall field s  creating the largest department store chain in the us. the combined firm will operate about 1 000 stores across the us  with combined annual sales of $30bn. the two companies  facing competition\"\"\",\n",
        "    \"\"\"A Scottish woman who allegedly inspired the character Martha in the hit Netflix drama Baby Reindeer is suing the streamer for defamation, negligence and privacy violations. Fiona Harvey, who says Martha is based on her, argued in a lawsuit filed in a California court on Thursday that Netflix told \"brutal lies\" about her to over 50 million viewers around the world. The lawsuit seeks over $170m (£132m) in damages for Ms Harvey, who claims the Baby Reindeer series falsely depicted her as a convicted criminal who spent time in prison for stalking. Netflix did not immediately respond to BBC's request for comment. Ms Harvey also denies that she sexually assaulted the show's creator, according to the court documents, which allege that Netflix “told these lies, and never stopped, because it was a better story than the truth, and better stories made money”. In one scene in the series, the Martha character is depicted as sexually assaulting the show's protagonist along a canal one night. Speaking to BBC News on Thursday, Ms Harvey said she was certain that Netflix would lose the case. \"I have no doubt about that. Otherwise, we wouldn't be doing it. We think we are going to win,\" she said. The first episode of the hit mini-series claims that \"this is a true story\". The show's end credits say that the programme \"is based on real events: however certain characters, names, incidents, locations, and dialogue have been fictionalized for dramatic purposes”. While giving evidence before the Culture Media and Sport Committee in Parliament last month, Netflix executive Benjamin King said the show was \"obviously a true story of the horrific abuse that the writer and protagonist Richard Gadd suffered at the hands of a convicted stalker\". Mr Gadd, a comedian, wrote and stars in the series about his alleged experience of being stalked by a woman he met at the pub where he worked. He is not named as a defendant in Ms Harvey's lawsuit. Neither Mr Gadd nor Ms Harvey's real names are used in the series. On social media, Mr Gadd has previously appealed to fans to refrain from trying to identify Martha, the stalker character he first described in a stand-up comedy routine. Ms Harvey has identified herself as the woman portrayed as Martha in the series. Netflix and Mr Gadd have not confirmed this. Ms Harvey's lawsuit alleges that Netflix \"did literally nothing\" to confirm that Mr Gadd's story was true before creating the series.” “It never investigated whether Harvey was convicted, a very serious misrepresentation of the facts,” the complaint states, referring to the character Martha's prior conviction for stalking. “It did nothing to understand the relationship between Gadd and Harvey, if any. It did nothing to determine whether other facts, including an assault, the alleged stalking or the conviction was accurate.” Richard Roth, a New York-based lawyer representing Ms Harvey, told BBC News on Thursday that he has \"incontrovertible documentary evidence” proving that his client has never been convicted of a crime. The lawsuit includes a photo of a background check and a certificate that claims that Ms Harvey has no criminal convictions on her record. Martha, the Baby Reindeer character, is a convicted stalker who is later arrested after Mr Gadd's character reports her to police. Mr Roth added that there is \"no doubt\" whatsoever Ms Harvey's identity was used for Baby Reindeer's plot. Ms Harvey, who lives in the UK, says that since the series was released in April she has received numerous death threats. The experience has left her \"fearful of leaving her home or checking the news\", the lawsuits says, adding that she has \"become extremely secluded and isolated, in fear of the public, going days without leaving her home\". In a nearly hour-long interview with Piers Morgan last month, Ms Harvey confirmed that she had known Mr Gadd during his time working at a pub in London. But she denied that she had acted like the character Martha, who sends Mr Gadd's character 41,000 emails and leaves 350 hours of voicemail messages in the show. \"None of that's true. I don't think I sent him anything,\" she said. \"No, I think there may have been a couple of emails exchanged, but that was it. Just jokey banter emails.\" The lawsuit does allege, however, that real comments that she made to Mr Gadd - such as a tweet she sent him in 2014 - are used in the show's dialogue.\"\"\",\n",
        "    \"\"\"Narendra Modi is set to be India's prime minister for a third time, a day after humbling election results which saw his majority slashed by a resurgent opposition. Mr Modi was backed to be prime minister again following a meeting with his National Democratic Alliance (NDA) on Wednesday. The 73-year-old had found himself unexpectedly reliant on the NDA's smaller parties to reach a parliamentary majority after his own party fell short of the 272 needed to form the next government. However, the opposition - which won 232 seats to the NDA's 293 - has yet to formally concede. It was holding its own meeting on Wednesday in the capital, Delhi, to discuss next steps. Mr Modi is likely to be sworn in for a record-equalling third term later this week. Mr Modi and his Hindu-nationalist Bharatiya Janata Party (BJP) won 240 seats following the weeks-long, seven-stage election, making them the largest party in the Lok Sabha, India's lower house. But it is a significantly reduced number for the prime minister: in 2019, the BJP won 303 seats, and Mr Modi had said he was aiming for 370 seats this time round. Instead, they have had to rely on NDA partners to secure Mr Modi's third term. According to an NDA release, he was \"unanimously\" chosen as their leader at the meeting at his Delhi residence, adding they were \"committed to serving the poor, women, youth, farmers and exploited, deprived and oppressed citizens of India\". Exactly what concessions its partners may have elicited from the BJP remains to be seen. Ahead of the meeting, there was speculation that demands from more powerful groups may have included ministerial positions in return for their support. It is the first time Mr Modi will have governed in coalition without his party having an outright majority, and it is unclear what the next five years will look like. Nilanajan Mukhopadhyay, who has written a biography of Mr Modi, told AFP news agency it would \"force Modi to take the point of view of others\". \"We shall see more democracy and a healthy parliament,\" he added. \"He will have to be a leader that he has never been; we will have to see a new Modi.\" Meanwhile, the opposition INDIA coalition has been celebrating the results - despite not winning. Congress president Mallikarjun Kharge hailed the \"overwhelming support received by our alliance\" and said voters had sent a message opposing the BJP's \"politics of hate, corruption and deprivation\". \"This is a mandate in defence of the Constitution of India and against price rise, unemployment, and crony capitalism and also to save democracy,\" his statement on social media added. Following the NDA's declaration of victory, the White House congratulated Mr Modi and said the US was hoping to work with India to \"ensure a free and open\" Asia. This year's Indian election was the largest the world has ever seen. More than 600m people took part - or 66% of the country's eligible voters. Nearly a billion people had registered to vote in total – about one in eight of the global population. Voting was staggered over seven rounds between 19 April and 1 June for security and logistical reasons. Much of the election took place in extreme and deadly heat as temperatures in parts of India soared to nearly 50C.\"\"\",\n",
        "    \"\"\"President Joe Biden said in an interview Thursday he would not pardon his son, Hunter Biden, if he’s found guilty of criminal federal gun charges. When asked by interviewer David Muir of ABC if he would rule out pardoning Hunter Biden, the president said, “Yes.” He also affirmed he would accept the outcome of the trial, currently underway in Delaware. The courtroom proceedings have delved into a painful moment for the Biden family, as Hunter was struggling with drug addiction in the aftermath of his brother Beau’s death. First lady Dr. Jill Biden attended the trial before traveling to France to join her husband for D-Day commemoration activities. The White House had said previously that Biden would not pardon his son. “I’ve been very clear; the president is not going to pardon his son,” press secretary Karine Jean-Pierre said in December. The president’s son is accused of illegally purchasing and possessing a gun while abusing or being addicted to drugs, a violation of federal law. He pleaded not guilty to the three charges, though he’s been open about his struggles with alcohol and crack cocaine addiction. The charges were brought by special counsel David Weiss. It’s the first time in US history that the child of a sitting president has been on trial. Biden has previously voiced support for his son and said he was proud of his recovery from addiction. “I am the President, but I am also a Dad. Jill and I love our son, and we are so proud of the man he is today,” the president said in a statement Monday as the trial was getting underway. “Hunter’s resilience in the face of adversity and the strength he has brought to his recovery are inspiring to us. A lot of families have loved ones who have overcome addiction and know what we mean,” he went on. “As the President, I don’t and won’t comment on pending federal cases, but as a Dad, I have boundless love for my son, confidence in him, and respect for his strength. Our family has been through a lot together, and Jill and I are going to continue to be there for Hunter and our family with our love and support.” The trial is underway the week after a New York jury returned 34 guilty counts against former President Donald Trump for falsifying business records in an attempt to cover up hush money payments to a porn actress. Trump has decried the verdict and falsely accused Biden of orchestrating the criminal charges. In the interview Thursday, Biden accused Trump of trying to subvert the rule of law by questioning the verdict in his own criminal trial. “He’s trying to undermine it,” Biden told Muir. “He got a fair trial. The jury spoke.” This story has been updated with additional reporting.\"\"\",\n",
        "    \"\"\"The Federal Trade Commission is investigating a recent Microsoft deal with artificial intelligence startup Inflection, according to a person familiar with the matter, as US antitrust regulators ramp up scrutiny of the red-hot AI industry. Microsoft announced in March that it had hired Inflection’s co-founders and a number of its staff to lead its Copilot program, and Inflection said its AI model would be hosted on Microsoft’s cloud platform. As part of that deal, Microsoft was said to have paid Inflection $650 million. In its announcement at the time, Microsoft described the move as merely a hiring decision, not as an acquisition. The FTC probe into Microsoft concerns whether the company’s investment in Inflection constituted an acquisition that Microsoft failed to disclose to the government, one of the people said. The investigation comes as antitrust officials at the FTC and the Justice Department are nearing a final agreement this week on how to jointly oversee AI giants such as Microsoft, Google, Nvidia, OpenAI and others, two people familiar with the matter told CNN. That agreement, which could be finalized within days, would appoint DOJ as the lead investigator of Nvidia, while the FTC would take responsibility for investigating Microsoft and OpenAI, the people said. DOJ will likely continue its role in overseeing Google, one of the people indicated. Any investigations would focus on whether the companies have used their dominant positions in the AI industry to harm competition through abusive and illegal behavior. The agreement shows enforcers are poised for a broad crackdown on some of the most well-known players in the AI sector, said Sarah Myers West, managing director of the AI Now Institute and a former AI advisor to the FTC. “Clearance processes like this are usually a key step before advancing an investigation,” West said. “This is a clear sign they’re moving quickly here.” Microsoft declined to comment on the DOJ-FTC agreement but, in a statement, defended its partnership with Inflection. “Our agreements with Inflection gave us the opportunity to recruit individuals at Inflection AI and build a team capable of accelerating Microsoft Copilot, while enabling Inflection to continue pursuing its independent business and ambition as an AI studio,” a Microsoft spokesperson said, adding that the company is “confident” it has complied with its reporting obligations. Inflection and Google didn’t immediately respond to a request for comment; Nvidia and OpenAI declined to comment. The DOJ and FTC declined to comment. The FTC-DOJ agreement was earlier reported by The New York Times; the FTC’s Microsoft probe was earlier reported by The Wall Street Journal. FTC Chair Lina Khan has warned in op-eds and congressional testimony that, left unchecked, artificial intelligence could “turbocharge” fraud and scams. The agency has published numerous reminders and warnings that businesses can be held liable for making misleading claims about their AI tools or for covertly using consumer data to train AI models. The FTC is currently investigating Reddit’s AI content licensing practices, and is separately investigating OpenAI for possible violations of consumer protection law. The US agencies’ division-of-labor agreement opens the door to more intensive probes of a sector that has energized investors, enthralled consumers and raised alarm bells among critics who say AI urgently needs regulation to forestall widespread job displacement, discrimination and fraud. Specifically, it carves out roles for the FTC and DOJ to review whether tech giants and AI companies are behaving in anticompetitive ways. And it highlights how enforcers are increasingly trying to bring existing laws to bear on the industry as prospects for new US laws governing AI have dimmed. The United States is widely viewed as a laggard on AI regulation as others such as the European Union have leapfrogged it with tough rules about how the technology can be used in high-risk contexts. The EU AI Act, for example, outlaws social scoring systems powered by AI and any biometric-based tools used to guess a person’s race, political leanings or sexual orientation. It also bans the use of AI to interpret the emotions of people in schools and workplaces, as well as some types of automated profiling intended to predict a person’s likelihood of committing future crimes. For years, technology critics and regulators have worried that major tech companies may be monopolizing entire sectors of the economy. That has led to high-profile US government antitrust suits targeting Amazon, Apple, Google, Meta and Microsoft. Some fear that tech companies could abuse their powerful roles in business and society to extend their dominance into the fast-growing field of generative AI, which exploded onto the scene in 2022 when OpenAI released ChatGPT. Nvidia’s soaring stock prices have served as a barometer of the AI frenzy, underscoring the company’s position as a leading supplier of computing chips necessary for training advanced AI models. On Wednesday, Nvidia became the second-largest publicly traded company in the United States, ending the day with a market capitalization of more than $3 trillion and edging out Apple. “AI relies on massive amounts of data and computing power, which can give already-dominant firms a substantial advantage,” DOJ antitrust chief Jonathan Kanter said last week in a speech at Stanford University, adding that Americans’ reliance on just a handful of technology giants could allow them to “control these new markets.” One way for tech giants to wield anticompetitive influence in the AI sector, critics say, is through exclusive partnerships with AI startups. The agreements can potentially “lock in” AI developers as customers of large cloud computing services and give the tech giants significant stakes or influence over the development of AI. Those types of deals, including Microsoft’s relationship with OpenAI, are the subject of an ongoing study by the FTC announced in January. The DOJ has also become increasingly vocal on AI issues. In 2022, the agency’s antitrust division hired Susan Athey, a Stanford University professor and AI expert, to be its chief economist. In looking at competition in the AI industry, antitrust enforcers should take lessons on how technology giants have behaved anticompetitively in the past, Athey told CNN at a recent event in Washington hosted by Bloomberg News. That could include gatekeeping or bottlenecking tactics, making it harder for consumers or customers to switch providers, or becoming the biggest buyer of key supplies — such as AI chips, for example — and denying those necessary supplies to competing rivals. “We should look back to historical analogues and see where sources of market power have been and how people have preserved them, and those are the kinds of tactics we might worry about going forward,” Athey said. That the DOJ is picking up oversight of Nvidia from the FTC is particularly notable, West told CNN. “It’s possible that means criminal penalties are now on the table, because that’s one of the tools DOJ uniquely carries.\"\"\",\n",
        "    \"\"\"Drones have changed war. Small, cheap, and deadly robots buzz in the skies high above the world’s battlefields, taking pictures and dropping explosives. They’re hard to counter. ZeroMark, a defense startup based in the United States, thinks it has a solution. It wants to turn the rifles of frontline soldiers into “handheld Iron Domes.” The idea is simple: Make it easier to shoot a drone out of the sky with a bullet. The problem is that drones are fast and maneuverable, making them hard for even a skilled marksman to hit. ZeroMark’s system would add aim assistance to existing rifles, ostensibly helping soldiers put a bullet in just the right place. “We’re mostly a software company,” ZeroMark CEO Joel Anderson tells WIRED. He says that the way it works is by placing a sensor on the rail mount at the front of a rifle, the same place you might put a scope. The sensor interacts with an actuator either in the stock or the foregrip of the rifle that makes adjustments to the soldier’s aim while they’re pointing the rifle at a target. A soldier beset by a drone would point their rifle at the target, turn on the system, and let the actuators solidify their aim before pulling the trigger. “So there’s a machine perception, computer vision component. We use lidar and electro-optical sensors to detect drones, classify them, and determine what they’re doing,” Anderson says. “The part that is ballistics is actually quite trivial … It’s numerical regression, it’s ballistic physics.” According to Anderson, ZeroMarks’ system is able to do things a human can’t. “For them to be able to calculate things like the bullet drop and trajectory and windage … It’s a very difficult thing to do for a person, but for a computer, it’s pretty easy,” he says. “And so we predetermined where the shot needs to land so that when they pull the trigger, it’s going to have a high likelihood of intersecting the path of the drone.” ZeroMark makes a tantalizing pitch—one so attractive that venture capital firm Andreesen Horowitz invested $7 million in the project. The reasons why are obvious for anyone paying attention to modern war. Cheap and deadly flying robots define the conflict between Russia and Ukraine. Every month, both sides send thousands of small drones to drop explosives, take pictures, and generate propaganda. With the world’s militaries looking for a way to fight back, counter-drone systems are a growth industry. There are hundreds of solutions, many of them not worth the PowerPoint slide they’re pitched from. Can a machine-learning aim-assist system like what ZeroMark is pitching work? It remains to be seen. According to Anderson, ZeroMark isn’t on the battlefield anywhere, but the company has “partners in Ukraine that are doing evaluations. We’re hoping to change that by the end of the summer.” There’s good reason to be skeptical. “I’d love a demonstration. If it works, show us. Till that happens, there are a lot of question marks around a technology like this,” Arthur Holland Michel, a counter-drone expert and senior fellow at the Carnegie Council for Ethics in International Affairs, tells WIRED. “There’s the question of the inherent unpredictability and brittleness of machine-learning-based systems that are trained on data that is, at best, only a small slice of what the system is likely to encounter in the field.” Anderson says that ZeroMark’s training data is built from “a variety of videos and drone behaviors that have been synthesized into different kinds of data sets and structures. But it’s mostly empirical information that’s coming out of places like Ukraine.” Michel also contends that the physics, which Anderson says are simple, are actually quite hard. ZeroMark’s pitch is that it will help soldiers knock a fast-moving object out of the sky with a bullet. “And that is very difficult,” Michel says. “It’s a very difficult equation. People have been trying to shoot drones out of the sky [for] as long as there have been drones in the sky. And it’s difficult, even when you have a drone that is not trying to avoid small arms fire.” That doesn’t mean ZeroMark doesn’t work—just that it’s good to remain skeptical in the face of bold claims from a new company promising to save lives. “The only truly trustworthy metric of whether a counter-drone system works is if it gets used widely in the field—if militaries don’t just buy three of them, they buy thousands of them,” Michel says. “Until the Pentagon buys 10,000, or 5,000, or even 1,000, it’s hard to say, and a little skepticism is very much merited.”\"\"\",\n",
        "    \"\"\"It sounds like Jennifer Lopez is trying to keep it positive. Following her cancellation last week of her summer concert tour so she could spend more time with her family, Lopez has offered a note of gratitude to her fans. Visitors to her On The Jlo were greeted this week with a note from the singer/actress about the success of her latest film “Atlas,” which is steaming on Netflix. The message begins with “Thank you.” “Hi everybody,” it reads. “I just found out some great news and it’s all because of YOU!!” Lopez shares that “Atlas” recently reached No. 1 on the Netflix top 10 movie titles list and she credited her supporters. “It may seem like there’s a lot of negativity out in the world right now…but don’t let the voices of a few drown out there is soooo much love out there,” her note reads. “Thank you, thank you, thank you!! I love you all so much. Jennifer.” Live Nation and Lopez announced the cancelation of her “THIS IS ME…LIVE” tour last Friday, stating she would “taking time off to be with her children, family and close friends.” “I am completely heartsick and devastated about letting you down. Please know that I wouldn’t do this if I didn’t feel that it was absolutely necessary,” Lopez said at the time. “I promise I will make it up to you and we will all be together again. I love you all so much. Until next time.” The news came amid recent reports Lopez and her husband Ben Affleck have been living apart. CNN has reached out to representatives for both stars for comment. The couple married in July 2022. Lopez’s tour was scheduled to kickoff June 26 in Orlando.\"\"\",\n",
        "    \"\"\"Slim Shady is back, back again. But, apparently, not for long. Eminem appears to be killing off his alter ego in his latest project, an album titled, “The Death of Slim Shady (Coup de Grace).” He has dropped the album’s first single, “Houdini,” which samples the Steve Miller Band’s 1982 hit, “Abracadabra.” His lyrics take aim at several people, including Megan Thee Stallion and even himself. “If I was to ask for Megan Thee Stallion if she would collab with me, would I really have a shot at a feat?” he raps, referencing Tory Lanez shooting Megan Thee Stallion in the feet in 2020, a crime for which Lanez was sentenced to 10 years in prison. Eminem also released a music video for the song in which he gets a little help from some of his celebrity friends. It opens with the 51-year-old rapper’s music manager, Paul Rosenberg, leaving a seemingly less than positive voicemail about the new album before Eminem’s mentor and frequent collaborator, Dr. Dre, tells him, “We’ve got a problem.” The issue being a portal that allows time travel between the present and 2002. Cameos from 50 Cent, Snoop Dogg and Pete Davidson follow. Eminem battles it out with his younger-self in the video, which will remind fans of his 2002 “Without Me” video. Recently a faux obituary appeared for his Slim Shady alter ego in his hometown newspaper, the Detroit Free Press. It referred to Slim Shady a “a rogue splinter in the flourishing underground rap scene.” “The Death of Slim Shady (Coup de Grâce)” follows Eminem’s 2020 album, “Music to Be Murdered By.” The full collection of new music is set for release this summer.\"\"\",\n",
        "    \"\"\"Lenny Kravitz’s leather pants continue to stay on. In a recently published interview with The Guardian, Kravitz opened about discovering that his father was having an affair, a story recounted in his 2020 memoir. His father, he said, told him he too would end up cheating on his spouse. “He became right. After the marriage [to actress Lisa Bonet], I became more like him,” Kravitz told The Guardian. “I was becoming a player.” The Grammy-winning singer and guitarist ended up divorcing Bonet in 1993 and discussed how he felt about his cheating ways. “I didn’t like it. I didn’t want to be that guy,” he said. “So I had to tackle that and it took years.” That meant “taking responsibility,” he said, adding he had to have “discipline” and not let “my own desires take over.” Kravitz said he’s not been in a serious relationship in nine years. The reporter then asked if Kravitz was being genuine when he stated previously that he wanted to be celibate until he found the right person. Kravitz confirmed his stance. “Yes,” he said. “It’s a spiritual thing.” CNN has reached out to a representative for the star for comment. Kravitz had previously opened up in interviews about his decision to stay celibate, saying in an interview with CBS Sunday Morning in 2008 that he believed that “in the end, that’s going to help me to find the right person.” “So I’m not going to waver on that. That’s a promise I made to God three years ago,” he said at the time. Kravitz has been linked to stars like Nicole Kidman, Adriana Lima and Kylie Minogue in the past. He now says, however, “I have become very set in my ways, in the way I live.” That sound you are hearing is the many volunteers asking Kravitz, “Are you going go my way?”\"\"\",\n",
        "    \"\"\"Kristaps Porzingis didn’t want to make predictions about how his body would respond heading into the NBA Finals after he spent more than a month on the sideline with a calf injury. Just fine, it turned out. Jaylen Brown scored 22 points, Porzingis made an immediate impact off the bench and added 20 and the Boston Celtics powered past the Dallas Mavericks 107-89 on Thursday night in Game 1. Derrick White finished with 15 points for Boston, which led by 29 points in the first half and connected on 16 3-pointers in a powerful start to its quest for an 18th NBA title. Porzingis, a 7-footer who had been sidelined since April 29, added six rebounds and three blocks in 21 minutes. “Tonight was affirmation to myself that I’m pretty good,” Porzingis said. “I’m not perfect but I can play like this and I can add to this team.” The last Celtics player to enter the court for pregame warmups, he said he received a jolt of energy from a home crowd, which erupted when he emerged from the tunnel. “The adrenaline was pumping through my veins,” Porzingis said. Celtics coach Joe Mazzulla wasn’t concerned about the layoff affecting Porzingis’ aggressiveness.\"\"\",\n",
        "    \"\"\"FORT WORTH, Texas — What do you get for the nine-time national all-around champion who has everything? An engraved silver belt buckle, apparently. Simone Biles earned the Texas token as a trophy Sunday night after clinching the top spot at the U.S. Gymnastics Championships with a total score of 119.750 (60.450 on Day 1, 59.300 on Day 2). She also swept every individual event title. Over two nights, the 27-year-old’s biggest mistake was over-rotating her Yurchenko double pike vault and landing on her back. Even with the fall, she still earned a 15.000 because the vault is so difficult and Biles executed it with little flaw aside from the landing. She began on the balance beam with a solid routine for a 14.800 and added a 15.100 on floor exercise. Though she had a bit too much juice on her triple-twisting double back tuck and bounced out of bounds, she incurred only minor landing deductions for her other three über difficult tumbling passes. Biles capped her winning all-around performance on the uneven bars, coasting through in her typical speedy fashion to score a 14.400. Amidst her historic night, Biles found time to boost up a fellow Olympic all-around champion after Suni Lee opened the night with a fall on vault. Lee competed a double-twisting vault Friday and warmed it up Sunday but didn’t get enough height off the vault table to complete two twists due to her hand slipping when performing the vault in competition. She managed 1 1/2 twists and sat the vault down, similar to the incomplete vault Biles did during team finals in Tokyo when struggling with the twisties. Lee wasn’t injured but stepped off the competition floor to gather herself after the fluke vault, and Biles found her to offer some support. “After Suni vaulted, I knew exactly what was going through her head. I dealt with that in Tokyo so I just knew that she needed some encouragement and somebody to trust her gymnastics for her and to believe in her, so that’s exactly what I did,” Biles said, adding that Lee asked her to stand by the uneven bars during her next routine after vault. Lee righted the ship with a 14.500 on bars for a routine that does not yet include the full difficulty she plans to show at Olympic Trials and landed in fourth in her first all-around competition back since 2021. Following the meet, she credited Biles for helping her stay grounded after the vault fall. “She’s been one of my biggest inspirations for a long time. I know that we’re kinda teammates and competitors, but she’s somebody that I look up to so to hear those words coming from her means a lot,” Lee said. Skye Blakely held on to her second-place position with another strong showing, highlighted by a 14.450 on bars. The 19-year-old has struggled with consistency issues when competing internationally, but with the debut of an upgraded vault and eight hit routines over both nights of competition, she strengthened her case for making the Paris Olympic team. Tokyo silver medalist Jordan Chiles climbed to fifth in the all-around standings behind a big 14.100 floor routine while 2020 floor Olympic gold medalist Jade Carey finished seventh. Both recorded falls on beam, but they weren’t alone. The four-inch event gave multiple gymnasts trouble Sunday, as Paris contender Leanne Wong also fell. Sixteen gymnasts earned an invite to Olympic Trials, slated for June 27-30 in Minneapolis, where they will compete for a spot on the five-person squad headed to Paris. Among the field are Shilese Jones and Kaliya Lincoln, who withdrew from championships but successfully petitioned to Trials. In the men’s competition, Brody Malone won his third national all-around title and took first on high bar Saturday night in his first competition back after a devastating knee injury in March 2023 that required three surgeries. The 24-year-old, who had to relearn how to walk after suffering a tibial plateau fracture, a partially torn PCL and a fully torn LCL, is on track to make his second Olympic team this summer alongside Michigan’s Frederick Richard and Stanford’s Khoi Young. The two 2023 World Championship medalists finished second and third, respectively, in the men’s all-around.\"\"\",\n",
        "    \"\"\"Islam Makhachev made a third successful defense of his UFC lightweight championship on Saturday night, and his reward in the pound-for-pound rankings is ... nothing. Makhachev (26-1), who submitted Dustin Poirier in Round 5 of their UFC 302 main event, was already No. 1 in the ESPN rankings. There was nowhere for him to rise. However, Makhachev surely solidified his position at the top of the sport in the eyes of our voters and many fans, although not the person whose job it is to promote him. \"I don't think he's the pound-for-pound best fighter in the world,\" UFC CEO Dana White said after Saturday's fights. \"For anyone to call Islam the pound-for-pound best fighter in the world when Jon Jones is still f---ing fighting is nuts and shouldn't be ranking in the pound-for-pound or doing any of the f---ing rankings ever, if that's what you really think.\" One can argue for Jones, for sure, but there's a strong case to be made for Makhachev. Unlike the GOAT discussion, rankings are not a lifetime achievement award. Jones has fought only once in over four years. In that time, Makhachev is 8-0 with seven finishes. And when it comes to ESPN's rankings, our eligibility rules require a fighter to have competed in the past year or have a fight booked. Jones last fought on March 4, 2023 -- exactly 15 months ago, after a three-year absence -- and has no upcoming fight scheduled. He is ineligible. (Interestingly, Jones does appear in the UFC's official rankings -- at No. 2, behind Makhachev.) Eligibility rules account for the one (small) change in these ESPN rankings. One of our voters had Demetrious Johnson in his top 10 the last time we published rankings, but \"Mighty Mouse\" is now ineligible. Removing Johnson, whose last fight was 13 months ago, bumped up Israel Adesanya one spot on that voter's ballot, putting Adesanya in an overall tie with his teammate, Alexander Volkanovski.\"\"\"\n",
        "]\n",
        "\n",
        "actual_classes = ['business', 'business', 'entertainment', 'politics', 'politics', 'tech', 'tech', 'entertainment', 'entertainment', 'entertainment', 'sports', 'sports', 'sports']"
      ],
      "metadata": {
        "id": "pLTxuje6-Rvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(articles)\n",
        "len(actual_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeTyV6lVOxh0",
        "outputId": "0451c735-cbfa-4325-b3cb-6a4827fd9da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class_labels = ['business', 'entertainment', 'politics', 'sports', 'tech']\n",
        "\n",
        "all_predictions = []\n",
        "\n",
        "for article in articles:\n",
        "    embedding = embedding_model([article])\n",
        "    article_tensor = torch.tensor(embedding.numpy(), dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        output = model(article_tensor)\n",
        "    probabilities = F.softmax(output, dim=1) # output probabilities\n",
        "    predicted_class = torch.argmax(probabilities, dim=1).item() # get index of maximum probability\n",
        "    all_predictions.append(predicted_class)\n",
        "\n",
        "\n",
        "for i, prediction in enumerate(all_predictions):\n",
        "    predicted_label = class_labels[prediction]\n",
        "    actual_label = actual_classes[i]\n",
        "    print(f\"Article {i+1} Prediction: {predicted_label}, Actual: {actual_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6NSLZOW9-Hi",
        "outputId": "0061b925-3806-4fd7-abae-a646cc79d613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1 Prediction: business, Actual: business\n",
            "Article 2 Prediction: business, Actual: business\n",
            "Article 3 Prediction: entertainment, Actual: entertainment\n",
            "Article 4 Prediction: politics, Actual: politics\n",
            "Article 5 Prediction: politics, Actual: politics\n",
            "Article 6 Prediction: business, Actual: tech\n",
            "Article 7 Prediction: tech, Actual: tech\n",
            "Article 8 Prediction: entertainment, Actual: entertainment\n",
            "Article 9 Prediction: entertainment, Actual: entertainment\n",
            "Article 10 Prediction: sports, Actual: entertainment\n",
            "Article 11 Prediction: sports, Actual: sports\n",
            "Article 12 Prediction: sports, Actual: sports\n",
            "Article 13 Prediction: sports, Actual: sports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "only one wrong prediction :)"
      ],
      "metadata": {
        "id": "dezNg8E5c106"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Playing with the Model / Optimizing Hyper Parameters"
      ],
      "metadata": {
        "id": "Ac1QcYIbdUAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring Learning Rates"
      ],
      "metadata": {
        "id": "jdE2ZbYedna7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_learning_rate(learning_rate):\n",
        "    model = SimpleNN(embedding_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, output_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_type = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for sequences, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(sequences)\n",
        "            loss = loss_type(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        train_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}')\n",
        "\n",
        "    return train_losses\n"
      ],
      "metadata": {
        "id": "eRSuu6cedYyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to visualize training loss across epochs for a given learning rates\n",
        "def plot_training_loss(learning_rates, train_losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for lr, losses in zip(learning_rates, train_losses):\n",
        "        plt.plot(range(1, num_epochs+1), losses, label=f'LR: {lr}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Training Loss')\n",
        "    plt.title('Training Loss vs Epochs for Different Learning Rates')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ATdbsoq-ds36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "num_epochs = 10\n",
        "train_losses_collection = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f'Training with learning rate: {lr}')\n",
        "    train_losses = train_with_learning_rate(lr)\n",
        "    train_losses_collection.append(train_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5hy3mG2d29a",
        "outputId": "9e585020-0027-45f7-87f3-7d4bf025c345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with learning rate: 0.1\n",
            "Epoch 1/10, Train Loss: 2.7219\n",
            "Epoch 2/10, Train Loss: 1.5260\n",
            "Epoch 3/10, Train Loss: 1.5122\n",
            "Epoch 4/10, Train Loss: 1.4129\n",
            "Epoch 5/10, Train Loss: 1.3636\n",
            "Epoch 6/10, Train Loss: 1.5235\n",
            "Epoch 7/10, Train Loss: 1.4763\n",
            "Epoch 8/10, Train Loss: 1.5201\n",
            "Epoch 9/10, Train Loss: 1.3778\n",
            "Epoch 10/10, Train Loss: 1.5634\n",
            "Training with learning rate: 0.01\n",
            "Epoch 1/10, Train Loss: 1.1343\n",
            "Epoch 2/10, Train Loss: 0.4069\n",
            "Epoch 3/10, Train Loss: 0.2702\n",
            "Epoch 4/10, Train Loss: 0.2867\n",
            "Epoch 5/10, Train Loss: 0.1914\n",
            "Epoch 6/10, Train Loss: 0.1818\n",
            "Epoch 7/10, Train Loss: 0.1402\n",
            "Epoch 8/10, Train Loss: 0.1160\n",
            "Epoch 9/10, Train Loss: 0.1896\n",
            "Epoch 10/10, Train Loss: 0.1418\n",
            "Training with learning rate: 0.001\n",
            "Epoch 1/10, Train Loss: 1.5896\n",
            "Epoch 2/10, Train Loss: 1.2415\n",
            "Epoch 3/10, Train Loss: 0.7648\n",
            "Epoch 4/10, Train Loss: 0.5735\n",
            "Epoch 5/10, Train Loss: 0.3654\n",
            "Epoch 6/10, Train Loss: 0.2624\n",
            "Epoch 7/10, Train Loss: 0.2145\n",
            "Epoch 8/10, Train Loss: 0.1716\n",
            "Epoch 9/10, Train Loss: 0.1868\n",
            "Epoch 10/10, Train Loss: 0.1327\n",
            "Training with learning rate: 0.0001\n",
            "Epoch 1/10, Train Loss: 1.6125\n",
            "Epoch 2/10, Train Loss: 1.6098\n",
            "Epoch 3/10, Train Loss: 1.6056\n",
            "Epoch 4/10, Train Loss: 1.5998\n",
            "Epoch 5/10, Train Loss: 1.5891\n",
            "Epoch 6/10, Train Loss: 1.5725\n",
            "Epoch 7/10, Train Loss: 1.5397\n",
            "Epoch 8/10, Train Loss: 1.4766\n",
            "Epoch 9/10, Train Loss: 1.3642\n",
            "Epoch 10/10, Train Loss: 1.2166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_loss(learning_rates, train_losses_collection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "LVLw2RGQeuGL",
        "outputId": "3919c710-3904-4fbc-cbf3-37e5ab7d8a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation**\n",
        "\n",
        "- __0.1 (Blue Line):__\n",
        " - Starts relatively high and decreases sharply in first epoch\n",
        " - Fluctuate after inital decrease and doesn't consistently decrease.\n",
        " - 0.1 might be too high cause model to overshoot and not converge properly\n",
        "\n",
        "- **0.01 (Orange Line):**\n",
        " - Rapid inital decrease. By epoch 2, loss is low and remains stable\n",
        " - 0.01 allows model to converge quickly and consistently - effective training.\n",
        "\n",
        "- **0.001 (Green Line):**\n",
        " - Decreases rapidly over epochs. No significant fluctuations.\n",
        " - Remains stable thereafter.\n",
        " - Also effective - gradual and stable convergence.\n",
        "\n",
        "- **0.0001 (Red Line):**\n",
        " - Decreases slowly over epochs and remains higher.\n",
        " - Minimal loss reduction\n",
        " - Too slow"
      ],
      "metadata": {
        "id": "gENu8kIjiGgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "- 0.01 is best choice.\n",
        "- 0.001 is also a good choice but it converges more slowly compared to 0.01"
      ],
      "metadata": {
        "id": "j0mW31bOkMi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring Batch Size"
      ],
      "metadata": {
        "id": "xQ8MfSYylxDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [8, 16, 32, 64, 128, 256]\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "accuracies = []\n",
        "\n",
        "# training the model for different batch sizes\n",
        "for batch_size in batch_sizes:\n",
        "  model = SimpleNN(embedding_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, output_dim)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, loss_type)\n",
        "    print(f'Batch Size: {batch_size}, Epoch: {epoch+1}, Training Loss: {train_loss:.4f}')\n",
        "\n",
        "  test_loss, test_preds, test_labels = evaluate(model, test_loader, loss_type)\n",
        "  accuracy = np.mean(np.array(test_preds) == np.array(test_labels))\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  test_losses.append(test_loss)\n",
        "  accuracies.append(accuracy)\n",
        "\n",
        "  print(f'Batch Size: {batch_size}, Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BEeEUAEl1FL",
        "outputId": "1616fdf6-8d81-4f1a-a5b8-7020d72ef566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size: 8, Epoch: 1, Training Loss: 0.9311\n",
            "Batch Size: 8, Epoch: 2, Training Loss: 0.6044\n",
            "Batch Size: 8, Epoch: 3, Training Loss: 0.5204\n",
            "Batch Size: 8, Epoch: 4, Training Loss: 0.3949\n",
            "Batch Size: 8, Epoch: 5, Training Loss: 0.4643\n",
            "Batch Size: 8, Epoch: 6, Training Loss: 0.3839\n",
            "Batch Size: 8, Epoch: 7, Training Loss: 0.3816\n",
            "Batch Size: 8, Epoch: 8, Training Loss: 0.4403\n",
            "Batch Size: 8, Epoch: 9, Training Loss: 0.3224\n",
            "Batch Size: 8, Epoch: 10, Training Loss: 0.3063\n",
            "Batch Size: 8, Test Loss: 0.6343, Test Accuracy: 0.8854\n",
            "\n",
            "\n",
            "Batch Size: 16, Epoch: 1, Training Loss: 0.7883\n",
            "Batch Size: 16, Epoch: 2, Training Loss: 0.3866\n",
            "Batch Size: 16, Epoch: 3, Training Loss: 0.3171\n",
            "Batch Size: 16, Epoch: 4, Training Loss: 0.2829\n",
            "Batch Size: 16, Epoch: 5, Training Loss: 0.2584\n",
            "Batch Size: 16, Epoch: 6, Training Loss: 0.2405\n",
            "Batch Size: 16, Epoch: 7, Training Loss: 0.3444\n",
            "Batch Size: 16, Epoch: 8, Training Loss: 0.1997\n",
            "Batch Size: 16, Epoch: 9, Training Loss: 0.2120\n",
            "Batch Size: 16, Epoch: 10, Training Loss: 0.2920\n",
            "Batch Size: 16, Test Loss: 0.1884, Test Accuracy: 0.9573\n",
            "\n",
            "\n",
            "Batch Size: 32, Epoch: 1, Training Loss: 0.8728\n",
            "Batch Size: 32, Epoch: 2, Training Loss: 0.4147\n",
            "Batch Size: 32, Epoch: 3, Training Loss: 0.2711\n",
            "Batch Size: 32, Epoch: 4, Training Loss: 0.2087\n",
            "Batch Size: 32, Epoch: 5, Training Loss: 0.2339\n",
            "Batch Size: 32, Epoch: 6, Training Loss: 0.1813\n",
            "Batch Size: 32, Epoch: 7, Training Loss: 0.2504\n",
            "Batch Size: 32, Epoch: 8, Training Loss: 0.1684\n",
            "Batch Size: 32, Epoch: 9, Training Loss: 0.1626\n",
            "Batch Size: 32, Epoch: 10, Training Loss: 0.1530\n",
            "Batch Size: 32, Test Loss: 0.1702, Test Accuracy: 0.9528\n",
            "\n",
            "\n",
            "Batch Size: 64, Epoch: 1, Training Loss: 1.0077\n",
            "Batch Size: 64, Epoch: 2, Training Loss: 0.3072\n",
            "Batch Size: 64, Epoch: 3, Training Loss: 0.2417\n",
            "Batch Size: 64, Epoch: 4, Training Loss: 0.1937\n",
            "Batch Size: 64, Epoch: 5, Training Loss: 0.1987\n",
            "Batch Size: 64, Epoch: 6, Training Loss: 0.1462\n",
            "Batch Size: 64, Epoch: 7, Training Loss: 0.1567\n",
            "Batch Size: 64, Epoch: 8, Training Loss: 0.1612\n",
            "Batch Size: 64, Epoch: 9, Training Loss: 0.1580\n",
            "Batch Size: 64, Epoch: 10, Training Loss: 0.1458\n",
            "Batch Size: 64, Test Loss: 0.1980, Test Accuracy: 0.9596\n",
            "\n",
            "\n",
            "Batch Size: 128, Epoch: 1, Training Loss: 1.2385\n",
            "Batch Size: 128, Epoch: 2, Training Loss: 0.4531\n",
            "Batch Size: 128, Epoch: 3, Training Loss: 0.2538\n",
            "Batch Size: 128, Epoch: 4, Training Loss: 0.1912\n",
            "Batch Size: 128, Epoch: 5, Training Loss: 0.2191\n",
            "Batch Size: 128, Epoch: 6, Training Loss: 0.1699\n",
            "Batch Size: 128, Epoch: 7, Training Loss: 0.1305\n",
            "Batch Size: 128, Epoch: 8, Training Loss: 0.1184\n",
            "Batch Size: 128, Epoch: 9, Training Loss: 0.1247\n",
            "Batch Size: 128, Epoch: 10, Training Loss: 0.1010\n",
            "Batch Size: 128, Test Loss: 0.1672, Test Accuracy: 0.9528\n",
            "\n",
            "\n",
            "Batch Size: 256, Epoch: 1, Training Loss: 1.5045\n",
            "Batch Size: 256, Epoch: 2, Training Loss: 0.7292\n",
            "Batch Size: 256, Epoch: 3, Training Loss: 0.3789\n",
            "Batch Size: 256, Epoch: 4, Training Loss: 0.2271\n",
            "Batch Size: 256, Epoch: 5, Training Loss: 0.1994\n",
            "Batch Size: 256, Epoch: 6, Training Loss: 0.2142\n",
            "Batch Size: 256, Epoch: 7, Training Loss: 0.1646\n",
            "Batch Size: 256, Epoch: 8, Training Loss: 0.1259\n",
            "Batch Size: 256, Epoch: 9, Training Loss: 0.1116\n",
            "Batch Size: 256, Epoch: 10, Training Loss: 0.0974\n",
            "Batch Size: 256, Test Loss: 0.1547, Test Accuracy: 0.9528\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing training and test loss across epochs for different batch sizes\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(batch_sizes, train_losses, label='Training Loss')\n",
        "plt.plot(batch_sizes, test_losses, label='Test Loss')\n",
        "plt.title('Training and Test Loss vs. Batch Size')\n",
        "plt.xlabel('Batch Size')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Visualizing test accuracy for different batch sizes\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(batch_sizes, accuracies, marker='o')\n",
        "plt.title('Test Accuracy vs. Batch Size')\n",
        "plt.xlabel('Batch Size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "NaCG5eU8p0Mi",
        "outputId": "37d4fdc3-cde1-4602-a520-3b8469b3d08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "- Batch size 64 seems to be the best one, though 128 is also a good choice. Batch 64 has a slightly higher training and testing loss than 128, but it has highest test accuracy among all the tested batch sizes"
      ],
      "metadata": {
        "id": "vmCFqgXZp-pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring Number of Epochs"
      ],
      "metadata": {
        "id": "YUTiYchBx_Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = [5, 10, 15, 20, 25, 30, 35]\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "accuracies = []\n",
        "train_loader = DataLoader(train_dataset, batch_size = 128, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 128, shuffle = False)\n",
        "\n",
        "# training the model for different epochs and recording training, test loss and test accuracy\n",
        "for epoch in epochs:\n",
        "    num_epochs = epoch\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(model, train_loader, optimizer, loss_type)\n",
        "        print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}')\n",
        "\n",
        "    test_loss, test_preds, test_labels = evaluate(model, test_loader, loss_type)\n",
        "    accuracy = np.mean(np.array(test_preds) == np.array(test_labels))\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    print(f'Epochs: {epoch+1}, Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xCi7KBeyBHa",
        "outputId": "e3f89e7c-c0e0-4480-9084-5b09364b557d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.0894\n",
            "Epoch: 2, Training Loss: 0.1299\n",
            "Epoch: 3, Training Loss: 0.1268\n",
            "Epoch: 4, Training Loss: 0.0954\n",
            "Epoch: 5, Training Loss: 0.0930\n",
            "Epochs: 5, Test Loss: 0.1645, Test Accuracy: 0.9573\n",
            "\n",
            "\n",
            "Epoch: 1, Training Loss: 0.1275\n",
            "Epoch: 2, Training Loss: 0.1041\n",
            "Epoch: 3, Training Loss: 0.1118\n",
            "Epoch: 4, Training Loss: 0.1345\n",
            "Epoch: 5, Training Loss: 0.0768\n",
            "Epoch: 6, Training Loss: 0.0655\n",
            "Epoch: 7, Training Loss: 0.0704\n",
            "Epoch: 8, Training Loss: 0.0663\n",
            "Epoch: 9, Training Loss: 0.1045\n",
            "Epoch: 10, Training Loss: 0.1085\n",
            "Epochs: 10, Test Loss: 0.1642, Test Accuracy: 0.9663\n",
            "\n",
            "\n",
            "Epoch: 1, Training Loss: 0.0555\n",
            "Epoch: 2, Training Loss: 0.0488\n",
            "Epoch: 3, Training Loss: 0.0814\n",
            "Epoch: 4, Training Loss: 0.0801\n",
            "Epoch: 5, Training Loss: 0.0541\n",
            "Epoch: 6, Training Loss: 0.0806\n",
            "Epoch: 7, Training Loss: 0.1203\n",
            "Epoch: 8, Training Loss: 0.0651\n",
            "Epoch: 9, Training Loss: 0.0624\n",
            "Epoch: 10, Training Loss: 0.0480\n",
            "Epoch: 11, Training Loss: 0.0559\n",
            "Epoch: 12, Training Loss: 0.0767\n",
            "Epoch: 13, Training Loss: 0.0397\n",
            "Epoch: 14, Training Loss: 0.0289\n",
            "Epoch: 15, Training Loss: 0.0459\n",
            "Epochs: 15, Test Loss: 0.2338, Test Accuracy: 0.9596\n",
            "\n",
            "\n",
            "Epoch: 1, Training Loss: 0.0364\n",
            "Epoch: 2, Training Loss: 0.0229\n",
            "Epoch: 3, Training Loss: 0.0493\n",
            "Epoch: 4, Training Loss: 0.0480\n",
            "Epoch: 5, Training Loss: 0.0421\n",
            "Epoch: 6, Training Loss: 0.0667\n",
            "Epoch: 7, Training Loss: 0.0993\n",
            "Epoch: 8, Training Loss: 0.0540\n",
            "Epoch: 9, Training Loss: 0.0508\n",
            "Epoch: 10, Training Loss: 0.0626\n",
            "Epoch: 11, Training Loss: 0.0531\n",
            "Epoch: 12, Training Loss: 0.0579\n",
            "Epoch: 13, Training Loss: 0.0457\n",
            "Epoch: 14, Training Loss: 0.0247\n",
            "Epoch: 15, Training Loss: 0.0788\n",
            "Epoch: 16, Training Loss: 0.0754\n",
            "Epoch: 17, Training Loss: 0.0578\n",
            "Epoch: 18, Training Loss: 0.0489\n",
            "Epoch: 19, Training Loss: 0.0445\n",
            "Epoch: 20, Training Loss: 0.0496\n",
            "Epochs: 20, Test Loss: 0.3594, Test Accuracy: 0.9528\n",
            "\n",
            "\n",
            "Epoch: 1, Training Loss: 0.0479\n",
            "Epoch: 2, Training Loss: 0.0749\n",
            "Epoch: 3, Training Loss: 0.0473\n",
            "Epoch: 4, Training Loss: 0.0262\n",
            "Epoch: 5, Training Loss: 0.0250\n",
            "Epoch: 6, Training Loss: 0.0577\n",
            "Epoch: 7, Training Loss: 0.0361\n",
            "Epoch: 8, Training Loss: 0.0433\n",
            "Epoch: 9, Training Loss: 0.0373\n",
            "Epoch: 10, Training Loss: 0.0451\n",
            "Epoch: 11, Training Loss: 0.0407\n",
            "Epoch: 12, Training Loss: 0.0497\n",
            "Epoch: 13, Training Loss: 0.0530\n",
            "Epoch: 14, Training Loss: 0.0351\n",
            "Epoch: 15, Training Loss: 0.0321\n",
            "Epoch: 16, Training Loss: 0.0487\n",
            "Epoch: 17, Training Loss: 0.0317\n",
            "Epoch: 18, Training Loss: 0.0411\n",
            "Epoch: 19, Training Loss: 0.0542\n",
            "Epoch: 20, Training Loss: 0.0682\n",
            "Epoch: 21, Training Loss: 0.0358\n",
            "Epoch: 22, Training Loss: 0.0718\n",
            "Epoch: 23, Training Loss: 0.0493\n",
            "Epoch: 24, Training Loss: 0.0432\n",
            "Epoch: 25, Training Loss: 0.0395\n",
            "Epochs: 25, Test Loss: 0.3422, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "Epoch: 1, Training Loss: 0.0483\n",
            "Epoch: 2, Training Loss: 0.0351\n",
            "Epoch: 3, Training Loss: 0.0433\n",
            "Epoch: 4, Training Loss: 0.0643\n",
            "Epoch: 5, Training Loss: 0.0559\n",
            "Epoch: 6, Training Loss: 0.0360\n",
            "Epoch: 7, Training Loss: 0.0211\n",
            "Epoch: 8, Training Loss: 0.0271\n",
            "Epoch: 9, Training Loss: 0.0542\n",
            "Epoch: 10, Training Loss: 0.0887\n",
            "Epoch: 11, Training Loss: 0.0911\n",
            "Epoch: 12, Training Loss: 0.0478\n",
            "Epoch: 13, Training Loss: 0.0328\n",
            "Epoch: 14, Training Loss: 0.0480\n",
            "Epoch: 15, Training Loss: 0.0291\n",
            "Epoch: 16, Training Loss: 0.0390\n",
            "Epoch: 17, Training Loss: 0.0591\n",
            "Epoch: 18, Training Loss: 0.0278\n",
            "Epoch: 19, Training Loss: 0.0271\n",
            "Epoch: 20, Training Loss: 0.0251\n",
            "Epoch: 21, Training Loss: 0.0178\n",
            "Epoch: 22, Training Loss: 0.0402\n",
            "Epoch: 23, Training Loss: 0.0325\n",
            "Epoch: 24, Training Loss: 0.0539\n",
            "Epoch: 25, Training Loss: 0.0374\n",
            "Epoch: 26, Training Loss: 0.0182\n",
            "Epoch: 27, Training Loss: 0.0361\n",
            "Epoch: 28, Training Loss: 0.0341\n",
            "Epoch: 29, Training Loss: 0.0434\n",
            "Epoch: 30, Training Loss: 0.0293\n",
            "Epochs: 30, Test Loss: 0.3796, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "Epoch: 1, Training Loss: 0.0225\n",
            "Epoch: 2, Training Loss: 0.0134\n",
            "Epoch: 3, Training Loss: 0.0346\n",
            "Epoch: 4, Training Loss: 0.0452\n",
            "Epoch: 5, Training Loss: 0.0821\n",
            "Epoch: 6, Training Loss: 0.1294\n",
            "Epoch: 7, Training Loss: 0.1164\n",
            "Epoch: 8, Training Loss: 0.1260\n",
            "Epoch: 9, Training Loss: 0.0874\n",
            "Epoch: 10, Training Loss: 0.0516\n",
            "Epoch: 11, Training Loss: 0.0635\n",
            "Epoch: 12, Training Loss: 0.0253\n",
            "Epoch: 13, Training Loss: 0.0371\n",
            "Epoch: 14, Training Loss: 0.0586\n",
            "Epoch: 15, Training Loss: 0.0601\n",
            "Epoch: 16, Training Loss: 0.0616\n",
            "Epoch: 17, Training Loss: 0.0696\n",
            "Epoch: 18, Training Loss: 0.0592\n",
            "Epoch: 19, Training Loss: 0.0588\n",
            "Epoch: 20, Training Loss: 0.0585\n",
            "Epoch: 21, Training Loss: 0.0455\n",
            "Epoch: 22, Training Loss: 0.0238\n",
            "Epoch: 23, Training Loss: 0.0325\n",
            "Epoch: 24, Training Loss: 0.0405\n",
            "Epoch: 25, Training Loss: 0.0221\n",
            "Epoch: 26, Training Loss: 0.0519\n",
            "Epoch: 27, Training Loss: 0.0352\n",
            "Epoch: 28, Training Loss: 0.0391\n",
            "Epoch: 29, Training Loss: 0.0849\n",
            "Epoch: 30, Training Loss: 0.0395\n",
            "Epoch: 31, Training Loss: 0.0949\n",
            "Epoch: 32, Training Loss: 0.0634\n",
            "Epoch: 33, Training Loss: 0.0650\n",
            "Epoch: 34, Training Loss: 0.0439\n",
            "Epoch: 35, Training Loss: 0.0341\n",
            "Epochs: 35, Test Loss: 0.2913, Test Accuracy: 0.9596\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing training and test loss for after training the model for different epochs\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, label='Training Loss')\n",
        "plt.plot(epochs, test_losses, label='Test Loss')\n",
        "plt.title('Training and Test Loss vs. Number of Epochs')\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Visualizing test accuracy for different epochs\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, accuracies, marker='o')\n",
        "plt.title('Test Accuracy vs. Number of Epochs')\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "3WhJlIQ5yLl7",
        "outputId": "b49e5be3-25d8-436a-cbc3-6ac6f4eabd04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation**:\n",
        "- Training Loss increases slightly, decreases, and remains low and relatively stable, indicating that model is learning well on training date without much overfitting.\n",
        "- Test accuracy varies considerably while remaining high overall (> 95%).\n",
        "- Noticeable drop in test accuracy at 15, 20 epochs, with lowest at 20 epochs.\n",
        "\n",
        "**Conclusion**:\n",
        "- 15 Epochs - Realtively same Test Accuracy to 10 epochs, Low Train Loss, Relatively Low Test Loss"
      ],
      "metadata": {
        "id": "6hVjD9x8zJ_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Testing Multiple Hyperparameters"
      ],
      "metadata": {
        "id": "Eyqq404T03F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "batch_sizes = [8, 16, 32, 64, 128]\n",
        "num_epochs_list = [5, 10, 15, 20, 25, 30]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "# training model for all combinations of batch sizes, epochs, and learning rates\n",
        "# finding the model with the best accuracy\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        for num_epochs in num_epochs_list:\n",
        "\n",
        "            model = SimpleNN(embedding_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, output_dim)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                train_loss = train(model, train_loader, optimizer, loss_type)\n",
        "                print(f'LR: {lr}, Batch Size: {batch_size}, Epoch: {epoch+1}, Training Loss: {train_loss:.4f}')\n",
        "\n",
        "            test_loss, test_preds, test_labels = evaluate(model, test_loader, loss_type)\n",
        "            accuracy = np.mean(np.array(test_preds) == np.array(test_labels))\n",
        "\n",
        "            print(f'LR: {lr}, Batch Size: {batch_size}, Num Epochs: {num_epochs}, Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "            print(\"\\n\")\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = {'learning_rate': lr, 'batch_size': batch_size, 'num_epochs': num_epochs}\n",
        "\n",
        "print(f'Best Accuracy: {best_accuracy:.4f}')\n",
        "print(f'Best Hyperparameters: {best_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ukl6r1Y-1QyB",
        "outputId": "07d89779-c3d2-4a02-a5eb-7051dfe194f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR: 0.001, Batch Size: 8, Epoch: 1, Training Loss: 0.9669\n",
            "LR: 0.001, Batch Size: 8, Epoch: 2, Training Loss: 0.2679\n",
            "LR: 0.001, Batch Size: 8, Epoch: 3, Training Loss: 0.2033\n",
            "LR: 0.001, Batch Size: 8, Epoch: 4, Training Loss: 0.1529\n",
            "LR: 0.001, Batch Size: 8, Epoch: 5, Training Loss: 0.1577\n",
            "LR: 0.001, Batch Size: 8, Num Epochs: 5, Test Loss: 0.1501, Test Accuracy: 0.9528\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 8, Epoch: 1, Training Loss: 1.0491\n",
            "LR: 0.001, Batch Size: 8, Epoch: 2, Training Loss: 0.3178\n",
            "LR: 0.001, Batch Size: 8, Epoch: 3, Training Loss: 0.2146\n",
            "LR: 0.001, Batch Size: 8, Epoch: 4, Training Loss: 0.1609\n",
            "LR: 0.001, Batch Size: 8, Epoch: 5, Training Loss: 0.1604\n",
            "LR: 0.001, Batch Size: 8, Epoch: 6, Training Loss: 0.1320\n",
            "LR: 0.001, Batch Size: 8, Epoch: 7, Training Loss: 0.1079\n",
            "LR: 0.001, Batch Size: 8, Epoch: 8, Training Loss: 0.0881\n",
            "LR: 0.001, Batch Size: 8, Epoch: 9, Training Loss: 0.0910\n",
            "LR: 0.001, Batch Size: 8, Epoch: 10, Training Loss: 0.0779\n",
            "LR: 0.001, Batch Size: 8, Num Epochs: 10, Test Loss: 0.1412, Test Accuracy: 0.9596\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 8, Epoch: 1, Training Loss: 1.0560\n",
            "LR: 0.001, Batch Size: 8, Epoch: 2, Training Loss: 0.3108\n",
            "LR: 0.001, Batch Size: 8, Epoch: 3, Training Loss: 0.2047\n",
            "LR: 0.001, Batch Size: 8, Epoch: 4, Training Loss: 0.1544\n",
            "LR: 0.001, Batch Size: 8, Epoch: 5, Training Loss: 0.1609\n",
            "LR: 0.001, Batch Size: 8, Epoch: 6, Training Loss: 0.1169\n",
            "LR: 0.001, Batch Size: 8, Epoch: 7, Training Loss: 0.1103\n",
            "LR: 0.001, Batch Size: 8, Epoch: 8, Training Loss: 0.1037\n",
            "LR: 0.001, Batch Size: 8, Epoch: 9, Training Loss: 0.0907\n",
            "LR: 0.001, Batch Size: 8, Epoch: 10, Training Loss: 0.0948\n",
            "LR: 0.001, Batch Size: 8, Epoch: 11, Training Loss: 0.0792\n",
            "LR: 0.001, Batch Size: 8, Epoch: 12, Training Loss: 0.0849\n",
            "LR: 0.001, Batch Size: 8, Epoch: 13, Training Loss: 0.0647\n",
            "LR: 0.001, Batch Size: 8, Epoch: 14, Training Loss: 0.0657\n",
            "LR: 0.001, Batch Size: 8, Epoch: 15, Training Loss: 0.0627\n",
            "LR: 0.001, Batch Size: 8, Num Epochs: 15, Test Loss: 0.1930, Test Accuracy: 0.9393\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 8, Epoch: 1, Training Loss: 0.9844\n",
            "LR: 0.001, Batch Size: 8, Epoch: 2, Training Loss: 0.3627\n",
            "LR: 0.001, Batch Size: 8, Epoch: 3, Training Loss: 0.2340\n",
            "LR: 0.001, Batch Size: 8, Epoch: 4, Training Loss: 0.1759\n",
            "LR: 0.001, Batch Size: 8, Epoch: 5, Training Loss: 0.1503\n",
            "LR: 0.001, Batch Size: 8, Epoch: 6, Training Loss: 0.1218\n",
            "LR: 0.001, Batch Size: 8, Epoch: 7, Training Loss: 0.1124\n",
            "LR: 0.001, Batch Size: 8, Epoch: 8, Training Loss: 0.0985\n",
            "LR: 0.001, Batch Size: 8, Epoch: 9, Training Loss: 0.0941\n",
            "LR: 0.001, Batch Size: 8, Epoch: 10, Training Loss: 0.1164\n",
            "LR: 0.001, Batch Size: 8, Epoch: 11, Training Loss: 0.1226\n",
            "LR: 0.001, Batch Size: 8, Epoch: 12, Training Loss: 0.0729\n",
            "LR: 0.001, Batch Size: 8, Epoch: 13, Training Loss: 0.0629\n",
            "LR: 0.001, Batch Size: 8, Epoch: 14, Training Loss: 0.0790\n",
            "LR: 0.001, Batch Size: 8, Epoch: 15, Training Loss: 0.0385\n",
            "LR: 0.001, Batch Size: 8, Epoch: 16, Training Loss: 0.0735\n",
            "LR: 0.001, Batch Size: 8, Epoch: 17, Training Loss: 0.0523\n",
            "LR: 0.001, Batch Size: 8, Epoch: 18, Training Loss: 0.0670\n",
            "LR: 0.001, Batch Size: 8, Epoch: 19, Training Loss: 0.0606\n",
            "LR: 0.001, Batch Size: 8, Epoch: 20, Training Loss: 0.0456\n",
            "LR: 0.001, Batch Size: 8, Num Epochs: 20, Test Loss: 0.2405, Test Accuracy: 0.9640\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 8, Epoch: 1, Training Loss: 0.9940\n",
            "LR: 0.001, Batch Size: 8, Epoch: 2, Training Loss: 0.2912\n",
            "LR: 0.001, Batch Size: 8, Epoch: 3, Training Loss: 0.1893\n",
            "LR: 0.001, Batch Size: 8, Epoch: 4, Training Loss: 0.1686\n",
            "LR: 0.001, Batch Size: 8, Epoch: 5, Training Loss: 0.1226\n",
            "LR: 0.001, Batch Size: 8, Epoch: 6, Training Loss: 0.1264\n",
            "LR: 0.001, Batch Size: 8, Epoch: 7, Training Loss: 0.1166\n",
            "LR: 0.001, Batch Size: 8, Epoch: 8, Training Loss: 0.1159\n",
            "LR: 0.001, Batch Size: 8, Epoch: 9, Training Loss: 0.1095\n",
            "LR: 0.001, Batch Size: 8, Epoch: 10, Training Loss: 0.0902\n",
            "LR: 0.001, Batch Size: 8, Epoch: 11, Training Loss: 0.0903\n",
            "LR: 0.001, Batch Size: 8, Epoch: 12, Training Loss: 0.0837\n",
            "LR: 0.001, Batch Size: 8, Epoch: 13, Training Loss: 0.0695\n",
            "LR: 0.001, Batch Size: 8, Epoch: 14, Training Loss: 0.0684\n",
            "LR: 0.001, Batch Size: 8, Epoch: 15, Training Loss: 0.0566\n",
            "LR: 0.001, Batch Size: 8, Epoch: 16, Training Loss: 0.0559\n",
            "LR: 0.001, Batch Size: 8, Epoch: 17, Training Loss: 0.0490\n",
            "LR: 0.001, Batch Size: 8, Epoch: 18, Training Loss: 0.0538\n",
            "LR: 0.001, Batch Size: 8, Epoch: 19, Training Loss: 0.0524\n",
            "LR: 0.001, Batch Size: 8, Epoch: 20, Training Loss: 0.0504\n",
            "LR: 0.001, Batch Size: 8, Epoch: 21, Training Loss: 0.0465\n",
            "LR: 0.001, Batch Size: 8, Epoch: 22, Training Loss: 0.0615\n",
            "LR: 0.001, Batch Size: 8, Epoch: 23, Training Loss: 0.0276\n",
            "LR: 0.001, Batch Size: 8, Epoch: 24, Training Loss: 0.0404\n",
            "LR: 0.001, Batch Size: 8, Epoch: 25, Training Loss: 0.0451\n",
            "LR: 0.001, Batch Size: 8, Num Epochs: 25, Test Loss: 0.2500, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 8, Epoch: 1, Training Loss: 1.0351\n",
            "LR: 0.001, Batch Size: 8, Epoch: 2, Training Loss: 0.3224\n",
            "LR: 0.001, Batch Size: 8, Epoch: 3, Training Loss: 0.2022\n",
            "LR: 0.001, Batch Size: 8, Epoch: 4, Training Loss: 0.1768\n",
            "LR: 0.001, Batch Size: 8, Epoch: 5, Training Loss: 0.1463\n",
            "LR: 0.001, Batch Size: 8, Epoch: 6, Training Loss: 0.1016\n",
            "LR: 0.001, Batch Size: 8, Epoch: 7, Training Loss: 0.1237\n",
            "LR: 0.001, Batch Size: 8, Epoch: 8, Training Loss: 0.1069\n",
            "LR: 0.001, Batch Size: 8, Epoch: 9, Training Loss: 0.1066\n",
            "LR: 0.001, Batch Size: 8, Epoch: 10, Training Loss: 0.0652\n",
            "LR: 0.001, Batch Size: 8, Epoch: 11, Training Loss: 0.0710\n",
            "LR: 0.001, Batch Size: 8, Epoch: 12, Training Loss: 0.0663\n",
            "LR: 0.001, Batch Size: 8, Epoch: 13, Training Loss: 0.0899\n",
            "LR: 0.001, Batch Size: 8, Epoch: 14, Training Loss: 0.0792\n",
            "LR: 0.001, Batch Size: 8, Epoch: 15, Training Loss: 0.0711\n",
            "LR: 0.001, Batch Size: 8, Epoch: 16, Training Loss: 0.0332\n",
            "LR: 0.001, Batch Size: 8, Epoch: 17, Training Loss: 0.0514\n",
            "LR: 0.001, Batch Size: 8, Epoch: 18, Training Loss: 0.0454\n",
            "LR: 0.001, Batch Size: 8, Epoch: 19, Training Loss: 0.0608\n",
            "LR: 0.001, Batch Size: 8, Epoch: 20, Training Loss: 0.0590\n",
            "LR: 0.001, Batch Size: 8, Epoch: 21, Training Loss: 0.0466\n",
            "LR: 0.001, Batch Size: 8, Epoch: 22, Training Loss: 0.0561\n",
            "LR: 0.001, Batch Size: 8, Epoch: 23, Training Loss: 0.0289\n",
            "LR: 0.001, Batch Size: 8, Epoch: 24, Training Loss: 0.0386\n",
            "LR: 0.001, Batch Size: 8, Epoch: 25, Training Loss: 0.0510\n",
            "LR: 0.001, Batch Size: 8, Epoch: 26, Training Loss: 0.0440\n",
            "LR: 0.001, Batch Size: 8, Epoch: 27, Training Loss: 0.0521\n",
            "LR: 0.001, Batch Size: 8, Epoch: 28, Training Loss: 0.0113\n",
            "LR: 0.001, Batch Size: 8, Epoch: 29, Training Loss: 0.0521\n",
            "LR: 0.001, Batch Size: 8, Epoch: 30, Training Loss: 0.0447\n",
            "LR: 0.001, Batch Size: 8, Num Epochs: 30, Test Loss: 0.2399, Test Accuracy: 0.9528\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 16, Epoch: 1, Training Loss: 1.2967\n",
            "LR: 0.001, Batch Size: 16, Epoch: 2, Training Loss: 0.5005\n",
            "LR: 0.001, Batch Size: 16, Epoch: 3, Training Loss: 0.2861\n",
            "LR: 0.001, Batch Size: 16, Epoch: 4, Training Loss: 0.1925\n",
            "LR: 0.001, Batch Size: 16, Epoch: 5, Training Loss: 0.1795\n",
            "LR: 0.001, Batch Size: 16, Num Epochs: 5, Test Loss: 0.1482, Test Accuracy: 0.9506\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 16, Epoch: 1, Training Loss: 1.2601\n",
            "LR: 0.001, Batch Size: 16, Epoch: 2, Training Loss: 0.4582\n",
            "LR: 0.001, Batch Size: 16, Epoch: 3, Training Loss: 0.2636\n",
            "LR: 0.001, Batch Size: 16, Epoch: 4, Training Loss: 0.1962\n",
            "LR: 0.001, Batch Size: 16, Epoch: 5, Training Loss: 0.1607\n",
            "LR: 0.001, Batch Size: 16, Epoch: 6, Training Loss: 0.1384\n",
            "LR: 0.001, Batch Size: 16, Epoch: 7, Training Loss: 0.1240\n",
            "LR: 0.001, Batch Size: 16, Epoch: 8, Training Loss: 0.1278\n",
            "LR: 0.001, Batch Size: 16, Epoch: 9, Training Loss: 0.0954\n",
            "LR: 0.001, Batch Size: 16, Epoch: 10, Training Loss: 0.1094\n",
            "LR: 0.001, Batch Size: 16, Num Epochs: 10, Test Loss: 0.1629, Test Accuracy: 0.9573\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 16, Epoch: 1, Training Loss: 1.2053\n",
            "LR: 0.001, Batch Size: 16, Epoch: 2, Training Loss: 0.3595\n",
            "LR: 0.001, Batch Size: 16, Epoch: 3, Training Loss: 0.2218\n",
            "LR: 0.001, Batch Size: 16, Epoch: 4, Training Loss: 0.1985\n",
            "LR: 0.001, Batch Size: 16, Epoch: 5, Training Loss: 0.1801\n",
            "LR: 0.001, Batch Size: 16, Epoch: 6, Training Loss: 0.1427\n",
            "LR: 0.001, Batch Size: 16, Epoch: 7, Training Loss: 0.1159\n",
            "LR: 0.001, Batch Size: 16, Epoch: 8, Training Loss: 0.1117\n",
            "LR: 0.001, Batch Size: 16, Epoch: 9, Training Loss: 0.1039\n",
            "LR: 0.001, Batch Size: 16, Epoch: 10, Training Loss: 0.0968\n",
            "LR: 0.001, Batch Size: 16, Epoch: 11, Training Loss: 0.0765\n",
            "LR: 0.001, Batch Size: 16, Epoch: 12, Training Loss: 0.0968\n",
            "LR: 0.001, Batch Size: 16, Epoch: 13, Training Loss: 0.0738\n",
            "LR: 0.001, Batch Size: 16, Epoch: 14, Training Loss: 0.0677\n",
            "LR: 0.001, Batch Size: 16, Epoch: 15, Training Loss: 0.0636\n",
            "LR: 0.001, Batch Size: 16, Num Epochs: 15, Test Loss: 0.1727, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 16, Epoch: 1, Training Loss: 1.2477\n",
            "LR: 0.001, Batch Size: 16, Epoch: 2, Training Loss: 0.4317\n",
            "LR: 0.001, Batch Size: 16, Epoch: 3, Training Loss: 0.2479\n",
            "LR: 0.001, Batch Size: 16, Epoch: 4, Training Loss: 0.2014\n",
            "LR: 0.001, Batch Size: 16, Epoch: 5, Training Loss: 0.1480\n",
            "LR: 0.001, Batch Size: 16, Epoch: 6, Training Loss: 0.1464\n",
            "LR: 0.001, Batch Size: 16, Epoch: 7, Training Loss: 0.1266\n",
            "LR: 0.001, Batch Size: 16, Epoch: 8, Training Loss: 0.1056\n",
            "LR: 0.001, Batch Size: 16, Epoch: 9, Training Loss: 0.0979\n",
            "LR: 0.001, Batch Size: 16, Epoch: 10, Training Loss: 0.0816\n",
            "LR: 0.001, Batch Size: 16, Epoch: 11, Training Loss: 0.0953\n",
            "LR: 0.001, Batch Size: 16, Epoch: 12, Training Loss: 0.0973\n",
            "LR: 0.001, Batch Size: 16, Epoch: 13, Training Loss: 0.0736\n",
            "LR: 0.001, Batch Size: 16, Epoch: 14, Training Loss: 0.0667\n",
            "LR: 0.001, Batch Size: 16, Epoch: 15, Training Loss: 0.0551\n",
            "LR: 0.001, Batch Size: 16, Epoch: 16, Training Loss: 0.0667\n",
            "LR: 0.001, Batch Size: 16, Epoch: 17, Training Loss: 0.0602\n",
            "LR: 0.001, Batch Size: 16, Epoch: 18, Training Loss: 0.0536\n",
            "LR: 0.001, Batch Size: 16, Epoch: 19, Training Loss: 0.0520\n",
            "LR: 0.001, Batch Size: 16, Epoch: 20, Training Loss: 0.0311\n",
            "LR: 0.001, Batch Size: 16, Num Epochs: 20, Test Loss: 0.1911, Test Accuracy: 0.9618\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 16, Epoch: 1, Training Loss: 1.3245\n",
            "LR: 0.001, Batch Size: 16, Epoch: 2, Training Loss: 0.5109\n",
            "LR: 0.001, Batch Size: 16, Epoch: 3, Training Loss: 0.2671\n",
            "LR: 0.001, Batch Size: 16, Epoch: 4, Training Loss: 0.1734\n",
            "LR: 0.001, Batch Size: 16, Epoch: 5, Training Loss: 0.1597\n",
            "LR: 0.001, Batch Size: 16, Epoch: 6, Training Loss: 0.1550\n",
            "LR: 0.001, Batch Size: 16, Epoch: 7, Training Loss: 0.1374\n",
            "LR: 0.001, Batch Size: 16, Epoch: 8, Training Loss: 0.1133\n",
            "LR: 0.001, Batch Size: 16, Epoch: 9, Training Loss: 0.0923\n",
            "LR: 0.001, Batch Size: 16, Epoch: 10, Training Loss: 0.0929\n",
            "LR: 0.001, Batch Size: 16, Epoch: 11, Training Loss: 0.1070\n",
            "LR: 0.001, Batch Size: 16, Epoch: 12, Training Loss: 0.0805\n",
            "LR: 0.001, Batch Size: 16, Epoch: 13, Training Loss: 0.0804\n",
            "LR: 0.001, Batch Size: 16, Epoch: 14, Training Loss: 0.0711\n",
            "LR: 0.001, Batch Size: 16, Epoch: 15, Training Loss: 0.0549\n",
            "LR: 0.001, Batch Size: 16, Epoch: 16, Training Loss: 0.0670\n",
            "LR: 0.001, Batch Size: 16, Epoch: 17, Training Loss: 0.0550\n",
            "LR: 0.001, Batch Size: 16, Epoch: 18, Training Loss: 0.0440\n",
            "LR: 0.001, Batch Size: 16, Epoch: 19, Training Loss: 0.0557\n",
            "LR: 0.001, Batch Size: 16, Epoch: 20, Training Loss: 0.0478\n",
            "LR: 0.001, Batch Size: 16, Epoch: 21, Training Loss: 0.0543\n",
            "LR: 0.001, Batch Size: 16, Epoch: 22, Training Loss: 0.0553\n",
            "LR: 0.001, Batch Size: 16, Epoch: 23, Training Loss: 0.0477\n",
            "LR: 0.001, Batch Size: 16, Epoch: 24, Training Loss: 0.0324\n",
            "LR: 0.001, Batch Size: 16, Epoch: 25, Training Loss: 0.0422\n",
            "LR: 0.001, Batch Size: 16, Num Epochs: 25, Test Loss: 0.2031, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 16, Epoch: 1, Training Loss: 1.2779\n",
            "LR: 0.001, Batch Size: 16, Epoch: 2, Training Loss: 0.4042\n",
            "LR: 0.001, Batch Size: 16, Epoch: 3, Training Loss: 0.2450\n",
            "LR: 0.001, Batch Size: 16, Epoch: 4, Training Loss: 0.1876\n",
            "LR: 0.001, Batch Size: 16, Epoch: 5, Training Loss: 0.1676\n",
            "LR: 0.001, Batch Size: 16, Epoch: 6, Training Loss: 0.1340\n",
            "LR: 0.001, Batch Size: 16, Epoch: 7, Training Loss: 0.1260\n",
            "LR: 0.001, Batch Size: 16, Epoch: 8, Training Loss: 0.0962\n",
            "LR: 0.001, Batch Size: 16, Epoch: 9, Training Loss: 0.0890\n",
            "LR: 0.001, Batch Size: 16, Epoch: 10, Training Loss: 0.1037\n",
            "LR: 0.001, Batch Size: 16, Epoch: 11, Training Loss: 0.0790\n",
            "LR: 0.001, Batch Size: 16, Epoch: 12, Training Loss: 0.0626\n",
            "LR: 0.001, Batch Size: 16, Epoch: 13, Training Loss: 0.0698\n",
            "LR: 0.001, Batch Size: 16, Epoch: 14, Training Loss: 0.0724\n",
            "LR: 0.001, Batch Size: 16, Epoch: 15, Training Loss: 0.0765\n",
            "LR: 0.001, Batch Size: 16, Epoch: 16, Training Loss: 0.0798\n",
            "LR: 0.001, Batch Size: 16, Epoch: 17, Training Loss: 0.0494\n",
            "LR: 0.001, Batch Size: 16, Epoch: 18, Training Loss: 0.0488\n",
            "LR: 0.001, Batch Size: 16, Epoch: 19, Training Loss: 0.0599\n",
            "LR: 0.001, Batch Size: 16, Epoch: 20, Training Loss: 0.0323\n",
            "LR: 0.001, Batch Size: 16, Epoch: 21, Training Loss: 0.0432\n",
            "LR: 0.001, Batch Size: 16, Epoch: 22, Training Loss: 0.0375\n",
            "LR: 0.001, Batch Size: 16, Epoch: 23, Training Loss: 0.0354\n",
            "LR: 0.001, Batch Size: 16, Epoch: 24, Training Loss: 0.0360\n",
            "LR: 0.001, Batch Size: 16, Epoch: 25, Training Loss: 0.0280\n",
            "LR: 0.001, Batch Size: 16, Epoch: 26, Training Loss: 0.0260\n",
            "LR: 0.001, Batch Size: 16, Epoch: 27, Training Loss: 0.0410\n",
            "LR: 0.001, Batch Size: 16, Epoch: 28, Training Loss: 0.0290\n",
            "LR: 0.001, Batch Size: 16, Epoch: 29, Training Loss: 0.0368\n",
            "LR: 0.001, Batch Size: 16, Epoch: 30, Training Loss: 0.0421\n",
            "LR: 0.001, Batch Size: 16, Num Epochs: 30, Test Loss: 0.1878, Test Accuracy: 0.9618\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 32, Epoch: 1, Training Loss: 1.4605\n",
            "LR: 0.001, Batch Size: 32, Epoch: 2, Training Loss: 0.7187\n",
            "LR: 0.001, Batch Size: 32, Epoch: 3, Training Loss: 0.3559\n",
            "LR: 0.001, Batch Size: 32, Epoch: 4, Training Loss: 0.2352\n",
            "LR: 0.001, Batch Size: 32, Epoch: 5, Training Loss: 0.2165\n",
            "LR: 0.001, Batch Size: 32, Num Epochs: 5, Test Loss: 0.1792, Test Accuracy: 0.9348\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 32, Epoch: 1, Training Loss: 1.4342\n",
            "LR: 0.001, Batch Size: 32, Epoch: 2, Training Loss: 0.6710\n",
            "LR: 0.001, Batch Size: 32, Epoch: 3, Training Loss: 0.3141\n",
            "LR: 0.001, Batch Size: 32, Epoch: 4, Training Loss: 0.2467\n",
            "LR: 0.001, Batch Size: 32, Epoch: 5, Training Loss: 0.1774\n",
            "LR: 0.001, Batch Size: 32, Epoch: 6, Training Loss: 0.1754\n",
            "LR: 0.001, Batch Size: 32, Epoch: 7, Training Loss: 0.1404\n",
            "LR: 0.001, Batch Size: 32, Epoch: 8, Training Loss: 0.1365\n",
            "LR: 0.001, Batch Size: 32, Epoch: 9, Training Loss: 0.1250\n",
            "LR: 0.001, Batch Size: 32, Epoch: 10, Training Loss: 0.1199\n",
            "LR: 0.001, Batch Size: 32, Num Epochs: 10, Test Loss: 0.1210, Test Accuracy: 0.9618\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 32, Epoch: 1, Training Loss: 1.4464\n",
            "LR: 0.001, Batch Size: 32, Epoch: 2, Training Loss: 0.6373\n",
            "LR: 0.001, Batch Size: 32, Epoch: 3, Training Loss: 0.3332\n",
            "LR: 0.001, Batch Size: 32, Epoch: 4, Training Loss: 0.2460\n",
            "LR: 0.001, Batch Size: 32, Epoch: 5, Training Loss: 0.1920\n",
            "LR: 0.001, Batch Size: 32, Epoch: 6, Training Loss: 0.1680\n",
            "LR: 0.001, Batch Size: 32, Epoch: 7, Training Loss: 0.1545\n",
            "LR: 0.001, Batch Size: 32, Epoch: 8, Training Loss: 0.1356\n",
            "LR: 0.001, Batch Size: 32, Epoch: 9, Training Loss: 0.1180\n",
            "LR: 0.001, Batch Size: 32, Epoch: 10, Training Loss: 0.0957\n",
            "LR: 0.001, Batch Size: 32, Epoch: 11, Training Loss: 0.0899\n",
            "LR: 0.001, Batch Size: 32, Epoch: 12, Training Loss: 0.0842\n",
            "LR: 0.001, Batch Size: 32, Epoch: 13, Training Loss: 0.0846\n",
            "LR: 0.001, Batch Size: 32, Epoch: 14, Training Loss: 0.0975\n",
            "LR: 0.001, Batch Size: 32, Epoch: 15, Training Loss: 0.0639\n",
            "LR: 0.001, Batch Size: 32, Num Epochs: 15, Test Loss: 0.1618, Test Accuracy: 0.9483\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 32, Epoch: 1, Training Loss: 1.4365\n",
            "LR: 0.001, Batch Size: 32, Epoch: 2, Training Loss: 0.6797\n",
            "LR: 0.001, Batch Size: 32, Epoch: 3, Training Loss: 0.4031\n",
            "LR: 0.001, Batch Size: 32, Epoch: 4, Training Loss: 0.2612\n",
            "LR: 0.001, Batch Size: 32, Epoch: 5, Training Loss: 0.2030\n",
            "LR: 0.001, Batch Size: 32, Epoch: 6, Training Loss: 0.1791\n",
            "LR: 0.001, Batch Size: 32, Epoch: 7, Training Loss: 0.1439\n",
            "LR: 0.001, Batch Size: 32, Epoch: 8, Training Loss: 0.1243\n",
            "LR: 0.001, Batch Size: 32, Epoch: 9, Training Loss: 0.1101\n",
            "LR: 0.001, Batch Size: 32, Epoch: 10, Training Loss: 0.1063\n",
            "LR: 0.001, Batch Size: 32, Epoch: 11, Training Loss: 0.0923\n",
            "LR: 0.001, Batch Size: 32, Epoch: 12, Training Loss: 0.0784\n",
            "LR: 0.001, Batch Size: 32, Epoch: 13, Training Loss: 0.0737\n",
            "LR: 0.001, Batch Size: 32, Epoch: 14, Training Loss: 0.0868\n",
            "LR: 0.001, Batch Size: 32, Epoch: 15, Training Loss: 0.0700\n",
            "LR: 0.001, Batch Size: 32, Epoch: 16, Training Loss: 0.0471\n",
            "LR: 0.001, Batch Size: 32, Epoch: 17, Training Loss: 0.0585\n",
            "LR: 0.001, Batch Size: 32, Epoch: 18, Training Loss: 0.0426\n",
            "LR: 0.001, Batch Size: 32, Epoch: 19, Training Loss: 0.0462\n",
            "LR: 0.001, Batch Size: 32, Epoch: 20, Training Loss: 0.0427\n",
            "LR: 0.001, Batch Size: 32, Num Epochs: 20, Test Loss: 0.1623, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 32, Epoch: 1, Training Loss: 1.4967\n",
            "LR: 0.001, Batch Size: 32, Epoch: 2, Training Loss: 0.7688\n",
            "LR: 0.001, Batch Size: 32, Epoch: 3, Training Loss: 0.3793\n",
            "LR: 0.001, Batch Size: 32, Epoch: 4, Training Loss: 0.2646\n",
            "LR: 0.001, Batch Size: 32, Epoch: 5, Training Loss: 0.2053\n",
            "LR: 0.001, Batch Size: 32, Epoch: 6, Training Loss: 0.1747\n",
            "LR: 0.001, Batch Size: 32, Epoch: 7, Training Loss: 0.1564\n",
            "LR: 0.001, Batch Size: 32, Epoch: 8, Training Loss: 0.1278\n",
            "LR: 0.001, Batch Size: 32, Epoch: 9, Training Loss: 0.1192\n",
            "LR: 0.001, Batch Size: 32, Epoch: 10, Training Loss: 0.0957\n",
            "LR: 0.001, Batch Size: 32, Epoch: 11, Training Loss: 0.1127\n",
            "LR: 0.001, Batch Size: 32, Epoch: 12, Training Loss: 0.0915\n",
            "LR: 0.001, Batch Size: 32, Epoch: 13, Training Loss: 0.0841\n",
            "LR: 0.001, Batch Size: 32, Epoch: 14, Training Loss: 0.0679\n",
            "LR: 0.001, Batch Size: 32, Epoch: 15, Training Loss: 0.0709\n",
            "LR: 0.001, Batch Size: 32, Epoch: 16, Training Loss: 0.0500\n",
            "LR: 0.001, Batch Size: 32, Epoch: 17, Training Loss: 0.0584\n",
            "LR: 0.001, Batch Size: 32, Epoch: 18, Training Loss: 0.0468\n",
            "LR: 0.001, Batch Size: 32, Epoch: 19, Training Loss: 0.0675\n",
            "LR: 0.001, Batch Size: 32, Epoch: 20, Training Loss: 0.0515\n",
            "LR: 0.001, Batch Size: 32, Epoch: 21, Training Loss: 0.0511\n",
            "LR: 0.001, Batch Size: 32, Epoch: 22, Training Loss: 0.0444\n",
            "LR: 0.001, Batch Size: 32, Epoch: 23, Training Loss: 0.0428\n",
            "LR: 0.001, Batch Size: 32, Epoch: 24, Training Loss: 0.0354\n",
            "LR: 0.001, Batch Size: 32, Epoch: 25, Training Loss: 0.0424\n",
            "LR: 0.001, Batch Size: 32, Num Epochs: 25, Test Loss: 0.1705, Test Accuracy: 0.9596\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 32, Epoch: 1, Training Loss: 1.5329\n",
            "LR: 0.001, Batch Size: 32, Epoch: 2, Training Loss: 0.7785\n",
            "LR: 0.001, Batch Size: 32, Epoch: 3, Training Loss: 0.3601\n",
            "LR: 0.001, Batch Size: 32, Epoch: 4, Training Loss: 0.2604\n",
            "LR: 0.001, Batch Size: 32, Epoch: 5, Training Loss: 0.2076\n",
            "LR: 0.001, Batch Size: 32, Epoch: 6, Training Loss: 0.1653\n",
            "LR: 0.001, Batch Size: 32, Epoch: 7, Training Loss: 0.1763\n",
            "LR: 0.001, Batch Size: 32, Epoch: 8, Training Loss: 0.1263\n",
            "LR: 0.001, Batch Size: 32, Epoch: 9, Training Loss: 0.1136\n",
            "LR: 0.001, Batch Size: 32, Epoch: 10, Training Loss: 0.1099\n",
            "LR: 0.001, Batch Size: 32, Epoch: 11, Training Loss: 0.1025\n",
            "LR: 0.001, Batch Size: 32, Epoch: 12, Training Loss: 0.0843\n",
            "LR: 0.001, Batch Size: 32, Epoch: 13, Training Loss: 0.0790\n",
            "LR: 0.001, Batch Size: 32, Epoch: 14, Training Loss: 0.0849\n",
            "LR: 0.001, Batch Size: 32, Epoch: 15, Training Loss: 0.0792\n",
            "LR: 0.001, Batch Size: 32, Epoch: 16, Training Loss: 0.0506\n",
            "LR: 0.001, Batch Size: 32, Epoch: 17, Training Loss: 0.0428\n",
            "LR: 0.001, Batch Size: 32, Epoch: 18, Training Loss: 0.0571\n",
            "LR: 0.001, Batch Size: 32, Epoch: 19, Training Loss: 0.0586\n",
            "LR: 0.001, Batch Size: 32, Epoch: 20, Training Loss: 0.0303\n",
            "LR: 0.001, Batch Size: 32, Epoch: 21, Training Loss: 0.0304\n",
            "LR: 0.001, Batch Size: 32, Epoch: 22, Training Loss: 0.0257\n",
            "LR: 0.001, Batch Size: 32, Epoch: 23, Training Loss: 0.0358\n",
            "LR: 0.001, Batch Size: 32, Epoch: 24, Training Loss: 0.0243\n",
            "LR: 0.001, Batch Size: 32, Epoch: 25, Training Loss: 0.0301\n",
            "LR: 0.001, Batch Size: 32, Epoch: 26, Training Loss: 0.0315\n",
            "LR: 0.001, Batch Size: 32, Epoch: 27, Training Loss: 0.0270\n",
            "LR: 0.001, Batch Size: 32, Epoch: 28, Training Loss: 0.0212\n",
            "LR: 0.001, Batch Size: 32, Epoch: 29, Training Loss: 0.0461\n",
            "LR: 0.001, Batch Size: 32, Epoch: 30, Training Loss: 0.0302\n",
            "LR: 0.001, Batch Size: 32, Num Epochs: 30, Test Loss: 0.2013, Test Accuracy: 0.9640\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 64, Epoch: 1, Training Loss: 1.5689\n",
            "LR: 0.001, Batch Size: 64, Epoch: 2, Training Loss: 1.2047\n",
            "LR: 0.001, Batch Size: 64, Epoch: 3, Training Loss: 0.7070\n",
            "LR: 0.001, Batch Size: 64, Epoch: 4, Training Loss: 0.4005\n",
            "LR: 0.001, Batch Size: 64, Epoch: 5, Training Loss: 0.2703\n",
            "LR: 0.001, Batch Size: 64, Num Epochs: 5, Test Loss: 0.1654, Test Accuracy: 0.9393\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 64, Epoch: 1, Training Loss: 1.5813\n",
            "LR: 0.001, Batch Size: 64, Epoch: 2, Training Loss: 1.1843\n",
            "LR: 0.001, Batch Size: 64, Epoch: 3, Training Loss: 0.7092\n",
            "LR: 0.001, Batch Size: 64, Epoch: 4, Training Loss: 0.4241\n",
            "LR: 0.001, Batch Size: 64, Epoch: 5, Training Loss: 0.3087\n",
            "LR: 0.001, Batch Size: 64, Epoch: 6, Training Loss: 0.2329\n",
            "LR: 0.001, Batch Size: 64, Epoch: 7, Training Loss: 0.2218\n",
            "LR: 0.001, Batch Size: 64, Epoch: 8, Training Loss: 0.1695\n",
            "LR: 0.001, Batch Size: 64, Epoch: 9, Training Loss: 0.1636\n",
            "LR: 0.001, Batch Size: 64, Epoch: 10, Training Loss: 0.1468\n",
            "LR: 0.001, Batch Size: 64, Num Epochs: 10, Test Loss: 0.1722, Test Accuracy: 0.9438\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 64, Epoch: 1, Training Loss: 1.5849\n",
            "LR: 0.001, Batch Size: 64, Epoch: 2, Training Loss: 1.1979\n",
            "LR: 0.001, Batch Size: 64, Epoch: 3, Training Loss: 0.6368\n",
            "LR: 0.001, Batch Size: 64, Epoch: 4, Training Loss: 0.3724\n",
            "LR: 0.001, Batch Size: 64, Epoch: 5, Training Loss: 0.2724\n",
            "LR: 0.001, Batch Size: 64, Epoch: 6, Training Loss: 0.2401\n",
            "LR: 0.001, Batch Size: 64, Epoch: 7, Training Loss: 0.2069\n",
            "LR: 0.001, Batch Size: 64, Epoch: 8, Training Loss: 0.1660\n",
            "LR: 0.001, Batch Size: 64, Epoch: 9, Training Loss: 0.1617\n",
            "LR: 0.001, Batch Size: 64, Epoch: 10, Training Loss: 0.1270\n",
            "LR: 0.001, Batch Size: 64, Epoch: 11, Training Loss: 0.1263\n",
            "LR: 0.001, Batch Size: 64, Epoch: 12, Training Loss: 0.1257\n",
            "LR: 0.001, Batch Size: 64, Epoch: 13, Training Loss: 0.1038\n",
            "LR: 0.001, Batch Size: 64, Epoch: 14, Training Loss: 0.1140\n",
            "LR: 0.001, Batch Size: 64, Epoch: 15, Training Loss: 0.1130\n",
            "LR: 0.001, Batch Size: 64, Num Epochs: 15, Test Loss: 0.1588, Test Accuracy: 0.9573\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 64, Epoch: 1, Training Loss: 1.5781\n",
            "LR: 0.001, Batch Size: 64, Epoch: 2, Training Loss: 1.2461\n",
            "LR: 0.001, Batch Size: 64, Epoch: 3, Training Loss: 0.8348\n",
            "LR: 0.001, Batch Size: 64, Epoch: 4, Training Loss: 0.5201\n",
            "LR: 0.001, Batch Size: 64, Epoch: 5, Training Loss: 0.3525\n",
            "LR: 0.001, Batch Size: 64, Epoch: 6, Training Loss: 0.2602\n",
            "LR: 0.001, Batch Size: 64, Epoch: 7, Training Loss: 0.2039\n",
            "LR: 0.001, Batch Size: 64, Epoch: 8, Training Loss: 0.1717\n",
            "LR: 0.001, Batch Size: 64, Epoch: 9, Training Loss: 0.1586\n",
            "LR: 0.001, Batch Size: 64, Epoch: 10, Training Loss: 0.1359\n",
            "LR: 0.001, Batch Size: 64, Epoch: 11, Training Loss: 0.1277\n",
            "LR: 0.001, Batch Size: 64, Epoch: 12, Training Loss: 0.1218\n",
            "LR: 0.001, Batch Size: 64, Epoch: 13, Training Loss: 0.0963\n",
            "LR: 0.001, Batch Size: 64, Epoch: 14, Training Loss: 0.1047\n",
            "LR: 0.001, Batch Size: 64, Epoch: 15, Training Loss: 0.0957\n",
            "LR: 0.001, Batch Size: 64, Epoch: 16, Training Loss: 0.0806\n",
            "LR: 0.001, Batch Size: 64, Epoch: 17, Training Loss: 0.0735\n",
            "LR: 0.001, Batch Size: 64, Epoch: 18, Training Loss: 0.0729\n",
            "LR: 0.001, Batch Size: 64, Epoch: 19, Training Loss: 0.0632\n",
            "LR: 0.001, Batch Size: 64, Epoch: 20, Training Loss: 0.0709\n",
            "LR: 0.001, Batch Size: 64, Num Epochs: 20, Test Loss: 0.1671, Test Accuracy: 0.9528\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 64, Epoch: 1, Training Loss: 1.5775\n",
            "LR: 0.001, Batch Size: 64, Epoch: 2, Training Loss: 1.2041\n",
            "LR: 0.001, Batch Size: 64, Epoch: 3, Training Loss: 0.8656\n",
            "LR: 0.001, Batch Size: 64, Epoch: 4, Training Loss: 0.6326\n",
            "LR: 0.001, Batch Size: 64, Epoch: 5, Training Loss: 0.4533\n",
            "LR: 0.001, Batch Size: 64, Epoch: 6, Training Loss: 0.3508\n",
            "LR: 0.001, Batch Size: 64, Epoch: 7, Training Loss: 0.2676\n",
            "LR: 0.001, Batch Size: 64, Epoch: 8, Training Loss: 0.2138\n",
            "LR: 0.001, Batch Size: 64, Epoch: 9, Training Loss: 0.1574\n",
            "LR: 0.001, Batch Size: 64, Epoch: 10, Training Loss: 0.1561\n",
            "LR: 0.001, Batch Size: 64, Epoch: 11, Training Loss: 0.1367\n",
            "LR: 0.001, Batch Size: 64, Epoch: 12, Training Loss: 0.1403\n",
            "LR: 0.001, Batch Size: 64, Epoch: 13, Training Loss: 0.1174\n",
            "LR: 0.001, Batch Size: 64, Epoch: 14, Training Loss: 0.1102\n",
            "LR: 0.001, Batch Size: 64, Epoch: 15, Training Loss: 0.0905\n",
            "LR: 0.001, Batch Size: 64, Epoch: 16, Training Loss: 0.0928\n",
            "LR: 0.001, Batch Size: 64, Epoch: 17, Training Loss: 0.0811\n",
            "LR: 0.001, Batch Size: 64, Epoch: 18, Training Loss: 0.0712\n",
            "LR: 0.001, Batch Size: 64, Epoch: 19, Training Loss: 0.0596\n",
            "LR: 0.001, Batch Size: 64, Epoch: 20, Training Loss: 0.0573\n",
            "LR: 0.001, Batch Size: 64, Epoch: 21, Training Loss: 0.0708\n",
            "LR: 0.001, Batch Size: 64, Epoch: 22, Training Loss: 0.0571\n",
            "LR: 0.001, Batch Size: 64, Epoch: 23, Training Loss: 0.0583\n",
            "LR: 0.001, Batch Size: 64, Epoch: 24, Training Loss: 0.0590\n",
            "LR: 0.001, Batch Size: 64, Epoch: 25, Training Loss: 0.0459\n",
            "LR: 0.001, Batch Size: 64, Num Epochs: 25, Test Loss: 0.1659, Test Accuracy: 0.9528\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 64, Epoch: 1, Training Loss: 1.5777\n",
            "LR: 0.001, Batch Size: 64, Epoch: 2, Training Loss: 1.2048\n",
            "LR: 0.001, Batch Size: 64, Epoch: 3, Training Loss: 0.7269\n",
            "LR: 0.001, Batch Size: 64, Epoch: 4, Training Loss: 0.3783\n",
            "LR: 0.001, Batch Size: 64, Epoch: 5, Training Loss: 0.2651\n",
            "LR: 0.001, Batch Size: 64, Epoch: 6, Training Loss: 0.2289\n",
            "LR: 0.001, Batch Size: 64, Epoch: 7, Training Loss: 0.1814\n",
            "LR: 0.001, Batch Size: 64, Epoch: 8, Training Loss: 0.1570\n",
            "LR: 0.001, Batch Size: 64, Epoch: 9, Training Loss: 0.1524\n",
            "LR: 0.001, Batch Size: 64, Epoch: 10, Training Loss: 0.1501\n",
            "LR: 0.001, Batch Size: 64, Epoch: 11, Training Loss: 0.1211\n",
            "LR: 0.001, Batch Size: 64, Epoch: 12, Training Loss: 0.1112\n",
            "LR: 0.001, Batch Size: 64, Epoch: 13, Training Loss: 0.0982\n",
            "LR: 0.001, Batch Size: 64, Epoch: 14, Training Loss: 0.1054\n",
            "LR: 0.001, Batch Size: 64, Epoch: 15, Training Loss: 0.0968\n",
            "LR: 0.001, Batch Size: 64, Epoch: 16, Training Loss: 0.0833\n",
            "LR: 0.001, Batch Size: 64, Epoch: 17, Training Loss: 0.0813\n",
            "LR: 0.001, Batch Size: 64, Epoch: 18, Training Loss: 0.0766\n",
            "LR: 0.001, Batch Size: 64, Epoch: 19, Training Loss: 0.0755\n",
            "LR: 0.001, Batch Size: 64, Epoch: 20, Training Loss: 0.0592\n",
            "LR: 0.001, Batch Size: 64, Epoch: 21, Training Loss: 0.0620\n",
            "LR: 0.001, Batch Size: 64, Epoch: 22, Training Loss: 0.0497\n",
            "LR: 0.001, Batch Size: 64, Epoch: 23, Training Loss: 0.0480\n",
            "LR: 0.001, Batch Size: 64, Epoch: 24, Training Loss: 0.0488\n",
            "LR: 0.001, Batch Size: 64, Epoch: 25, Training Loss: 0.0492\n",
            "LR: 0.001, Batch Size: 64, Epoch: 26, Training Loss: 0.0422\n",
            "LR: 0.001, Batch Size: 64, Epoch: 27, Training Loss: 0.0262\n",
            "LR: 0.001, Batch Size: 64, Epoch: 28, Training Loss: 0.0364\n",
            "LR: 0.001, Batch Size: 64, Epoch: 29, Training Loss: 0.0321\n",
            "LR: 0.001, Batch Size: 64, Epoch: 30, Training Loss: 0.0406\n",
            "LR: 0.001, Batch Size: 64, Num Epochs: 30, Test Loss: 0.2109, Test Accuracy: 0.9573\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 128, Epoch: 1, Training Loss: 1.6112\n",
            "LR: 0.001, Batch Size: 128, Epoch: 2, Training Loss: 1.4801\n",
            "LR: 0.001, Batch Size: 128, Epoch: 3, Training Loss: 1.1425\n",
            "LR: 0.001, Batch Size: 128, Epoch: 4, Training Loss: 0.8207\n",
            "LR: 0.001, Batch Size: 128, Epoch: 5, Training Loss: 0.5806\n",
            "LR: 0.001, Batch Size: 128, Num Epochs: 5, Test Loss: 0.3813, Test Accuracy: 0.8966\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 128, Epoch: 1, Training Loss: 1.6087\n",
            "LR: 0.001, Batch Size: 128, Epoch: 2, Training Loss: 1.5179\n",
            "LR: 0.001, Batch Size: 128, Epoch: 3, Training Loss: 1.2126\n",
            "LR: 0.001, Batch Size: 128, Epoch: 4, Training Loss: 0.8278\n",
            "LR: 0.001, Batch Size: 128, Epoch: 5, Training Loss: 0.5214\n",
            "LR: 0.001, Batch Size: 128, Epoch: 6, Training Loss: 0.4088\n",
            "LR: 0.001, Batch Size: 128, Epoch: 7, Training Loss: 0.3033\n",
            "LR: 0.001, Batch Size: 128, Epoch: 8, Training Loss: 0.2504\n",
            "LR: 0.001, Batch Size: 128, Epoch: 9, Training Loss: 0.2151\n",
            "LR: 0.001, Batch Size: 128, Epoch: 10, Training Loss: 0.2061\n",
            "LR: 0.001, Batch Size: 128, Num Epochs: 10, Test Loss: 0.1449, Test Accuracy: 0.9461\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 128, Epoch: 1, Training Loss: 1.6041\n",
            "LR: 0.001, Batch Size: 128, Epoch: 2, Training Loss: 1.5134\n",
            "LR: 0.001, Batch Size: 128, Epoch: 3, Training Loss: 1.2255\n",
            "LR: 0.001, Batch Size: 128, Epoch: 4, Training Loss: 0.8997\n",
            "LR: 0.001, Batch Size: 128, Epoch: 5, Training Loss: 0.7117\n",
            "LR: 0.001, Batch Size: 128, Epoch: 6, Training Loss: 0.5329\n",
            "LR: 0.001, Batch Size: 128, Epoch: 7, Training Loss: 0.3976\n",
            "LR: 0.001, Batch Size: 128, Epoch: 8, Training Loss: 0.2818\n",
            "LR: 0.001, Batch Size: 128, Epoch: 9, Training Loss: 0.2323\n",
            "LR: 0.001, Batch Size: 128, Epoch: 10, Training Loss: 0.1897\n",
            "LR: 0.001, Batch Size: 128, Epoch: 11, Training Loss: 0.1831\n",
            "LR: 0.001, Batch Size: 128, Epoch: 12, Training Loss: 0.1662\n",
            "LR: 0.001, Batch Size: 128, Epoch: 13, Training Loss: 0.1225\n",
            "LR: 0.001, Batch Size: 128, Epoch: 14, Training Loss: 0.1231\n",
            "LR: 0.001, Batch Size: 128, Epoch: 15, Training Loss: 0.1077\n",
            "LR: 0.001, Batch Size: 128, Num Epochs: 15, Test Loss: 0.1300, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 128, Epoch: 1, Training Loss: 1.6115\n",
            "LR: 0.001, Batch Size: 128, Epoch: 2, Training Loss: 1.5270\n",
            "LR: 0.001, Batch Size: 128, Epoch: 3, Training Loss: 1.2257\n",
            "LR: 0.001, Batch Size: 128, Epoch: 4, Training Loss: 0.8083\n",
            "LR: 0.001, Batch Size: 128, Epoch: 5, Training Loss: 0.5275\n",
            "LR: 0.001, Batch Size: 128, Epoch: 6, Training Loss: 0.4097\n",
            "LR: 0.001, Batch Size: 128, Epoch: 7, Training Loss: 0.3122\n",
            "LR: 0.001, Batch Size: 128, Epoch: 8, Training Loss: 0.2536\n",
            "LR: 0.001, Batch Size: 128, Epoch: 9, Training Loss: 0.2420\n",
            "LR: 0.001, Batch Size: 128, Epoch: 10, Training Loss: 0.2077\n",
            "LR: 0.001, Batch Size: 128, Epoch: 11, Training Loss: 0.1862\n",
            "LR: 0.001, Batch Size: 128, Epoch: 12, Training Loss: 0.1747\n",
            "LR: 0.001, Batch Size: 128, Epoch: 13, Training Loss: 0.1443\n",
            "LR: 0.001, Batch Size: 128, Epoch: 14, Training Loss: 0.1477\n",
            "LR: 0.001, Batch Size: 128, Epoch: 15, Training Loss: 0.1365\n",
            "LR: 0.001, Batch Size: 128, Epoch: 16, Training Loss: 0.1186\n",
            "LR: 0.001, Batch Size: 128, Epoch: 17, Training Loss: 0.1133\n",
            "LR: 0.001, Batch Size: 128, Epoch: 18, Training Loss: 0.1025\n",
            "LR: 0.001, Batch Size: 128, Epoch: 19, Training Loss: 0.0904\n",
            "LR: 0.001, Batch Size: 128, Epoch: 20, Training Loss: 0.0867\n",
            "LR: 0.001, Batch Size: 128, Num Epochs: 20, Test Loss: 0.1397, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 128, Epoch: 1, Training Loss: 1.6054\n",
            "LR: 0.001, Batch Size: 128, Epoch: 2, Training Loss: 1.5394\n",
            "LR: 0.001, Batch Size: 128, Epoch: 3, Training Loss: 1.2433\n",
            "LR: 0.001, Batch Size: 128, Epoch: 4, Training Loss: 0.8198\n",
            "LR: 0.001, Batch Size: 128, Epoch: 5, Training Loss: 0.5740\n",
            "LR: 0.001, Batch Size: 128, Epoch: 6, Training Loss: 0.3998\n",
            "LR: 0.001, Batch Size: 128, Epoch: 7, Training Loss: 0.3244\n",
            "LR: 0.001, Batch Size: 128, Epoch: 8, Training Loss: 0.2708\n",
            "LR: 0.001, Batch Size: 128, Epoch: 9, Training Loss: 0.2420\n",
            "LR: 0.001, Batch Size: 128, Epoch: 10, Training Loss: 0.2087\n",
            "LR: 0.001, Batch Size: 128, Epoch: 11, Training Loss: 0.1722\n",
            "LR: 0.001, Batch Size: 128, Epoch: 12, Training Loss: 0.1841\n",
            "LR: 0.001, Batch Size: 128, Epoch: 13, Training Loss: 0.1600\n",
            "LR: 0.001, Batch Size: 128, Epoch: 14, Training Loss: 0.1512\n",
            "LR: 0.001, Batch Size: 128, Epoch: 15, Training Loss: 0.1280\n",
            "LR: 0.001, Batch Size: 128, Epoch: 16, Training Loss: 0.1250\n",
            "LR: 0.001, Batch Size: 128, Epoch: 17, Training Loss: 0.1168\n",
            "LR: 0.001, Batch Size: 128, Epoch: 18, Training Loss: 0.1145\n",
            "LR: 0.001, Batch Size: 128, Epoch: 19, Training Loss: 0.1142\n",
            "LR: 0.001, Batch Size: 128, Epoch: 20, Training Loss: 0.0912\n",
            "LR: 0.001, Batch Size: 128, Epoch: 21, Training Loss: 0.0841\n",
            "LR: 0.001, Batch Size: 128, Epoch: 22, Training Loss: 0.0936\n",
            "LR: 0.001, Batch Size: 128, Epoch: 23, Training Loss: 0.0812\n",
            "LR: 0.001, Batch Size: 128, Epoch: 24, Training Loss: 0.0709\n",
            "LR: 0.001, Batch Size: 128, Epoch: 25, Training Loss: 0.0740\n",
            "LR: 0.001, Batch Size: 128, Num Epochs: 25, Test Loss: 0.1641, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "LR: 0.001, Batch Size: 128, Epoch: 1, Training Loss: 1.5998\n",
            "LR: 0.001, Batch Size: 128, Epoch: 2, Training Loss: 1.5390\n",
            "LR: 0.001, Batch Size: 128, Epoch: 3, Training Loss: 1.2684\n",
            "LR: 0.001, Batch Size: 128, Epoch: 4, Training Loss: 0.9007\n",
            "LR: 0.001, Batch Size: 128, Epoch: 5, Training Loss: 0.6431\n",
            "LR: 0.001, Batch Size: 128, Epoch: 6, Training Loss: 0.5014\n",
            "LR: 0.001, Batch Size: 128, Epoch: 7, Training Loss: 0.3643\n",
            "LR: 0.001, Batch Size: 128, Epoch: 8, Training Loss: 0.3220\n",
            "LR: 0.001, Batch Size: 128, Epoch: 9, Training Loss: 0.2562\n",
            "LR: 0.001, Batch Size: 128, Epoch: 10, Training Loss: 0.2201\n",
            "LR: 0.001, Batch Size: 128, Epoch: 11, Training Loss: 0.1842\n",
            "LR: 0.001, Batch Size: 128, Epoch: 12, Training Loss: 0.1628\n",
            "LR: 0.001, Batch Size: 128, Epoch: 13, Training Loss: 0.1444\n",
            "LR: 0.001, Batch Size: 128, Epoch: 14, Training Loss: 0.1548\n",
            "LR: 0.001, Batch Size: 128, Epoch: 15, Training Loss: 0.1422\n",
            "LR: 0.001, Batch Size: 128, Epoch: 16, Training Loss: 0.1289\n",
            "LR: 0.001, Batch Size: 128, Epoch: 17, Training Loss: 0.1189\n",
            "LR: 0.001, Batch Size: 128, Epoch: 18, Training Loss: 0.1094\n",
            "LR: 0.001, Batch Size: 128, Epoch: 19, Training Loss: 0.1040\n",
            "LR: 0.001, Batch Size: 128, Epoch: 20, Training Loss: 0.0992\n",
            "LR: 0.001, Batch Size: 128, Epoch: 21, Training Loss: 0.0809\n",
            "LR: 0.001, Batch Size: 128, Epoch: 22, Training Loss: 0.0971\n",
            "LR: 0.001, Batch Size: 128, Epoch: 23, Training Loss: 0.0794\n",
            "LR: 0.001, Batch Size: 128, Epoch: 24, Training Loss: 0.0705\n",
            "LR: 0.001, Batch Size: 128, Epoch: 25, Training Loss: 0.0768\n",
            "LR: 0.001, Batch Size: 128, Epoch: 26, Training Loss: 0.0648\n",
            "LR: 0.001, Batch Size: 128, Epoch: 27, Training Loss: 0.0700\n",
            "LR: 0.001, Batch Size: 128, Epoch: 28, Training Loss: 0.0640\n",
            "LR: 0.001, Batch Size: 128, Epoch: 29, Training Loss: 0.0518\n",
            "LR: 0.001, Batch Size: 128, Epoch: 30, Training Loss: 0.0502\n",
            "LR: 0.001, Batch Size: 128, Num Epochs: 30, Test Loss: 0.1738, Test Accuracy: 0.9528\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 8, Epoch: 1, Training Loss: 0.9052\n",
            "LR: 0.01, Batch Size: 8, Epoch: 2, Training Loss: 0.5528\n",
            "LR: 0.01, Batch Size: 8, Epoch: 3, Training Loss: 0.5037\n",
            "LR: 0.01, Batch Size: 8, Epoch: 4, Training Loss: 0.4441\n",
            "LR: 0.01, Batch Size: 8, Epoch: 5, Training Loss: 0.4708\n",
            "LR: 0.01, Batch Size: 8, Num Epochs: 5, Test Loss: 0.3782, Test Accuracy: 0.9236\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 8, Epoch: 1, Training Loss: 0.9136\n",
            "LR: 0.01, Batch Size: 8, Epoch: 2, Training Loss: 0.4997\n",
            "LR: 0.01, Batch Size: 8, Epoch: 3, Training Loss: 0.5586\n",
            "LR: 0.01, Batch Size: 8, Epoch: 4, Training Loss: 0.3710\n",
            "LR: 0.01, Batch Size: 8, Epoch: 5, Training Loss: 0.4893\n",
            "LR: 0.01, Batch Size: 8, Epoch: 6, Training Loss: 0.4103\n",
            "LR: 0.01, Batch Size: 8, Epoch: 7, Training Loss: 0.3590\n",
            "LR: 0.01, Batch Size: 8, Epoch: 8, Training Loss: 0.3359\n",
            "LR: 0.01, Batch Size: 8, Epoch: 9, Training Loss: 0.4624\n",
            "LR: 0.01, Batch Size: 8, Epoch: 10, Training Loss: 0.3579\n",
            "LR: 0.01, Batch Size: 8, Num Epochs: 10, Test Loss: 0.2163, Test Accuracy: 0.9281\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 8, Epoch: 1, Training Loss: 0.8781\n",
            "LR: 0.01, Batch Size: 8, Epoch: 2, Training Loss: 0.5441\n",
            "LR: 0.01, Batch Size: 8, Epoch: 3, Training Loss: 0.4619\n",
            "LR: 0.01, Batch Size: 8, Epoch: 4, Training Loss: 0.3748\n",
            "LR: 0.01, Batch Size: 8, Epoch: 5, Training Loss: 0.5269\n",
            "LR: 0.01, Batch Size: 8, Epoch: 6, Training Loss: 0.3771\n",
            "LR: 0.01, Batch Size: 8, Epoch: 7, Training Loss: 0.2976\n",
            "LR: 0.01, Batch Size: 8, Epoch: 8, Training Loss: 0.3178\n",
            "LR: 0.01, Batch Size: 8, Epoch: 9, Training Loss: 0.3606\n",
            "LR: 0.01, Batch Size: 8, Epoch: 10, Training Loss: 0.2585\n",
            "LR: 0.01, Batch Size: 8, Epoch: 11, Training Loss: 0.4420\n",
            "LR: 0.01, Batch Size: 8, Epoch: 12, Training Loss: 0.3614\n",
            "LR: 0.01, Batch Size: 8, Epoch: 13, Training Loss: 0.3863\n",
            "LR: 0.01, Batch Size: 8, Epoch: 14, Training Loss: 0.3358\n",
            "LR: 0.01, Batch Size: 8, Epoch: 15, Training Loss: 0.4242\n",
            "LR: 0.01, Batch Size: 8, Num Epochs: 15, Test Loss: 0.2772, Test Accuracy: 0.9348\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 8, Epoch: 1, Training Loss: 0.8845\n",
            "LR: 0.01, Batch Size: 8, Epoch: 2, Training Loss: 0.5775\n",
            "LR: 0.01, Batch Size: 8, Epoch: 3, Training Loss: 0.4999\n",
            "LR: 0.01, Batch Size: 8, Epoch: 4, Training Loss: 0.4442\n",
            "LR: 0.01, Batch Size: 8, Epoch: 5, Training Loss: 0.4459\n",
            "LR: 0.01, Batch Size: 8, Epoch: 6, Training Loss: 0.3707\n",
            "LR: 0.01, Batch Size: 8, Epoch: 7, Training Loss: 0.3711\n",
            "LR: 0.01, Batch Size: 8, Epoch: 8, Training Loss: 0.3787\n",
            "LR: 0.01, Batch Size: 8, Epoch: 9, Training Loss: 0.2831\n",
            "LR: 0.01, Batch Size: 8, Epoch: 10, Training Loss: 0.2571\n",
            "LR: 0.01, Batch Size: 8, Epoch: 11, Training Loss: 0.3231\n",
            "LR: 0.01, Batch Size: 8, Epoch: 12, Training Loss: 0.3682\n",
            "LR: 0.01, Batch Size: 8, Epoch: 13, Training Loss: 0.3443\n",
            "LR: 0.01, Batch Size: 8, Epoch: 14, Training Loss: 0.3020\n",
            "LR: 0.01, Batch Size: 8, Epoch: 15, Training Loss: 0.3373\n",
            "LR: 0.01, Batch Size: 8, Epoch: 16, Training Loss: 0.3053\n",
            "LR: 0.01, Batch Size: 8, Epoch: 17, Training Loss: 0.4085\n",
            "LR: 0.01, Batch Size: 8, Epoch: 18, Training Loss: 0.3457\n",
            "LR: 0.01, Batch Size: 8, Epoch: 19, Training Loss: 0.4243\n",
            "LR: 0.01, Batch Size: 8, Epoch: 20, Training Loss: 0.2900\n",
            "LR: 0.01, Batch Size: 8, Num Epochs: 20, Test Loss: 0.2503, Test Accuracy: 0.9461\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 8, Epoch: 1, Training Loss: 0.8034\n",
            "LR: 0.01, Batch Size: 8, Epoch: 2, Training Loss: 0.4550\n",
            "LR: 0.01, Batch Size: 8, Epoch: 3, Training Loss: 0.5223\n",
            "LR: 0.01, Batch Size: 8, Epoch: 4, Training Loss: 0.3859\n",
            "LR: 0.01, Batch Size: 8, Epoch: 5, Training Loss: 0.3606\n",
            "LR: 0.01, Batch Size: 8, Epoch: 6, Training Loss: 0.3384\n",
            "LR: 0.01, Batch Size: 8, Epoch: 7, Training Loss: 0.3259\n",
            "LR: 0.01, Batch Size: 8, Epoch: 8, Training Loss: 0.3069\n",
            "LR: 0.01, Batch Size: 8, Epoch: 9, Training Loss: 0.3265\n",
            "LR: 0.01, Batch Size: 8, Epoch: 10, Training Loss: 0.3169\n",
            "LR: 0.01, Batch Size: 8, Epoch: 11, Training Loss: 0.4584\n",
            "LR: 0.01, Batch Size: 8, Epoch: 12, Training Loss: 0.4853\n",
            "LR: 0.01, Batch Size: 8, Epoch: 13, Training Loss: 0.3557\n",
            "LR: 0.01, Batch Size: 8, Epoch: 14, Training Loss: 0.2058\n",
            "LR: 0.01, Batch Size: 8, Epoch: 15, Training Loss: 0.3497\n",
            "LR: 0.01, Batch Size: 8, Epoch: 16, Training Loss: 0.3473\n",
            "LR: 0.01, Batch Size: 8, Epoch: 17, Training Loss: 0.4025\n",
            "LR: 0.01, Batch Size: 8, Epoch: 18, Training Loss: 0.2524\n",
            "LR: 0.01, Batch Size: 8, Epoch: 19, Training Loss: 0.2348\n",
            "LR: 0.01, Batch Size: 8, Epoch: 20, Training Loss: 0.2289\n",
            "LR: 0.01, Batch Size: 8, Epoch: 21, Training Loss: 0.1804\n",
            "LR: 0.01, Batch Size: 8, Epoch: 22, Training Loss: 0.2716\n",
            "LR: 0.01, Batch Size: 8, Epoch: 23, Training Loss: 0.2700\n",
            "LR: 0.01, Batch Size: 8, Epoch: 24, Training Loss: 0.2529\n",
            "LR: 0.01, Batch Size: 8, Epoch: 25, Training Loss: 0.2312\n",
            "LR: 0.01, Batch Size: 8, Num Epochs: 25, Test Loss: 0.3556, Test Accuracy: 0.9371\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 8, Epoch: 1, Training Loss: 0.9418\n",
            "LR: 0.01, Batch Size: 8, Epoch: 2, Training Loss: 0.5332\n",
            "LR: 0.01, Batch Size: 8, Epoch: 3, Training Loss: 0.4290\n",
            "LR: 0.01, Batch Size: 8, Epoch: 4, Training Loss: 0.4209\n",
            "LR: 0.01, Batch Size: 8, Epoch: 5, Training Loss: 0.4411\n",
            "LR: 0.01, Batch Size: 8, Epoch: 6, Training Loss: 0.4188\n",
            "LR: 0.01, Batch Size: 8, Epoch: 7, Training Loss: 0.2948\n",
            "LR: 0.01, Batch Size: 8, Epoch: 8, Training Loss: 0.3133\n",
            "LR: 0.01, Batch Size: 8, Epoch: 9, Training Loss: 0.3392\n",
            "LR: 0.01, Batch Size: 8, Epoch: 10, Training Loss: 0.2826\n",
            "LR: 0.01, Batch Size: 8, Epoch: 11, Training Loss: 0.2443\n",
            "LR: 0.01, Batch Size: 8, Epoch: 12, Training Loss: 0.2831\n",
            "LR: 0.01, Batch Size: 8, Epoch: 13, Training Loss: 0.3205\n",
            "LR: 0.01, Batch Size: 8, Epoch: 14, Training Loss: 0.2472\n",
            "LR: 0.01, Batch Size: 8, Epoch: 15, Training Loss: 0.3518\n",
            "LR: 0.01, Batch Size: 8, Epoch: 16, Training Loss: 0.4945\n",
            "LR: 0.01, Batch Size: 8, Epoch: 17, Training Loss: 0.2956\n",
            "LR: 0.01, Batch Size: 8, Epoch: 18, Training Loss: 0.3185\n",
            "LR: 0.01, Batch Size: 8, Epoch: 19, Training Loss: 0.2871\n",
            "LR: 0.01, Batch Size: 8, Epoch: 20, Training Loss: 0.2403\n",
            "LR: 0.01, Batch Size: 8, Epoch: 21, Training Loss: 0.2230\n",
            "LR: 0.01, Batch Size: 8, Epoch: 22, Training Loss: 0.2255\n",
            "LR: 0.01, Batch Size: 8, Epoch: 23, Training Loss: 0.1992\n",
            "LR: 0.01, Batch Size: 8, Epoch: 24, Training Loss: 0.1993\n",
            "LR: 0.01, Batch Size: 8, Epoch: 25, Training Loss: 0.1690\n",
            "LR: 0.01, Batch Size: 8, Epoch: 26, Training Loss: 0.2442\n",
            "LR: 0.01, Batch Size: 8, Epoch: 27, Training Loss: 0.2046\n",
            "LR: 0.01, Batch Size: 8, Epoch: 28, Training Loss: 0.1164\n",
            "LR: 0.01, Batch Size: 8, Epoch: 29, Training Loss: 0.2147\n",
            "LR: 0.01, Batch Size: 8, Epoch: 30, Training Loss: 0.2347\n",
            "LR: 0.01, Batch Size: 8, Num Epochs: 30, Test Loss: 0.3057, Test Accuracy: 0.9326\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 16, Epoch: 1, Training Loss: 0.7451\n",
            "LR: 0.01, Batch Size: 16, Epoch: 2, Training Loss: 0.3723\n",
            "LR: 0.01, Batch Size: 16, Epoch: 3, Training Loss: 0.3764\n",
            "LR: 0.01, Batch Size: 16, Epoch: 4, Training Loss: 0.2688\n",
            "LR: 0.01, Batch Size: 16, Epoch: 5, Training Loss: 0.2602\n",
            "LR: 0.01, Batch Size: 16, Num Epochs: 5, Test Loss: 0.2806, Test Accuracy: 0.9461\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 16, Epoch: 1, Training Loss: 0.8136\n",
            "LR: 0.01, Batch Size: 16, Epoch: 2, Training Loss: 0.4346\n",
            "LR: 0.01, Batch Size: 16, Epoch: 3, Training Loss: 0.3269\n",
            "LR: 0.01, Batch Size: 16, Epoch: 4, Training Loss: 0.3318\n",
            "LR: 0.01, Batch Size: 16, Epoch: 5, Training Loss: 0.3575\n",
            "LR: 0.01, Batch Size: 16, Epoch: 6, Training Loss: 0.3908\n",
            "LR: 0.01, Batch Size: 16, Epoch: 7, Training Loss: 0.2378\n",
            "LR: 0.01, Batch Size: 16, Epoch: 8, Training Loss: 0.2521\n",
            "LR: 0.01, Batch Size: 16, Epoch: 9, Training Loss: 0.2685\n",
            "LR: 0.01, Batch Size: 16, Epoch: 10, Training Loss: 0.2449\n",
            "LR: 0.01, Batch Size: 16, Num Epochs: 10, Test Loss: 0.3197, Test Accuracy: 0.9213\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 16, Epoch: 1, Training Loss: 0.9196\n",
            "LR: 0.01, Batch Size: 16, Epoch: 2, Training Loss: 0.4730\n",
            "LR: 0.01, Batch Size: 16, Epoch: 3, Training Loss: 0.3382\n",
            "LR: 0.01, Batch Size: 16, Epoch: 4, Training Loss: 0.3167\n",
            "LR: 0.01, Batch Size: 16, Epoch: 5, Training Loss: 0.3145\n",
            "LR: 0.01, Batch Size: 16, Epoch: 6, Training Loss: 0.3196\n",
            "LR: 0.01, Batch Size: 16, Epoch: 7, Training Loss: 0.2821\n",
            "LR: 0.01, Batch Size: 16, Epoch: 8, Training Loss: 0.2570\n",
            "LR: 0.01, Batch Size: 16, Epoch: 9, Training Loss: 0.2695\n",
            "LR: 0.01, Batch Size: 16, Epoch: 10, Training Loss: 0.2288\n",
            "LR: 0.01, Batch Size: 16, Epoch: 11, Training Loss: 0.2424\n",
            "LR: 0.01, Batch Size: 16, Epoch: 12, Training Loss: 0.2733\n",
            "LR: 0.01, Batch Size: 16, Epoch: 13, Training Loss: 0.2338\n",
            "LR: 0.01, Batch Size: 16, Epoch: 14, Training Loss: 0.2517\n",
            "LR: 0.01, Batch Size: 16, Epoch: 15, Training Loss: 0.2023\n",
            "LR: 0.01, Batch Size: 16, Num Epochs: 15, Test Loss: 0.2095, Test Accuracy: 0.9506\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 16, Epoch: 1, Training Loss: 0.8409\n",
            "LR: 0.01, Batch Size: 16, Epoch: 2, Training Loss: 0.3587\n",
            "LR: 0.01, Batch Size: 16, Epoch: 3, Training Loss: 0.3285\n",
            "LR: 0.01, Batch Size: 16, Epoch: 4, Training Loss: 0.2599\n",
            "LR: 0.01, Batch Size: 16, Epoch: 5, Training Loss: 0.2707\n",
            "LR: 0.01, Batch Size: 16, Epoch: 6, Training Loss: 0.3283\n",
            "LR: 0.01, Batch Size: 16, Epoch: 7, Training Loss: 0.2663\n",
            "LR: 0.01, Batch Size: 16, Epoch: 8, Training Loss: 0.2368\n",
            "LR: 0.01, Batch Size: 16, Epoch: 9, Training Loss: 0.2576\n",
            "LR: 0.01, Batch Size: 16, Epoch: 10, Training Loss: 0.2041\n",
            "LR: 0.01, Batch Size: 16, Epoch: 11, Training Loss: 0.2360\n",
            "LR: 0.01, Batch Size: 16, Epoch: 12, Training Loss: 0.2500\n",
            "LR: 0.01, Batch Size: 16, Epoch: 13, Training Loss: 0.2309\n",
            "LR: 0.01, Batch Size: 16, Epoch: 14, Training Loss: 0.1635\n",
            "LR: 0.01, Batch Size: 16, Epoch: 15, Training Loss: 0.2267\n",
            "LR: 0.01, Batch Size: 16, Epoch: 16, Training Loss: 0.1839\n",
            "LR: 0.01, Batch Size: 16, Epoch: 17, Training Loss: 0.1871\n",
            "LR: 0.01, Batch Size: 16, Epoch: 18, Training Loss: 0.2812\n",
            "LR: 0.01, Batch Size: 16, Epoch: 19, Training Loss: 0.2995\n",
            "LR: 0.01, Batch Size: 16, Epoch: 20, Training Loss: 0.2169\n",
            "LR: 0.01, Batch Size: 16, Num Epochs: 20, Test Loss: 0.3006, Test Accuracy: 0.9506\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 16, Epoch: 1, Training Loss: 0.8305\n",
            "LR: 0.01, Batch Size: 16, Epoch: 2, Training Loss: 0.3796\n",
            "LR: 0.01, Batch Size: 16, Epoch: 3, Training Loss: 0.2970\n",
            "LR: 0.01, Batch Size: 16, Epoch: 4, Training Loss: 0.3670\n",
            "LR: 0.01, Batch Size: 16, Epoch: 5, Training Loss: 0.2475\n",
            "LR: 0.01, Batch Size: 16, Epoch: 6, Training Loss: 0.3134\n",
            "LR: 0.01, Batch Size: 16, Epoch: 7, Training Loss: 0.2775\n",
            "LR: 0.01, Batch Size: 16, Epoch: 8, Training Loss: 0.2757\n",
            "LR: 0.01, Batch Size: 16, Epoch: 9, Training Loss: 0.2251\n",
            "LR: 0.01, Batch Size: 16, Epoch: 10, Training Loss: 0.2302\n",
            "LR: 0.01, Batch Size: 16, Epoch: 11, Training Loss: 0.2356\n",
            "LR: 0.01, Batch Size: 16, Epoch: 12, Training Loss: 0.1679\n",
            "LR: 0.01, Batch Size: 16, Epoch: 13, Training Loss: 0.2256\n",
            "LR: 0.01, Batch Size: 16, Epoch: 14, Training Loss: 0.1581\n",
            "LR: 0.01, Batch Size: 16, Epoch: 15, Training Loss: 0.2994\n",
            "LR: 0.01, Batch Size: 16, Epoch: 16, Training Loss: 0.2245\n",
            "LR: 0.01, Batch Size: 16, Epoch: 17, Training Loss: 0.1712\n",
            "LR: 0.01, Batch Size: 16, Epoch: 18, Training Loss: 0.2264\n",
            "LR: 0.01, Batch Size: 16, Epoch: 19, Training Loss: 0.1898\n",
            "LR: 0.01, Batch Size: 16, Epoch: 20, Training Loss: 0.1549\n",
            "LR: 0.01, Batch Size: 16, Epoch: 21, Training Loss: 0.1437\n",
            "LR: 0.01, Batch Size: 16, Epoch: 22, Training Loss: 0.1635\n",
            "LR: 0.01, Batch Size: 16, Epoch: 23, Training Loss: 0.1158\n",
            "LR: 0.01, Batch Size: 16, Epoch: 24, Training Loss: 0.3003\n",
            "LR: 0.01, Batch Size: 16, Epoch: 25, Training Loss: 0.2361\n",
            "LR: 0.01, Batch Size: 16, Num Epochs: 25, Test Loss: 0.3530, Test Accuracy: 0.9303\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 16, Epoch: 1, Training Loss: 0.8655\n",
            "LR: 0.01, Batch Size: 16, Epoch: 2, Training Loss: 0.4428\n",
            "LR: 0.01, Batch Size: 16, Epoch: 3, Training Loss: 0.3366\n",
            "LR: 0.01, Batch Size: 16, Epoch: 4, Training Loss: 0.3278\n",
            "LR: 0.01, Batch Size: 16, Epoch: 5, Training Loss: 0.2809\n",
            "LR: 0.01, Batch Size: 16, Epoch: 6, Training Loss: 0.2980\n",
            "LR: 0.01, Batch Size: 16, Epoch: 7, Training Loss: 0.3218\n",
            "LR: 0.01, Batch Size: 16, Epoch: 8, Training Loss: 0.2193\n",
            "LR: 0.01, Batch Size: 16, Epoch: 9, Training Loss: 0.3015\n",
            "LR: 0.01, Batch Size: 16, Epoch: 10, Training Loss: 0.2814\n",
            "LR: 0.01, Batch Size: 16, Epoch: 11, Training Loss: 0.2224\n",
            "LR: 0.01, Batch Size: 16, Epoch: 12, Training Loss: 0.1823\n",
            "LR: 0.01, Batch Size: 16, Epoch: 13, Training Loss: 0.2168\n",
            "LR: 0.01, Batch Size: 16, Epoch: 14, Training Loss: 0.1688\n",
            "LR: 0.01, Batch Size: 16, Epoch: 15, Training Loss: 0.2191\n",
            "LR: 0.01, Batch Size: 16, Epoch: 16, Training Loss: 0.2358\n",
            "LR: 0.01, Batch Size: 16, Epoch: 17, Training Loss: 0.1554\n",
            "LR: 0.01, Batch Size: 16, Epoch: 18, Training Loss: 0.1695\n",
            "LR: 0.01, Batch Size: 16, Epoch: 19, Training Loss: 0.1570\n",
            "LR: 0.01, Batch Size: 16, Epoch: 20, Training Loss: 0.2097\n",
            "LR: 0.01, Batch Size: 16, Epoch: 21, Training Loss: 0.2624\n",
            "LR: 0.01, Batch Size: 16, Epoch: 22, Training Loss: 0.1922\n",
            "LR: 0.01, Batch Size: 16, Epoch: 23, Training Loss: 0.3291\n",
            "LR: 0.01, Batch Size: 16, Epoch: 24, Training Loss: 0.2836\n",
            "LR: 0.01, Batch Size: 16, Epoch: 25, Training Loss: 0.2072\n",
            "LR: 0.01, Batch Size: 16, Epoch: 26, Training Loss: 0.2659\n",
            "LR: 0.01, Batch Size: 16, Epoch: 27, Training Loss: 0.1566\n",
            "LR: 0.01, Batch Size: 16, Epoch: 28, Training Loss: 0.1604\n",
            "LR: 0.01, Batch Size: 16, Epoch: 29, Training Loss: 0.3213\n",
            "LR: 0.01, Batch Size: 16, Epoch: 30, Training Loss: 0.2358\n",
            "LR: 0.01, Batch Size: 16, Num Epochs: 30, Test Loss: 0.3542, Test Accuracy: 0.9169\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 32, Epoch: 1, Training Loss: 0.9698\n",
            "LR: 0.01, Batch Size: 32, Epoch: 2, Training Loss: 0.3692\n",
            "LR: 0.01, Batch Size: 32, Epoch: 3, Training Loss: 0.2883\n",
            "LR: 0.01, Batch Size: 32, Epoch: 4, Training Loss: 0.2567\n",
            "LR: 0.01, Batch Size: 32, Epoch: 5, Training Loss: 0.2324\n",
            "LR: 0.01, Batch Size: 32, Num Epochs: 5, Test Loss: 0.1796, Test Accuracy: 0.9416\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 32, Epoch: 1, Training Loss: 0.9352\n",
            "LR: 0.01, Batch Size: 32, Epoch: 2, Training Loss: 0.3477\n",
            "LR: 0.01, Batch Size: 32, Epoch: 3, Training Loss: 0.2675\n",
            "LR: 0.01, Batch Size: 32, Epoch: 4, Training Loss: 0.2471\n",
            "LR: 0.01, Batch Size: 32, Epoch: 5, Training Loss: 0.2194\n",
            "LR: 0.01, Batch Size: 32, Epoch: 6, Training Loss: 0.2160\n",
            "LR: 0.01, Batch Size: 32, Epoch: 7, Training Loss: 0.2284\n",
            "LR: 0.01, Batch Size: 32, Epoch: 8, Training Loss: 0.2745\n",
            "LR: 0.01, Batch Size: 32, Epoch: 9, Training Loss: 0.1790\n",
            "LR: 0.01, Batch Size: 32, Epoch: 10, Training Loss: 0.2058\n",
            "LR: 0.01, Batch Size: 32, Num Epochs: 10, Test Loss: 0.1907, Test Accuracy: 0.9438\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 32, Epoch: 1, Training Loss: 0.8402\n",
            "LR: 0.01, Batch Size: 32, Epoch: 2, Training Loss: 0.3334\n",
            "LR: 0.01, Batch Size: 32, Epoch: 3, Training Loss: 0.2638\n",
            "LR: 0.01, Batch Size: 32, Epoch: 4, Training Loss: 0.2130\n",
            "LR: 0.01, Batch Size: 32, Epoch: 5, Training Loss: 0.2707\n",
            "LR: 0.01, Batch Size: 32, Epoch: 6, Training Loss: 0.2323\n",
            "LR: 0.01, Batch Size: 32, Epoch: 7, Training Loss: 0.2296\n",
            "LR: 0.01, Batch Size: 32, Epoch: 8, Training Loss: 0.2355\n",
            "LR: 0.01, Batch Size: 32, Epoch: 9, Training Loss: 0.2284\n",
            "LR: 0.01, Batch Size: 32, Epoch: 10, Training Loss: 0.1636\n",
            "LR: 0.01, Batch Size: 32, Epoch: 11, Training Loss: 0.1607\n",
            "LR: 0.01, Batch Size: 32, Epoch: 12, Training Loss: 0.1674\n",
            "LR: 0.01, Batch Size: 32, Epoch: 13, Training Loss: 0.2079\n",
            "LR: 0.01, Batch Size: 32, Epoch: 14, Training Loss: 0.1699\n",
            "LR: 0.01, Batch Size: 32, Epoch: 15, Training Loss: 0.1729\n",
            "LR: 0.01, Batch Size: 32, Num Epochs: 15, Test Loss: 0.1689, Test Accuracy: 0.9528\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 32, Epoch: 1, Training Loss: 0.8653\n",
            "LR: 0.01, Batch Size: 32, Epoch: 2, Training Loss: 0.3336\n",
            "LR: 0.01, Batch Size: 32, Epoch: 3, Training Loss: 0.2415\n",
            "LR: 0.01, Batch Size: 32, Epoch: 4, Training Loss: 0.2460\n",
            "LR: 0.01, Batch Size: 32, Epoch: 5, Training Loss: 0.2174\n",
            "LR: 0.01, Batch Size: 32, Epoch: 6, Training Loss: 0.1903\n",
            "LR: 0.01, Batch Size: 32, Epoch: 7, Training Loss: 0.1858\n",
            "LR: 0.01, Batch Size: 32, Epoch: 8, Training Loss: 0.1923\n",
            "LR: 0.01, Batch Size: 32, Epoch: 9, Training Loss: 0.1424\n",
            "LR: 0.01, Batch Size: 32, Epoch: 10, Training Loss: 0.2223\n",
            "LR: 0.01, Batch Size: 32, Epoch: 11, Training Loss: 0.2508\n",
            "LR: 0.01, Batch Size: 32, Epoch: 12, Training Loss: 0.2070\n",
            "LR: 0.01, Batch Size: 32, Epoch: 13, Training Loss: 0.1463\n",
            "LR: 0.01, Batch Size: 32, Epoch: 14, Training Loss: 0.1233\n",
            "LR: 0.01, Batch Size: 32, Epoch: 15, Training Loss: 0.1588\n",
            "LR: 0.01, Batch Size: 32, Epoch: 16, Training Loss: 0.1361\n",
            "LR: 0.01, Batch Size: 32, Epoch: 17, Training Loss: 0.1590\n",
            "LR: 0.01, Batch Size: 32, Epoch: 18, Training Loss: 0.1370\n",
            "LR: 0.01, Batch Size: 32, Epoch: 19, Training Loss: 0.1652\n",
            "LR: 0.01, Batch Size: 32, Epoch: 20, Training Loss: 0.0838\n",
            "LR: 0.01, Batch Size: 32, Num Epochs: 20, Test Loss: 0.4609, Test Accuracy: 0.9483\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 32, Epoch: 1, Training Loss: 0.8799\n",
            "LR: 0.01, Batch Size: 32, Epoch: 2, Training Loss: 0.3298\n",
            "LR: 0.01, Batch Size: 32, Epoch: 3, Training Loss: 0.2698\n",
            "LR: 0.01, Batch Size: 32, Epoch: 4, Training Loss: 0.2560\n",
            "LR: 0.01, Batch Size: 32, Epoch: 5, Training Loss: 0.2367\n",
            "LR: 0.01, Batch Size: 32, Epoch: 6, Training Loss: 0.2067\n",
            "LR: 0.01, Batch Size: 32, Epoch: 7, Training Loss: 0.2427\n",
            "LR: 0.01, Batch Size: 32, Epoch: 8, Training Loss: 0.1857\n",
            "LR: 0.01, Batch Size: 32, Epoch: 9, Training Loss: 0.1765\n",
            "LR: 0.01, Batch Size: 32, Epoch: 10, Training Loss: 0.1446\n",
            "LR: 0.01, Batch Size: 32, Epoch: 11, Training Loss: 0.1278\n",
            "LR: 0.01, Batch Size: 32, Epoch: 12, Training Loss: 0.1493\n",
            "LR: 0.01, Batch Size: 32, Epoch: 13, Training Loss: 0.1943\n",
            "LR: 0.01, Batch Size: 32, Epoch: 14, Training Loss: 0.1691\n",
            "LR: 0.01, Batch Size: 32, Epoch: 15, Training Loss: 0.1431\n",
            "LR: 0.01, Batch Size: 32, Epoch: 16, Training Loss: 0.1837\n",
            "LR: 0.01, Batch Size: 32, Epoch: 17, Training Loss: 0.1617\n",
            "LR: 0.01, Batch Size: 32, Epoch: 18, Training Loss: 0.1030\n",
            "LR: 0.01, Batch Size: 32, Epoch: 19, Training Loss: 0.1273\n",
            "LR: 0.01, Batch Size: 32, Epoch: 20, Training Loss: 0.1473\n",
            "LR: 0.01, Batch Size: 32, Epoch: 21, Training Loss: 0.1249\n",
            "LR: 0.01, Batch Size: 32, Epoch: 22, Training Loss: 0.0873\n",
            "LR: 0.01, Batch Size: 32, Epoch: 23, Training Loss: 0.1196\n",
            "LR: 0.01, Batch Size: 32, Epoch: 24, Training Loss: 0.1137\n",
            "LR: 0.01, Batch Size: 32, Epoch: 25, Training Loss: 0.2172\n",
            "LR: 0.01, Batch Size: 32, Num Epochs: 25, Test Loss: 0.2101, Test Accuracy: 0.9506\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 32, Epoch: 1, Training Loss: 0.7763\n",
            "LR: 0.01, Batch Size: 32, Epoch: 2, Training Loss: 0.3486\n",
            "LR: 0.01, Batch Size: 32, Epoch: 3, Training Loss: 0.2255\n",
            "LR: 0.01, Batch Size: 32, Epoch: 4, Training Loss: 0.2384\n",
            "LR: 0.01, Batch Size: 32, Epoch: 5, Training Loss: 0.2062\n",
            "LR: 0.01, Batch Size: 32, Epoch: 6, Training Loss: 0.2308\n",
            "LR: 0.01, Batch Size: 32, Epoch: 7, Training Loss: 0.2358\n",
            "LR: 0.01, Batch Size: 32, Epoch: 8, Training Loss: 0.2702\n",
            "LR: 0.01, Batch Size: 32, Epoch: 9, Training Loss: 0.1785\n",
            "LR: 0.01, Batch Size: 32, Epoch: 10, Training Loss: 0.1217\n",
            "LR: 0.01, Batch Size: 32, Epoch: 11, Training Loss: 0.1741\n",
            "LR: 0.01, Batch Size: 32, Epoch: 12, Training Loss: 0.1455\n",
            "LR: 0.01, Batch Size: 32, Epoch: 13, Training Loss: 0.1659\n",
            "LR: 0.01, Batch Size: 32, Epoch: 14, Training Loss: 0.1117\n",
            "LR: 0.01, Batch Size: 32, Epoch: 15, Training Loss: 0.1441\n",
            "LR: 0.01, Batch Size: 32, Epoch: 16, Training Loss: 0.1433\n",
            "LR: 0.01, Batch Size: 32, Epoch: 17, Training Loss: 0.1224\n",
            "LR: 0.01, Batch Size: 32, Epoch: 18, Training Loss: 0.1896\n",
            "LR: 0.01, Batch Size: 32, Epoch: 19, Training Loss: 0.2007\n",
            "LR: 0.01, Batch Size: 32, Epoch: 20, Training Loss: 0.1817\n",
            "LR: 0.01, Batch Size: 32, Epoch: 21, Training Loss: 0.1071\n",
            "LR: 0.01, Batch Size: 32, Epoch: 22, Training Loss: 0.1009\n",
            "LR: 0.01, Batch Size: 32, Epoch: 23, Training Loss: 0.1493\n",
            "LR: 0.01, Batch Size: 32, Epoch: 24, Training Loss: 0.1539\n",
            "LR: 0.01, Batch Size: 32, Epoch: 25, Training Loss: 0.2293\n",
            "LR: 0.01, Batch Size: 32, Epoch: 26, Training Loss: 0.1214\n",
            "LR: 0.01, Batch Size: 32, Epoch: 27, Training Loss: 0.2405\n",
            "LR: 0.01, Batch Size: 32, Epoch: 28, Training Loss: 0.2314\n",
            "LR: 0.01, Batch Size: 32, Epoch: 29, Training Loss: 0.1537\n",
            "LR: 0.01, Batch Size: 32, Epoch: 30, Training Loss: 0.1462\n",
            "LR: 0.01, Batch Size: 32, Num Epochs: 30, Test Loss: 0.1709, Test Accuracy: 0.9483\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 64, Epoch: 1, Training Loss: 1.0616\n",
            "LR: 0.01, Batch Size: 64, Epoch: 2, Training Loss: 0.2979\n",
            "LR: 0.01, Batch Size: 64, Epoch: 3, Training Loss: 0.2577\n",
            "LR: 0.01, Batch Size: 64, Epoch: 4, Training Loss: 0.2013\n",
            "LR: 0.01, Batch Size: 64, Epoch: 5, Training Loss: 0.1613\n",
            "LR: 0.01, Batch Size: 64, Num Epochs: 5, Test Loss: 0.1891, Test Accuracy: 0.9438\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 64, Epoch: 1, Training Loss: 1.0750\n",
            "LR: 0.01, Batch Size: 64, Epoch: 2, Training Loss: 0.3567\n",
            "LR: 0.01, Batch Size: 64, Epoch: 3, Training Loss: 0.2340\n",
            "LR: 0.01, Batch Size: 64, Epoch: 4, Training Loss: 0.2005\n",
            "LR: 0.01, Batch Size: 64, Epoch: 5, Training Loss: 0.1915\n",
            "LR: 0.01, Batch Size: 64, Epoch: 6, Training Loss: 0.1763\n",
            "LR: 0.01, Batch Size: 64, Epoch: 7, Training Loss: 0.1330\n",
            "LR: 0.01, Batch Size: 64, Epoch: 8, Training Loss: 0.1250\n",
            "LR: 0.01, Batch Size: 64, Epoch: 9, Training Loss: 0.1698\n",
            "LR: 0.01, Batch Size: 64, Epoch: 10, Training Loss: 0.1238\n",
            "LR: 0.01, Batch Size: 64, Num Epochs: 10, Test Loss: 0.2292, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 64, Epoch: 1, Training Loss: 0.9545\n",
            "LR: 0.01, Batch Size: 64, Epoch: 2, Training Loss: 0.2936\n",
            "LR: 0.01, Batch Size: 64, Epoch: 3, Training Loss: 0.2177\n",
            "LR: 0.01, Batch Size: 64, Epoch: 4, Training Loss: 0.2190\n",
            "LR: 0.01, Batch Size: 64, Epoch: 5, Training Loss: 0.2155\n",
            "LR: 0.01, Batch Size: 64, Epoch: 6, Training Loss: 0.2008\n",
            "LR: 0.01, Batch Size: 64, Epoch: 7, Training Loss: 0.1377\n",
            "LR: 0.01, Batch Size: 64, Epoch: 8, Training Loss: 0.1112\n",
            "LR: 0.01, Batch Size: 64, Epoch: 9, Training Loss: 0.1388\n",
            "LR: 0.01, Batch Size: 64, Epoch: 10, Training Loss: 0.1732\n",
            "LR: 0.01, Batch Size: 64, Epoch: 11, Training Loss: 0.1546\n",
            "LR: 0.01, Batch Size: 64, Epoch: 12, Training Loss: 0.1267\n",
            "LR: 0.01, Batch Size: 64, Epoch: 13, Training Loss: 0.1263\n",
            "LR: 0.01, Batch Size: 64, Epoch: 14, Training Loss: 0.1436\n",
            "LR: 0.01, Batch Size: 64, Epoch: 15, Training Loss: 0.1039\n",
            "LR: 0.01, Batch Size: 64, Num Epochs: 15, Test Loss: 0.1504, Test Accuracy: 0.9640\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 64, Epoch: 1, Training Loss: 1.1063\n",
            "LR: 0.01, Batch Size: 64, Epoch: 2, Training Loss: 0.4807\n",
            "LR: 0.01, Batch Size: 64, Epoch: 3, Training Loss: 0.2790\n",
            "LR: 0.01, Batch Size: 64, Epoch: 4, Training Loss: 0.1958\n",
            "LR: 0.01, Batch Size: 64, Epoch: 5, Training Loss: 0.1989\n",
            "LR: 0.01, Batch Size: 64, Epoch: 6, Training Loss: 0.2052\n",
            "LR: 0.01, Batch Size: 64, Epoch: 7, Training Loss: 0.1433\n",
            "LR: 0.01, Batch Size: 64, Epoch: 8, Training Loss: 0.1431\n",
            "LR: 0.01, Batch Size: 64, Epoch: 9, Training Loss: 0.1731\n",
            "LR: 0.01, Batch Size: 64, Epoch: 10, Training Loss: 0.1405\n",
            "LR: 0.01, Batch Size: 64, Epoch: 11, Training Loss: 0.1824\n",
            "LR: 0.01, Batch Size: 64, Epoch: 12, Training Loss: 0.1087\n",
            "LR: 0.01, Batch Size: 64, Epoch: 13, Training Loss: 0.1131\n",
            "LR: 0.01, Batch Size: 64, Epoch: 14, Training Loss: 0.1179\n",
            "LR: 0.01, Batch Size: 64, Epoch: 15, Training Loss: 0.1867\n",
            "LR: 0.01, Batch Size: 64, Epoch: 16, Training Loss: 0.1095\n",
            "LR: 0.01, Batch Size: 64, Epoch: 17, Training Loss: 0.1032\n",
            "LR: 0.01, Batch Size: 64, Epoch: 18, Training Loss: 0.0836\n",
            "LR: 0.01, Batch Size: 64, Epoch: 19, Training Loss: 0.1435\n",
            "LR: 0.01, Batch Size: 64, Epoch: 20, Training Loss: 0.1045\n",
            "LR: 0.01, Batch Size: 64, Num Epochs: 20, Test Loss: 0.2195, Test Accuracy: 0.9573\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 64, Epoch: 1, Training Loss: 1.0451\n",
            "LR: 0.01, Batch Size: 64, Epoch: 2, Training Loss: 0.4078\n",
            "LR: 0.01, Batch Size: 64, Epoch: 3, Training Loss: 0.2568\n",
            "LR: 0.01, Batch Size: 64, Epoch: 4, Training Loss: 0.2008\n",
            "LR: 0.01, Batch Size: 64, Epoch: 5, Training Loss: 0.1783\n",
            "LR: 0.01, Batch Size: 64, Epoch: 6, Training Loss: 0.1873\n",
            "LR: 0.01, Batch Size: 64, Epoch: 7, Training Loss: 0.1249\n",
            "LR: 0.01, Batch Size: 64, Epoch: 8, Training Loss: 0.1473\n",
            "LR: 0.01, Batch Size: 64, Epoch: 9, Training Loss: 0.1388\n",
            "LR: 0.01, Batch Size: 64, Epoch: 10, Training Loss: 0.1502\n",
            "LR: 0.01, Batch Size: 64, Epoch: 11, Training Loss: 0.1202\n",
            "LR: 0.01, Batch Size: 64, Epoch: 12, Training Loss: 0.1088\n",
            "LR: 0.01, Batch Size: 64, Epoch: 13, Training Loss: 0.1575\n",
            "LR: 0.01, Batch Size: 64, Epoch: 14, Training Loss: 0.1513\n",
            "LR: 0.01, Batch Size: 64, Epoch: 15, Training Loss: 0.1194\n",
            "LR: 0.01, Batch Size: 64, Epoch: 16, Training Loss: 0.1228\n",
            "LR: 0.01, Batch Size: 64, Epoch: 17, Training Loss: 0.1014\n",
            "LR: 0.01, Batch Size: 64, Epoch: 18, Training Loss: 0.1343\n",
            "LR: 0.01, Batch Size: 64, Epoch: 19, Training Loss: 0.0953\n",
            "LR: 0.01, Batch Size: 64, Epoch: 20, Training Loss: 0.0979\n",
            "LR: 0.01, Batch Size: 64, Epoch: 21, Training Loss: 0.0844\n",
            "LR: 0.01, Batch Size: 64, Epoch: 22, Training Loss: 0.1247\n",
            "LR: 0.01, Batch Size: 64, Epoch: 23, Training Loss: 0.1095\n",
            "LR: 0.01, Batch Size: 64, Epoch: 24, Training Loss: 0.1467\n",
            "LR: 0.01, Batch Size: 64, Epoch: 25, Training Loss: 0.1116\n",
            "LR: 0.01, Batch Size: 64, Num Epochs: 25, Test Loss: 0.3105, Test Accuracy: 0.9483\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 64, Epoch: 1, Training Loss: 1.0996\n",
            "LR: 0.01, Batch Size: 64, Epoch: 2, Training Loss: 0.3088\n",
            "LR: 0.01, Batch Size: 64, Epoch: 3, Training Loss: 0.2371\n",
            "LR: 0.01, Batch Size: 64, Epoch: 4, Training Loss: 0.1881\n",
            "LR: 0.01, Batch Size: 64, Epoch: 5, Training Loss: 0.1961\n",
            "LR: 0.01, Batch Size: 64, Epoch: 6, Training Loss: 0.1802\n",
            "LR: 0.01, Batch Size: 64, Epoch: 7, Training Loss: 0.2170\n",
            "LR: 0.01, Batch Size: 64, Epoch: 8, Training Loss: 0.1338\n",
            "LR: 0.01, Batch Size: 64, Epoch: 9, Training Loss: 0.1272\n",
            "LR: 0.01, Batch Size: 64, Epoch: 10, Training Loss: 0.1069\n",
            "LR: 0.01, Batch Size: 64, Epoch: 11, Training Loss: 0.1358\n",
            "LR: 0.01, Batch Size: 64, Epoch: 12, Training Loss: 0.1234\n",
            "LR: 0.01, Batch Size: 64, Epoch: 13, Training Loss: 0.0806\n",
            "LR: 0.01, Batch Size: 64, Epoch: 14, Training Loss: 0.1232\n",
            "LR: 0.01, Batch Size: 64, Epoch: 15, Training Loss: 0.1365\n",
            "LR: 0.01, Batch Size: 64, Epoch: 16, Training Loss: 0.1179\n",
            "LR: 0.01, Batch Size: 64, Epoch: 17, Training Loss: 0.1564\n",
            "LR: 0.01, Batch Size: 64, Epoch: 18, Training Loss: 0.1245\n",
            "LR: 0.01, Batch Size: 64, Epoch: 19, Training Loss: 0.1287\n",
            "LR: 0.01, Batch Size: 64, Epoch: 20, Training Loss: 0.0951\n",
            "LR: 0.01, Batch Size: 64, Epoch: 21, Training Loss: 0.0966\n",
            "LR: 0.01, Batch Size: 64, Epoch: 22, Training Loss: 0.0788\n",
            "LR: 0.01, Batch Size: 64, Epoch: 23, Training Loss: 0.0917\n",
            "LR: 0.01, Batch Size: 64, Epoch: 24, Training Loss: 0.0835\n",
            "LR: 0.01, Batch Size: 64, Epoch: 25, Training Loss: 0.1275\n",
            "LR: 0.01, Batch Size: 64, Epoch: 26, Training Loss: 0.1084\n",
            "LR: 0.01, Batch Size: 64, Epoch: 27, Training Loss: 0.1013\n",
            "LR: 0.01, Batch Size: 64, Epoch: 28, Training Loss: 0.0828\n",
            "LR: 0.01, Batch Size: 64, Epoch: 29, Training Loss: 0.1065\n",
            "LR: 0.01, Batch Size: 64, Epoch: 30, Training Loss: 0.0745\n",
            "LR: 0.01, Batch Size: 64, Num Epochs: 30, Test Loss: 0.2445, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 128, Epoch: 1, Training Loss: 1.2881\n",
            "LR: 0.01, Batch Size: 128, Epoch: 2, Training Loss: 0.5911\n",
            "LR: 0.01, Batch Size: 128, Epoch: 3, Training Loss: 0.2929\n",
            "LR: 0.01, Batch Size: 128, Epoch: 4, Training Loss: 0.1840\n",
            "LR: 0.01, Batch Size: 128, Epoch: 5, Training Loss: 0.1835\n",
            "LR: 0.01, Batch Size: 128, Num Epochs: 5, Test Loss: 0.1433, Test Accuracy: 0.9483\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 128, Epoch: 1, Training Loss: 1.2395\n",
            "LR: 0.01, Batch Size: 128, Epoch: 2, Training Loss: 0.4834\n",
            "LR: 0.01, Batch Size: 128, Epoch: 3, Training Loss: 0.2673\n",
            "LR: 0.01, Batch Size: 128, Epoch: 4, Training Loss: 0.2142\n",
            "LR: 0.01, Batch Size: 128, Epoch: 5, Training Loss: 0.2036\n",
            "LR: 0.01, Batch Size: 128, Epoch: 6, Training Loss: 0.1358\n",
            "LR: 0.01, Batch Size: 128, Epoch: 7, Training Loss: 0.1283\n",
            "LR: 0.01, Batch Size: 128, Epoch: 8, Training Loss: 0.1415\n",
            "LR: 0.01, Batch Size: 128, Epoch: 9, Training Loss: 0.1131\n",
            "LR: 0.01, Batch Size: 128, Epoch: 10, Training Loss: 0.0955\n",
            "LR: 0.01, Batch Size: 128, Num Epochs: 10, Test Loss: 0.1609, Test Accuracy: 0.9551\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 128, Epoch: 1, Training Loss: 1.2791\n",
            "LR: 0.01, Batch Size: 128, Epoch: 2, Training Loss: 0.5181\n",
            "LR: 0.01, Batch Size: 128, Epoch: 3, Training Loss: 0.2952\n",
            "LR: 0.01, Batch Size: 128, Epoch: 4, Training Loss: 0.2351\n",
            "LR: 0.01, Batch Size: 128, Epoch: 5, Training Loss: 0.1799\n",
            "LR: 0.01, Batch Size: 128, Epoch: 6, Training Loss: 0.1716\n",
            "LR: 0.01, Batch Size: 128, Epoch: 7, Training Loss: 0.1560\n",
            "LR: 0.01, Batch Size: 128, Epoch: 8, Training Loss: 0.1428\n",
            "LR: 0.01, Batch Size: 128, Epoch: 9, Training Loss: 0.0954\n",
            "LR: 0.01, Batch Size: 128, Epoch: 10, Training Loss: 0.1078\n",
            "LR: 0.01, Batch Size: 128, Epoch: 11, Training Loss: 0.1153\n",
            "LR: 0.01, Batch Size: 128, Epoch: 12, Training Loss: 0.0882\n",
            "LR: 0.01, Batch Size: 128, Epoch: 13, Training Loss: 0.0785\n",
            "LR: 0.01, Batch Size: 128, Epoch: 14, Training Loss: 0.1135\n",
            "LR: 0.01, Batch Size: 128, Epoch: 15, Training Loss: 0.0833\n",
            "LR: 0.01, Batch Size: 128, Num Epochs: 15, Test Loss: 0.2155, Test Accuracy: 0.9528\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 128, Epoch: 1, Training Loss: 1.2039\n",
            "LR: 0.01, Batch Size: 128, Epoch: 2, Training Loss: 0.4734\n",
            "LR: 0.01, Batch Size: 128, Epoch: 3, Training Loss: 0.2740\n",
            "LR: 0.01, Batch Size: 128, Epoch: 4, Training Loss: 0.2209\n",
            "LR: 0.01, Batch Size: 128, Epoch: 5, Training Loss: 0.1841\n",
            "LR: 0.01, Batch Size: 128, Epoch: 6, Training Loss: 0.1804\n",
            "LR: 0.01, Batch Size: 128, Epoch: 7, Training Loss: 0.1300\n",
            "LR: 0.01, Batch Size: 128, Epoch: 8, Training Loss: 0.1248\n",
            "LR: 0.01, Batch Size: 128, Epoch: 9, Training Loss: 0.1073\n",
            "LR: 0.01, Batch Size: 128, Epoch: 10, Training Loss: 0.1068\n",
            "LR: 0.01, Batch Size: 128, Epoch: 11, Training Loss: 0.1057\n",
            "LR: 0.01, Batch Size: 128, Epoch: 12, Training Loss: 0.1013\n",
            "LR: 0.01, Batch Size: 128, Epoch: 13, Training Loss: 0.1130\n",
            "LR: 0.01, Batch Size: 128, Epoch: 14, Training Loss: 0.0905\n",
            "LR: 0.01, Batch Size: 128, Epoch: 15, Training Loss: 0.0684\n",
            "LR: 0.01, Batch Size: 128, Epoch: 16, Training Loss: 0.0731\n",
            "LR: 0.01, Batch Size: 128, Epoch: 17, Training Loss: 0.0610\n",
            "LR: 0.01, Batch Size: 128, Epoch: 18, Training Loss: 0.0744\n",
            "LR: 0.01, Batch Size: 128, Epoch: 19, Training Loss: 0.0981\n",
            "LR: 0.01, Batch Size: 128, Epoch: 20, Training Loss: 0.0647\n",
            "LR: 0.01, Batch Size: 128, Num Epochs: 20, Test Loss: 0.3058, Test Accuracy: 0.9326\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 128, Epoch: 1, Training Loss: 1.2482\n",
            "LR: 0.01, Batch Size: 128, Epoch: 2, Training Loss: 0.4648\n",
            "LR: 0.01, Batch Size: 128, Epoch: 3, Training Loss: 0.2834\n",
            "LR: 0.01, Batch Size: 128, Epoch: 4, Training Loss: 0.1797\n",
            "LR: 0.01, Batch Size: 128, Epoch: 5, Training Loss: 0.1516\n",
            "LR: 0.01, Batch Size: 128, Epoch: 6, Training Loss: 0.1547\n",
            "LR: 0.01, Batch Size: 128, Epoch: 7, Training Loss: 0.1208\n",
            "LR: 0.01, Batch Size: 128, Epoch: 8, Training Loss: 0.1075\n",
            "LR: 0.01, Batch Size: 128, Epoch: 9, Training Loss: 0.1185\n",
            "LR: 0.01, Batch Size: 128, Epoch: 10, Training Loss: 0.0984\n",
            "LR: 0.01, Batch Size: 128, Epoch: 11, Training Loss: 0.0822\n",
            "LR: 0.01, Batch Size: 128, Epoch: 12, Training Loss: 0.0912\n",
            "LR: 0.01, Batch Size: 128, Epoch: 13, Training Loss: 0.0868\n",
            "LR: 0.01, Batch Size: 128, Epoch: 14, Training Loss: 0.1186\n",
            "LR: 0.01, Batch Size: 128, Epoch: 15, Training Loss: 0.0918\n",
            "LR: 0.01, Batch Size: 128, Epoch: 16, Training Loss: 0.0810\n",
            "LR: 0.01, Batch Size: 128, Epoch: 17, Training Loss: 0.0558\n",
            "LR: 0.01, Batch Size: 128, Epoch: 18, Training Loss: 0.0432\n",
            "LR: 0.01, Batch Size: 128, Epoch: 19, Training Loss: 0.0965\n",
            "LR: 0.01, Batch Size: 128, Epoch: 20, Training Loss: 0.0826\n",
            "LR: 0.01, Batch Size: 128, Epoch: 21, Training Loss: 0.0800\n",
            "LR: 0.01, Batch Size: 128, Epoch: 22, Training Loss: 0.0567\n",
            "LR: 0.01, Batch Size: 128, Epoch: 23, Training Loss: 0.0813\n",
            "LR: 0.01, Batch Size: 128, Epoch: 24, Training Loss: 0.0869\n",
            "LR: 0.01, Batch Size: 128, Epoch: 25, Training Loss: 0.0446\n",
            "LR: 0.01, Batch Size: 128, Num Epochs: 25, Test Loss: 0.2612, Test Accuracy: 0.9506\n",
            "\n",
            "\n",
            "LR: 0.01, Batch Size: 128, Epoch: 1, Training Loss: 1.1369\n",
            "LR: 0.01, Batch Size: 128, Epoch: 2, Training Loss: 0.4883\n",
            "LR: 0.01, Batch Size: 128, Epoch: 3, Training Loss: 0.2823\n",
            "LR: 0.01, Batch Size: 128, Epoch: 4, Training Loss: 0.2127\n",
            "LR: 0.01, Batch Size: 128, Epoch: 5, Training Loss: 0.1442\n",
            "LR: 0.01, Batch Size: 128, Epoch: 6, Training Loss: 0.1526\n",
            "LR: 0.01, Batch Size: 128, Epoch: 7, Training Loss: 0.1318\n",
            "LR: 0.01, Batch Size: 128, Epoch: 8, Training Loss: 0.1368\n",
            "LR: 0.01, Batch Size: 128, Epoch: 9, Training Loss: 0.1181\n",
            "LR: 0.01, Batch Size: 128, Epoch: 10, Training Loss: 0.1120\n",
            "LR: 0.01, Batch Size: 128, Epoch: 11, Training Loss: 0.1163\n",
            "LR: 0.01, Batch Size: 128, Epoch: 12, Training Loss: 0.0870\n",
            "LR: 0.01, Batch Size: 128, Epoch: 13, Training Loss: 0.0791\n",
            "LR: 0.01, Batch Size: 128, Epoch: 14, Training Loss: 0.0560\n",
            "LR: 0.01, Batch Size: 128, Epoch: 15, Training Loss: 0.1133\n",
            "LR: 0.01, Batch Size: 128, Epoch: 16, Training Loss: 0.0823\n",
            "LR: 0.01, Batch Size: 128, Epoch: 17, Training Loss: 0.0996\n",
            "LR: 0.01, Batch Size: 128, Epoch: 18, Training Loss: 0.0879\n",
            "LR: 0.01, Batch Size: 128, Epoch: 19, Training Loss: 0.0824\n",
            "LR: 0.01, Batch Size: 128, Epoch: 20, Training Loss: 0.0688\n",
            "LR: 0.01, Batch Size: 128, Epoch: 21, Training Loss: 0.0782\n",
            "LR: 0.01, Batch Size: 128, Epoch: 22, Training Loss: 0.0922\n",
            "LR: 0.01, Batch Size: 128, Epoch: 23, Training Loss: 0.0716\n",
            "LR: 0.01, Batch Size: 128, Epoch: 24, Training Loss: 0.0636\n",
            "LR: 0.01, Batch Size: 128, Epoch: 25, Training Loss: 0.0795\n",
            "LR: 0.01, Batch Size: 128, Epoch: 26, Training Loss: 0.0733\n",
            "LR: 0.01, Batch Size: 128, Epoch: 27, Training Loss: 0.0862\n",
            "LR: 0.01, Batch Size: 128, Epoch: 28, Training Loss: 0.0622\n",
            "LR: 0.01, Batch Size: 128, Epoch: 29, Training Loss: 0.0437\n",
            "LR: 0.01, Batch Size: 128, Epoch: 30, Training Loss: 0.0402\n",
            "LR: 0.01, Batch Size: 128, Num Epochs: 30, Test Loss: 0.2520, Test Accuracy: 0.9573\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 8, Epoch: 1, Training Loss: 2.1476\n",
            "LR: 0.1, Batch Size: 8, Epoch: 2, Training Loss: 1.7312\n",
            "LR: 0.1, Batch Size: 8, Epoch: 3, Training Loss: 1.6570\n",
            "LR: 0.1, Batch Size: 8, Epoch: 4, Training Loss: 1.6219\n",
            "LR: 0.1, Batch Size: 8, Epoch: 5, Training Loss: 1.8095\n",
            "LR: 0.1, Batch Size: 8, Num Epochs: 5, Test Loss: 1.6248, Test Accuracy: 0.1865\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 8, Epoch: 1, Training Loss: 1.7450\n",
            "LR: 0.1, Batch Size: 8, Epoch: 2, Training Loss: 1.6590\n",
            "LR: 0.1, Batch Size: 8, Epoch: 3, Training Loss: 1.6227\n",
            "LR: 0.1, Batch Size: 8, Epoch: 4, Training Loss: 1.6491\n",
            "LR: 0.1, Batch Size: 8, Epoch: 5, Training Loss: 1.6180\n",
            "LR: 0.1, Batch Size: 8, Epoch: 6, Training Loss: 1.6228\n",
            "LR: 0.1, Batch Size: 8, Epoch: 7, Training Loss: 1.6169\n",
            "LR: 0.1, Batch Size: 8, Epoch: 8, Training Loss: 1.6194\n",
            "LR: 0.1, Batch Size: 8, Epoch: 9, Training Loss: 1.6868\n",
            "LR: 0.1, Batch Size: 8, Epoch: 10, Training Loss: 1.6212\n",
            "LR: 0.1, Batch Size: 8, Num Epochs: 10, Test Loss: 1.6256, Test Accuracy: 0.2202\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 8, Epoch: 1, Training Loss: 1.7688\n",
            "LR: 0.1, Batch Size: 8, Epoch: 2, Training Loss: 1.6180\n",
            "LR: 0.1, Batch Size: 8, Epoch: 3, Training Loss: 1.6169\n",
            "LR: 0.1, Batch Size: 8, Epoch: 4, Training Loss: 1.6173\n",
            "LR: 0.1, Batch Size: 8, Epoch: 5, Training Loss: 1.6160\n",
            "LR: 0.1, Batch Size: 8, Epoch: 6, Training Loss: 1.6208\n",
            "LR: 0.1, Batch Size: 8, Epoch: 7, Training Loss: 1.6138\n",
            "LR: 0.1, Batch Size: 8, Epoch: 8, Training Loss: 1.6168\n",
            "LR: 0.1, Batch Size: 8, Epoch: 9, Training Loss: 1.6136\n",
            "LR: 0.1, Batch Size: 8, Epoch: 10, Training Loss: 1.6180\n",
            "LR: 0.1, Batch Size: 8, Epoch: 11, Training Loss: 1.6252\n",
            "LR: 0.1, Batch Size: 8, Epoch: 12, Training Loss: 1.6190\n",
            "LR: 0.1, Batch Size: 8, Epoch: 13, Training Loss: 1.6182\n",
            "LR: 0.1, Batch Size: 8, Epoch: 14, Training Loss: 1.6221\n",
            "LR: 0.1, Batch Size: 8, Epoch: 15, Training Loss: 1.6200\n",
            "LR: 0.1, Batch Size: 8, Num Epochs: 15, Test Loss: 1.6164, Test Accuracy: 0.2270\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 8, Epoch: 1, Training Loss: 1.7025\n",
            "LR: 0.1, Batch Size: 8, Epoch: 2, Training Loss: 1.7558\n",
            "LR: 0.1, Batch Size: 8, Epoch: 3, Training Loss: 1.6386\n",
            "LR: 0.1, Batch Size: 8, Epoch: 4, Training Loss: 1.6223\n",
            "LR: 0.1, Batch Size: 8, Epoch: 5, Training Loss: 1.6238\n",
            "LR: 0.1, Batch Size: 8, Epoch: 6, Training Loss: 1.6366\n",
            "LR: 0.1, Batch Size: 8, Epoch: 7, Training Loss: 1.6179\n",
            "LR: 0.1, Batch Size: 8, Epoch: 8, Training Loss: 1.6173\n",
            "LR: 0.1, Batch Size: 8, Epoch: 9, Training Loss: 1.6205\n",
            "LR: 0.1, Batch Size: 8, Epoch: 10, Training Loss: 1.6263\n",
            "LR: 0.1, Batch Size: 8, Epoch: 11, Training Loss: 1.6166\n",
            "LR: 0.1, Batch Size: 8, Epoch: 12, Training Loss: 1.6187\n",
            "LR: 0.1, Batch Size: 8, Epoch: 13, Training Loss: 1.6196\n",
            "LR: 0.1, Batch Size: 8, Epoch: 14, Training Loss: 1.6346\n",
            "LR: 0.1, Batch Size: 8, Epoch: 15, Training Loss: 1.6231\n",
            "LR: 0.1, Batch Size: 8, Epoch: 16, Training Loss: 1.6144\n",
            "LR: 0.1, Batch Size: 8, Epoch: 17, Training Loss: 1.6353\n",
            "LR: 0.1, Batch Size: 8, Epoch: 18, Training Loss: 1.6207\n",
            "LR: 0.1, Batch Size: 8, Epoch: 19, Training Loss: 1.6165\n",
            "LR: 0.1, Batch Size: 8, Epoch: 20, Training Loss: 1.6204\n",
            "LR: 0.1, Batch Size: 8, Num Epochs: 20, Test Loss: 1.6218, Test Accuracy: 0.1865\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 8, Epoch: 1, Training Loss: 1.9799\n",
            "LR: 0.1, Batch Size: 8, Epoch: 2, Training Loss: 1.6138\n",
            "LR: 0.1, Batch Size: 8, Epoch: 3, Training Loss: 2.7068\n",
            "LR: 0.1, Batch Size: 8, Epoch: 4, Training Loss: 1.6308\n",
            "LR: 0.1, Batch Size: 8, Epoch: 5, Training Loss: 1.6252\n",
            "LR: 0.1, Batch Size: 8, Epoch: 6, Training Loss: 1.6265\n",
            "LR: 0.1, Batch Size: 8, Epoch: 7, Training Loss: 1.6142\n",
            "LR: 0.1, Batch Size: 8, Epoch: 8, Training Loss: 1.6223\n",
            "LR: 0.1, Batch Size: 8, Epoch: 9, Training Loss: 1.6300\n",
            "LR: 0.1, Batch Size: 8, Epoch: 10, Training Loss: 1.6259\n",
            "LR: 0.1, Batch Size: 8, Epoch: 11, Training Loss: 1.6205\n",
            "LR: 0.1, Batch Size: 8, Epoch: 12, Training Loss: 1.6876\n",
            "LR: 0.1, Batch Size: 8, Epoch: 13, Training Loss: 1.6340\n",
            "LR: 0.1, Batch Size: 8, Epoch: 14, Training Loss: 1.6213\n",
            "LR: 0.1, Batch Size: 8, Epoch: 15, Training Loss: 2.3549\n",
            "LR: 0.1, Batch Size: 8, Epoch: 16, Training Loss: 1.6148\n",
            "LR: 0.1, Batch Size: 8, Epoch: 17, Training Loss: 1.6241\n",
            "LR: 0.1, Batch Size: 8, Epoch: 18, Training Loss: 1.6178\n",
            "LR: 0.1, Batch Size: 8, Epoch: 19, Training Loss: 1.6174\n",
            "LR: 0.1, Batch Size: 8, Epoch: 20, Training Loss: 1.6221\n",
            "LR: 0.1, Batch Size: 8, Epoch: 21, Training Loss: 1.6133\n",
            "LR: 0.1, Batch Size: 8, Epoch: 22, Training Loss: 1.6193\n",
            "LR: 0.1, Batch Size: 8, Epoch: 23, Training Loss: 1.6196\n",
            "LR: 0.1, Batch Size: 8, Epoch: 24, Training Loss: 1.6222\n",
            "LR: 0.1, Batch Size: 8, Epoch: 25, Training Loss: 1.6136\n",
            "LR: 0.1, Batch Size: 8, Num Epochs: 25, Test Loss: 1.6152, Test Accuracy: 0.1820\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 8, Epoch: 1, Training Loss: 1.9271\n",
            "LR: 0.1, Batch Size: 8, Epoch: 2, Training Loss: 4.6806\n",
            "LR: 0.1, Batch Size: 8, Epoch: 3, Training Loss: 1.7209\n",
            "LR: 0.1, Batch Size: 8, Epoch: 4, Training Loss: 1.6360\n",
            "LR: 0.1, Batch Size: 8, Epoch: 5, Training Loss: 1.6225\n",
            "LR: 0.1, Batch Size: 8, Epoch: 6, Training Loss: 1.6162\n",
            "LR: 0.1, Batch Size: 8, Epoch: 7, Training Loss: 1.6166\n",
            "LR: 0.1, Batch Size: 8, Epoch: 8, Training Loss: 1.6192\n",
            "LR: 0.1, Batch Size: 8, Epoch: 9, Training Loss: 1.6351\n",
            "LR: 0.1, Batch Size: 8, Epoch: 10, Training Loss: 1.6210\n",
            "LR: 0.1, Batch Size: 8, Epoch: 11, Training Loss: 1.6200\n",
            "LR: 0.1, Batch Size: 8, Epoch: 12, Training Loss: 1.6217\n",
            "LR: 0.1, Batch Size: 8, Epoch: 13, Training Loss: 1.6199\n",
            "LR: 0.1, Batch Size: 8, Epoch: 14, Training Loss: 1.6210\n",
            "LR: 0.1, Batch Size: 8, Epoch: 15, Training Loss: 1.6178\n",
            "LR: 0.1, Batch Size: 8, Epoch: 16, Training Loss: 1.6217\n",
            "LR: 0.1, Batch Size: 8, Epoch: 17, Training Loss: 1.6147\n",
            "LR: 0.1, Batch Size: 8, Epoch: 18, Training Loss: 1.6209\n",
            "LR: 0.1, Batch Size: 8, Epoch: 19, Training Loss: 1.6211\n",
            "LR: 0.1, Batch Size: 8, Epoch: 20, Training Loss: 1.6266\n",
            "LR: 0.1, Batch Size: 8, Epoch: 21, Training Loss: 1.6277\n",
            "LR: 0.1, Batch Size: 8, Epoch: 22, Training Loss: 1.6213\n",
            "LR: 0.1, Batch Size: 8, Epoch: 23, Training Loss: 1.6224\n",
            "LR: 0.1, Batch Size: 8, Epoch: 24, Training Loss: 1.6188\n",
            "LR: 0.1, Batch Size: 8, Epoch: 25, Training Loss: 1.6205\n",
            "LR: 0.1, Batch Size: 8, Epoch: 26, Training Loss: 1.6172\n",
            "LR: 0.1, Batch Size: 8, Epoch: 27, Training Loss: 1.6167\n",
            "LR: 0.1, Batch Size: 8, Epoch: 28, Training Loss: 1.6187\n",
            "LR: 0.1, Batch Size: 8, Epoch: 29, Training Loss: 1.6172\n",
            "LR: 0.1, Batch Size: 8, Epoch: 30, Training Loss: 1.6241\n",
            "LR: 0.1, Batch Size: 8, Num Epochs: 30, Test Loss: 1.6059, Test Accuracy: 0.2270\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 16, Epoch: 1, Training Loss: 1.8803\n",
            "LR: 0.1, Batch Size: 16, Epoch: 2, Training Loss: 1.6163\n",
            "LR: 0.1, Batch Size: 16, Epoch: 3, Training Loss: 1.6192\n",
            "LR: 0.1, Batch Size: 16, Epoch: 4, Training Loss: 1.6286\n",
            "LR: 0.1, Batch Size: 16, Epoch: 5, Training Loss: 1.6083\n",
            "LR: 0.1, Batch Size: 16, Num Epochs: 5, Test Loss: 1.6186, Test Accuracy: 0.2270\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 16, Epoch: 1, Training Loss: 1.8207\n",
            "LR: 0.1, Batch Size: 16, Epoch: 2, Training Loss: 1.6170\n",
            "LR: 0.1, Batch Size: 16, Epoch: 3, Training Loss: 1.6144\n",
            "LR: 0.1, Batch Size: 16, Epoch: 4, Training Loss: 1.6143\n",
            "LR: 0.1, Batch Size: 16, Epoch: 5, Training Loss: 1.6123\n",
            "LR: 0.1, Batch Size: 16, Epoch: 6, Training Loss: 1.6144\n",
            "LR: 0.1, Batch Size: 16, Epoch: 7, Training Loss: 1.6134\n",
            "LR: 0.1, Batch Size: 16, Epoch: 8, Training Loss: 1.6128\n",
            "LR: 0.1, Batch Size: 16, Epoch: 9, Training Loss: 1.6102\n",
            "LR: 0.1, Batch Size: 16, Epoch: 10, Training Loss: 1.6124\n",
            "LR: 0.1, Batch Size: 16, Num Epochs: 10, Test Loss: 1.6111, Test Accuracy: 0.2202\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 16, Epoch: 1, Training Loss: 1.7795\n",
            "LR: 0.1, Batch Size: 16, Epoch: 2, Training Loss: 1.6749\n",
            "LR: 0.1, Batch Size: 16, Epoch: 3, Training Loss: 1.6310\n",
            "LR: 0.1, Batch Size: 16, Epoch: 4, Training Loss: 1.6109\n",
            "LR: 0.1, Batch Size: 16, Epoch: 5, Training Loss: 1.6136\n",
            "LR: 0.1, Batch Size: 16, Epoch: 6, Training Loss: 1.6161\n",
            "LR: 0.1, Batch Size: 16, Epoch: 7, Training Loss: 1.6162\n",
            "LR: 0.1, Batch Size: 16, Epoch: 8, Training Loss: 1.6165\n",
            "LR: 0.1, Batch Size: 16, Epoch: 9, Training Loss: 1.6126\n",
            "LR: 0.1, Batch Size: 16, Epoch: 10, Training Loss: 1.6119\n",
            "LR: 0.1, Batch Size: 16, Epoch: 11, Training Loss: 1.6143\n",
            "LR: 0.1, Batch Size: 16, Epoch: 12, Training Loss: 1.6139\n",
            "LR: 0.1, Batch Size: 16, Epoch: 13, Training Loss: 1.6118\n",
            "LR: 0.1, Batch Size: 16, Epoch: 14, Training Loss: 1.6170\n",
            "LR: 0.1, Batch Size: 16, Epoch: 15, Training Loss: 1.6187\n",
            "LR: 0.1, Batch Size: 16, Num Epochs: 15, Test Loss: 1.6165, Test Accuracy: 0.2270\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 16, Epoch: 1, Training Loss: 1.7016\n",
            "LR: 0.1, Batch Size: 16, Epoch: 2, Training Loss: 1.6435\n",
            "LR: 0.1, Batch Size: 16, Epoch: 3, Training Loss: 1.9023\n",
            "LR: 0.1, Batch Size: 16, Epoch: 4, Training Loss: 1.6132\n",
            "LR: 0.1, Batch Size: 16, Epoch: 5, Training Loss: 1.6190\n",
            "LR: 0.1, Batch Size: 16, Epoch: 6, Training Loss: 1.6084\n",
            "LR: 0.1, Batch Size: 16, Epoch: 7, Training Loss: 1.6658\n",
            "LR: 0.1, Batch Size: 16, Epoch: 8, Training Loss: 1.6120\n",
            "LR: 0.1, Batch Size: 16, Epoch: 9, Training Loss: 1.6160\n",
            "LR: 0.1, Batch Size: 16, Epoch: 10, Training Loss: 1.6132\n",
            "LR: 0.1, Batch Size: 16, Epoch: 11, Training Loss: 1.6189\n",
            "LR: 0.1, Batch Size: 16, Epoch: 12, Training Loss: 4.0719\n",
            "LR: 0.1, Batch Size: 16, Epoch: 13, Training Loss: 1.7751\n",
            "LR: 0.1, Batch Size: 16, Epoch: 14, Training Loss: 1.6973\n",
            "LR: 0.1, Batch Size: 16, Epoch: 15, Training Loss: 2.1710\n",
            "LR: 0.1, Batch Size: 16, Epoch: 16, Training Loss: 1.6159\n",
            "LR: 0.1, Batch Size: 16, Epoch: 17, Training Loss: 1.6134\n",
            "LR: 0.1, Batch Size: 16, Epoch: 18, Training Loss: 1.6191\n",
            "LR: 0.1, Batch Size: 16, Epoch: 19, Training Loss: 1.6179\n",
            "LR: 0.1, Batch Size: 16, Epoch: 20, Training Loss: 1.6125\n",
            "LR: 0.1, Batch Size: 16, Num Epochs: 20, Test Loss: 1.6113, Test Accuracy: 0.2202\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 16, Epoch: 1, Training Loss: 2.1271\n",
            "LR: 0.1, Batch Size: 16, Epoch: 2, Training Loss: 1.6671\n",
            "LR: 0.1, Batch Size: 16, Epoch: 3, Training Loss: 1.6162\n",
            "LR: 0.1, Batch Size: 16, Epoch: 4, Training Loss: 1.8607\n",
            "LR: 0.1, Batch Size: 16, Epoch: 5, Training Loss: 1.6161\n",
            "LR: 0.1, Batch Size: 16, Epoch: 6, Training Loss: 1.6131\n",
            "LR: 0.1, Batch Size: 16, Epoch: 7, Training Loss: 1.6122\n",
            "LR: 0.1, Batch Size: 16, Epoch: 8, Training Loss: 1.6207\n",
            "LR: 0.1, Batch Size: 16, Epoch: 9, Training Loss: 1.6156\n",
            "LR: 0.1, Batch Size: 16, Epoch: 10, Training Loss: 2.3546\n",
            "LR: 0.1, Batch Size: 16, Epoch: 11, Training Loss: 1.6175\n",
            "LR: 0.1, Batch Size: 16, Epoch: 12, Training Loss: 1.6126\n",
            "LR: 0.1, Batch Size: 16, Epoch: 13, Training Loss: 1.6160\n",
            "LR: 0.1, Batch Size: 16, Epoch: 14, Training Loss: 1.6149\n",
            "LR: 0.1, Batch Size: 16, Epoch: 15, Training Loss: 1.6143\n",
            "LR: 0.1, Batch Size: 16, Epoch: 16, Training Loss: 1.6203\n",
            "LR: 0.1, Batch Size: 16, Epoch: 17, Training Loss: 1.6133\n",
            "LR: 0.1, Batch Size: 16, Epoch: 18, Training Loss: 1.6146\n",
            "LR: 0.1, Batch Size: 16, Epoch: 19, Training Loss: 1.6147\n",
            "LR: 0.1, Batch Size: 16, Epoch: 20, Training Loss: 1.6091\n",
            "LR: 0.1, Batch Size: 16, Epoch: 21, Training Loss: 1.6100\n",
            "LR: 0.1, Batch Size: 16, Epoch: 22, Training Loss: 1.6194\n",
            "LR: 0.1, Batch Size: 16, Epoch: 23, Training Loss: 1.6120\n",
            "LR: 0.1, Batch Size: 16, Epoch: 24, Training Loss: 1.6131\n",
            "LR: 0.1, Batch Size: 16, Epoch: 25, Training Loss: 1.6173\n",
            "LR: 0.1, Batch Size: 16, Num Epochs: 25, Test Loss: 1.6110, Test Accuracy: 0.1843\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 16, Epoch: 1, Training Loss: 2.2538\n",
            "LR: 0.1, Batch Size: 16, Epoch: 2, Training Loss: 1.6322\n",
            "LR: 0.1, Batch Size: 16, Epoch: 3, Training Loss: 1.6091\n",
            "LR: 0.1, Batch Size: 16, Epoch: 4, Training Loss: 1.7029\n",
            "LR: 0.1, Batch Size: 16, Epoch: 5, Training Loss: 1.6271\n",
            "LR: 0.1, Batch Size: 16, Epoch: 6, Training Loss: 1.6110\n",
            "LR: 0.1, Batch Size: 16, Epoch: 7, Training Loss: 1.6296\n",
            "LR: 0.1, Batch Size: 16, Epoch: 8, Training Loss: 1.6198\n",
            "LR: 0.1, Batch Size: 16, Epoch: 9, Training Loss: 1.6138\n",
            "LR: 0.1, Batch Size: 16, Epoch: 10, Training Loss: 1.6185\n",
            "LR: 0.1, Batch Size: 16, Epoch: 11, Training Loss: 1.6155\n",
            "LR: 0.1, Batch Size: 16, Epoch: 12, Training Loss: 1.6121\n",
            "LR: 0.1, Batch Size: 16, Epoch: 13, Training Loss: 1.6170\n",
            "LR: 0.1, Batch Size: 16, Epoch: 14, Training Loss: 1.6089\n",
            "LR: 0.1, Batch Size: 16, Epoch: 15, Training Loss: 1.6187\n",
            "LR: 0.1, Batch Size: 16, Epoch: 16, Training Loss: 1.6154\n",
            "LR: 0.1, Batch Size: 16, Epoch: 17, Training Loss: 1.6086\n",
            "LR: 0.1, Batch Size: 16, Epoch: 18, Training Loss: 1.6142\n",
            "LR: 0.1, Batch Size: 16, Epoch: 19, Training Loss: 1.6145\n",
            "LR: 0.1, Batch Size: 16, Epoch: 20, Training Loss: 1.6106\n",
            "LR: 0.1, Batch Size: 16, Epoch: 21, Training Loss: 1.6126\n",
            "LR: 0.1, Batch Size: 16, Epoch: 22, Training Loss: 1.6151\n",
            "LR: 0.1, Batch Size: 16, Epoch: 23, Training Loss: 1.6139\n",
            "LR: 0.1, Batch Size: 16, Epoch: 24, Training Loss: 1.6173\n",
            "LR: 0.1, Batch Size: 16, Epoch: 25, Training Loss: 1.6130\n",
            "LR: 0.1, Batch Size: 16, Epoch: 26, Training Loss: 1.6229\n",
            "LR: 0.1, Batch Size: 16, Epoch: 27, Training Loss: 1.6124\n",
            "LR: 0.1, Batch Size: 16, Epoch: 28, Training Loss: 1.6151\n",
            "LR: 0.1, Batch Size: 16, Epoch: 29, Training Loss: 1.6117\n",
            "LR: 0.1, Batch Size: 16, Epoch: 30, Training Loss: 1.6123\n",
            "LR: 0.1, Batch Size: 16, Num Epochs: 30, Test Loss: 1.6119, Test Accuracy: 0.2270\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 32, Epoch: 1, Training Loss: 2.1891\n",
            "LR: 0.1, Batch Size: 32, Epoch: 2, Training Loss: 1.8916\n",
            "LR: 0.1, Batch Size: 32, Epoch: 3, Training Loss: 1.5727\n",
            "LR: 0.1, Batch Size: 32, Epoch: 4, Training Loss: 1.6122\n",
            "LR: 0.1, Batch Size: 32, Epoch: 5, Training Loss: 1.6718\n",
            "LR: 0.1, Batch Size: 32, Num Epochs: 5, Test Loss: 1.6095, Test Accuracy: 0.2270\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 32, Epoch: 1, Training Loss: 2.0935\n",
            "LR: 0.1, Batch Size: 32, Epoch: 2, Training Loss: 1.6133\n",
            "LR: 0.1, Batch Size: 32, Epoch: 3, Training Loss: 1.6538\n",
            "LR: 0.1, Batch Size: 32, Epoch: 4, Training Loss: 1.6465\n",
            "LR: 0.1, Batch Size: 32, Epoch: 5, Training Loss: 1.8387\n",
            "LR: 0.1, Batch Size: 32, Epoch: 6, Training Loss: 1.6302\n",
            "LR: 0.1, Batch Size: 32, Epoch: 7, Training Loss: 2.2385\n",
            "LR: 0.1, Batch Size: 32, Epoch: 8, Training Loss: 1.6070\n",
            "LR: 0.1, Batch Size: 32, Epoch: 9, Training Loss: 1.6084\n",
            "LR: 0.1, Batch Size: 32, Epoch: 10, Training Loss: 1.6150\n",
            "LR: 0.1, Batch Size: 32, Num Epochs: 10, Test Loss: 1.6051, Test Accuracy: 0.2270\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 32, Epoch: 1, Training Loss: 1.9837\n",
            "LR: 0.1, Batch Size: 32, Epoch: 2, Training Loss: 1.6085\n",
            "LR: 0.1, Batch Size: 32, Epoch: 3, Training Loss: 1.6468\n",
            "LR: 0.1, Batch Size: 32, Epoch: 4, Training Loss: 1.6108\n",
            "LR: 0.1, Batch Size: 32, Epoch: 5, Training Loss: 1.6170\n",
            "LR: 0.1, Batch Size: 32, Epoch: 6, Training Loss: 1.6106\n",
            "LR: 0.1, Batch Size: 32, Epoch: 7, Training Loss: 1.6125\n",
            "LR: 0.1, Batch Size: 32, Epoch: 8, Training Loss: 1.6064\n",
            "LR: 0.1, Batch Size: 32, Epoch: 9, Training Loss: 1.6103\n",
            "LR: 0.1, Batch Size: 32, Epoch: 10, Training Loss: 1.6113\n",
            "LR: 0.1, Batch Size: 32, Epoch: 11, Training Loss: 1.6183\n",
            "LR: 0.1, Batch Size: 32, Epoch: 12, Training Loss: 1.6150\n",
            "LR: 0.1, Batch Size: 32, Epoch: 13, Training Loss: 1.6090\n",
            "LR: 0.1, Batch Size: 32, Epoch: 14, Training Loss: 1.6135\n",
            "LR: 0.1, Batch Size: 32, Epoch: 15, Training Loss: 1.6083\n",
            "LR: 0.1, Batch Size: 32, Num Epochs: 15, Test Loss: 1.6113, Test Accuracy: 0.2270\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 32, Epoch: 1, Training Loss: 1.8731\n",
            "LR: 0.1, Batch Size: 32, Epoch: 2, Training Loss: 1.6587\n",
            "LR: 0.1, Batch Size: 32, Epoch: 3, Training Loss: 1.6104\n",
            "LR: 0.1, Batch Size: 32, Epoch: 4, Training Loss: 1.6453\n",
            "LR: 0.1, Batch Size: 32, Epoch: 5, Training Loss: 1.6094\n",
            "LR: 0.1, Batch Size: 32, Epoch: 6, Training Loss: 1.6750\n",
            "LR: 0.1, Batch Size: 32, Epoch: 7, Training Loss: 1.6320\n",
            "LR: 0.1, Batch Size: 32, Epoch: 8, Training Loss: 1.6099\n",
            "LR: 0.1, Batch Size: 32, Epoch: 9, Training Loss: 1.6081\n",
            "LR: 0.1, Batch Size: 32, Epoch: 10, Training Loss: 1.6107\n",
            "LR: 0.1, Batch Size: 32, Epoch: 11, Training Loss: 1.6146\n",
            "LR: 0.1, Batch Size: 32, Epoch: 12, Training Loss: 1.6103\n",
            "LR: 0.1, Batch Size: 32, Epoch: 13, Training Loss: 1.6099\n",
            "LR: 0.1, Batch Size: 32, Epoch: 14, Training Loss: 1.6120\n",
            "LR: 0.1, Batch Size: 32, Epoch: 15, Training Loss: 1.6139\n",
            "LR: 0.1, Batch Size: 32, Epoch: 16, Training Loss: 1.6111\n",
            "LR: 0.1, Batch Size: 32, Epoch: 17, Training Loss: 1.6084\n",
            "LR: 0.1, Batch Size: 32, Epoch: 18, Training Loss: 1.6095\n",
            "LR: 0.1, Batch Size: 32, Epoch: 19, Training Loss: 1.6093\n",
            "LR: 0.1, Batch Size: 32, Epoch: 20, Training Loss: 1.6103\n",
            "LR: 0.1, Batch Size: 32, Num Epochs: 20, Test Loss: 1.6089, Test Accuracy: 0.2202\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 32, Epoch: 1, Training Loss: 2.2337\n",
            "LR: 0.1, Batch Size: 32, Epoch: 2, Training Loss: 1.5959\n",
            "LR: 0.1, Batch Size: 32, Epoch: 3, Training Loss: 1.5706\n",
            "LR: 0.1, Batch Size: 32, Epoch: 4, Training Loss: 1.4769\n",
            "LR: 0.1, Batch Size: 32, Epoch: 5, Training Loss: 1.5790\n",
            "LR: 0.1, Batch Size: 32, Epoch: 6, Training Loss: 1.8705\n",
            "LR: 0.1, Batch Size: 32, Epoch: 7, Training Loss: 1.6092\n",
            "LR: 0.1, Batch Size: 32, Epoch: 8, Training Loss: 1.6070\n",
            "LR: 0.1, Batch Size: 32, Epoch: 9, Training Loss: 1.6125\n",
            "LR: 0.1, Batch Size: 32, Epoch: 10, Training Loss: 1.6091\n",
            "LR: 0.1, Batch Size: 32, Epoch: 11, Training Loss: 1.6091\n",
            "LR: 0.1, Batch Size: 32, Epoch: 12, Training Loss: 1.6098\n",
            "LR: 0.1, Batch Size: 32, Epoch: 13, Training Loss: 1.6091\n",
            "LR: 0.1, Batch Size: 32, Epoch: 14, Training Loss: 1.6112\n",
            "LR: 0.1, Batch Size: 32, Epoch: 15, Training Loss: 1.6125\n",
            "LR: 0.1, Batch Size: 32, Epoch: 16, Training Loss: 1.6142\n",
            "LR: 0.1, Batch Size: 32, Epoch: 17, Training Loss: 1.6112\n",
            "LR: 0.1, Batch Size: 32, Epoch: 18, Training Loss: 1.6117\n",
            "LR: 0.1, Batch Size: 32, Epoch: 19, Training Loss: 1.6140\n",
            "LR: 0.1, Batch Size: 32, Epoch: 20, Training Loss: 1.6087\n",
            "LR: 0.1, Batch Size: 32, Epoch: 21, Training Loss: 1.6091\n",
            "LR: 0.1, Batch Size: 32, Epoch: 22, Training Loss: 1.6104\n",
            "LR: 0.1, Batch Size: 32, Epoch: 23, Training Loss: 1.6093\n",
            "LR: 0.1, Batch Size: 32, Epoch: 24, Training Loss: 1.6099\n",
            "LR: 0.1, Batch Size: 32, Epoch: 25, Training Loss: 1.6080\n",
            "LR: 0.1, Batch Size: 32, Num Epochs: 25, Test Loss: 1.6173, Test Accuracy: 0.2202\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 32, Epoch: 1, Training Loss: 1.7813\n",
            "LR: 0.1, Batch Size: 32, Epoch: 2, Training Loss: 1.5664\n",
            "LR: 0.1, Batch Size: 32, Epoch: 3, Training Loss: 1.5852\n",
            "LR: 0.1, Batch Size: 32, Epoch: 4, Training Loss: 1.6132\n",
            "LR: 0.1, Batch Size: 32, Epoch: 5, Training Loss: 1.6074\n",
            "LR: 0.1, Batch Size: 32, Epoch: 6, Training Loss: 1.7101\n",
            "LR: 0.1, Batch Size: 32, Epoch: 7, Training Loss: 1.6091\n",
            "LR: 0.1, Batch Size: 32, Epoch: 8, Training Loss: 1.6032\n",
            "LR: 0.1, Batch Size: 32, Epoch: 9, Training Loss: 1.6084\n",
            "LR: 0.1, Batch Size: 32, Epoch: 10, Training Loss: 1.6021\n",
            "LR: 0.1, Batch Size: 32, Epoch: 11, Training Loss: 1.5596\n",
            "LR: 0.1, Batch Size: 32, Epoch: 12, Training Loss: 1.7930\n",
            "LR: 0.1, Batch Size: 32, Epoch: 13, Training Loss: 1.6105\n",
            "LR: 0.1, Batch Size: 32, Epoch: 14, Training Loss: 1.6077\n",
            "LR: 0.1, Batch Size: 32, Epoch: 15, Training Loss: 1.6090\n",
            "LR: 0.1, Batch Size: 32, Epoch: 16, Training Loss: 1.6120\n",
            "LR: 0.1, Batch Size: 32, Epoch: 17, Training Loss: 1.6051\n",
            "LR: 0.1, Batch Size: 32, Epoch: 18, Training Loss: 1.6129\n",
            "LR: 0.1, Batch Size: 32, Epoch: 19, Training Loss: 1.6106\n",
            "LR: 0.1, Batch Size: 32, Epoch: 20, Training Loss: 1.6092\n",
            "LR: 0.1, Batch Size: 32, Epoch: 21, Training Loss: 1.6114\n",
            "LR: 0.1, Batch Size: 32, Epoch: 22, Training Loss: 1.6103\n",
            "LR: 0.1, Batch Size: 32, Epoch: 23, Training Loss: 1.6161\n",
            "LR: 0.1, Batch Size: 32, Epoch: 24, Training Loss: 1.6072\n",
            "LR: 0.1, Batch Size: 32, Epoch: 25, Training Loss: 1.6120\n",
            "LR: 0.1, Batch Size: 32, Epoch: 26, Training Loss: 1.6131\n",
            "LR: 0.1, Batch Size: 32, Epoch: 27, Training Loss: 1.6122\n",
            "LR: 0.1, Batch Size: 32, Epoch: 28, Training Loss: 1.6085\n",
            "LR: 0.1, Batch Size: 32, Epoch: 29, Training Loss: 1.6071\n",
            "LR: 0.1, Batch Size: 32, Epoch: 30, Training Loss: 1.6094\n",
            "LR: 0.1, Batch Size: 32, Num Epochs: 30, Test Loss: 1.6082, Test Accuracy: 0.2202\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 64, Epoch: 1, Training Loss: 1.9595\n",
            "LR: 0.1, Batch Size: 64, Epoch: 2, Training Loss: 1.2871\n",
            "LR: 0.1, Batch Size: 64, Epoch: 3, Training Loss: 1.3462\n",
            "LR: 0.1, Batch Size: 64, Epoch: 4, Training Loss: 1.4633\n",
            "LR: 0.1, Batch Size: 64, Epoch: 5, Training Loss: 1.6268\n",
            "LR: 0.1, Batch Size: 64, Num Epochs: 5, Test Loss: 1.6045, Test Accuracy: 0.2315\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 64, Epoch: 1, Training Loss: 1.6983\n",
            "LR: 0.1, Batch Size: 64, Epoch: 2, Training Loss: 1.2988\n",
            "LR: 0.1, Batch Size: 64, Epoch: 3, Training Loss: 1.5718\n",
            "LR: 0.1, Batch Size: 64, Epoch: 4, Training Loss: 1.5985\n",
            "LR: 0.1, Batch Size: 64, Epoch: 5, Training Loss: 1.6472\n",
            "LR: 0.1, Batch Size: 64, Epoch: 6, Training Loss: 1.6050\n",
            "LR: 0.1, Batch Size: 64, Epoch: 7, Training Loss: 1.6044\n",
            "LR: 0.1, Batch Size: 64, Epoch: 8, Training Loss: 1.6060\n",
            "LR: 0.1, Batch Size: 64, Epoch: 9, Training Loss: 1.6078\n",
            "LR: 0.1, Batch Size: 64, Epoch: 10, Training Loss: 1.6040\n",
            "LR: 0.1, Batch Size: 64, Num Epochs: 10, Test Loss: 1.6055, Test Accuracy: 0.2292\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 64, Epoch: 1, Training Loss: 2.8627\n",
            "LR: 0.1, Batch Size: 64, Epoch: 2, Training Loss: 1.3833\n",
            "LR: 0.1, Batch Size: 64, Epoch: 3, Training Loss: 1.4905\n",
            "LR: 0.1, Batch Size: 64, Epoch: 4, Training Loss: 1.3644\n",
            "LR: 0.1, Batch Size: 64, Epoch: 5, Training Loss: 1.6132\n",
            "LR: 0.1, Batch Size: 64, Epoch: 6, Training Loss: 1.6115\n",
            "LR: 0.1, Batch Size: 64, Epoch: 7, Training Loss: 1.6132\n",
            "LR: 0.1, Batch Size: 64, Epoch: 8, Training Loss: 1.6051\n",
            "LR: 0.1, Batch Size: 64, Epoch: 9, Training Loss: 1.6057\n",
            "LR: 0.1, Batch Size: 64, Epoch: 10, Training Loss: 1.6046\n",
            "LR: 0.1, Batch Size: 64, Epoch: 11, Training Loss: 1.6084\n",
            "LR: 0.1, Batch Size: 64, Epoch: 12, Training Loss: 1.6066\n",
            "LR: 0.1, Batch Size: 64, Epoch: 13, Training Loss: 1.6077\n",
            "LR: 0.1, Batch Size: 64, Epoch: 14, Training Loss: 1.6097\n",
            "LR: 0.1, Batch Size: 64, Epoch: 15, Training Loss: 1.6084\n",
            "LR: 0.1, Batch Size: 64, Num Epochs: 15, Test Loss: 1.6078, Test Accuracy: 0.2292\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 64, Epoch: 1, Training Loss: 2.7227\n",
            "LR: 0.1, Batch Size: 64, Epoch: 2, Training Loss: 1.6218\n",
            "LR: 0.1, Batch Size: 64, Epoch: 3, Training Loss: 1.6082\n",
            "LR: 0.1, Batch Size: 64, Epoch: 4, Training Loss: 1.6091\n",
            "LR: 0.1, Batch Size: 64, Epoch: 5, Training Loss: 1.6096\n",
            "LR: 0.1, Batch Size: 64, Epoch: 6, Training Loss: 1.6068\n",
            "LR: 0.1, Batch Size: 64, Epoch: 7, Training Loss: 1.6100\n",
            "LR: 0.1, Batch Size: 64, Epoch: 8, Training Loss: 1.6072\n",
            "LR: 0.1, Batch Size: 64, Epoch: 9, Training Loss: 1.6102\n",
            "LR: 0.1, Batch Size: 64, Epoch: 10, Training Loss: 1.6050\n",
            "LR: 0.1, Batch Size: 64, Epoch: 11, Training Loss: 1.6094\n",
            "LR: 0.1, Batch Size: 64, Epoch: 12, Training Loss: 1.6059\n",
            "LR: 0.1, Batch Size: 64, Epoch: 13, Training Loss: 1.6053\n",
            "LR: 0.1, Batch Size: 64, Epoch: 14, Training Loss: 1.6072\n",
            "LR: 0.1, Batch Size: 64, Epoch: 15, Training Loss: 1.6097\n",
            "LR: 0.1, Batch Size: 64, Epoch: 16, Training Loss: 1.6060\n",
            "LR: 0.1, Batch Size: 64, Epoch: 17, Training Loss: 1.6454\n",
            "LR: 0.1, Batch Size: 64, Epoch: 18, Training Loss: 2.0494\n",
            "LR: 0.1, Batch Size: 64, Epoch: 19, Training Loss: 1.6479\n",
            "LR: 0.1, Batch Size: 64, Epoch: 20, Training Loss: 1.6107\n",
            "LR: 0.1, Batch Size: 64, Num Epochs: 20, Test Loss: 1.6083, Test Accuracy: 0.2202\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 64, Epoch: 1, Training Loss: 2.1259\n",
            "LR: 0.1, Batch Size: 64, Epoch: 2, Training Loss: 1.3893\n",
            "LR: 0.1, Batch Size: 64, Epoch: 3, Training Loss: 1.6770\n",
            "LR: 0.1, Batch Size: 64, Epoch: 4, Training Loss: 1.4328\n",
            "LR: 0.1, Batch Size: 64, Epoch: 5, Training Loss: 1.5555\n",
            "LR: 0.1, Batch Size: 64, Epoch: 6, Training Loss: 1.6096\n",
            "LR: 0.1, Batch Size: 64, Epoch: 7, Training Loss: 1.6134\n",
            "LR: 0.1, Batch Size: 64, Epoch: 8, Training Loss: 1.6045\n",
            "LR: 0.1, Batch Size: 64, Epoch: 9, Training Loss: 1.6075\n",
            "LR: 0.1, Batch Size: 64, Epoch: 10, Training Loss: 1.6076\n",
            "LR: 0.1, Batch Size: 64, Epoch: 11, Training Loss: 1.6116\n",
            "LR: 0.1, Batch Size: 64, Epoch: 12, Training Loss: 1.6088\n",
            "LR: 0.1, Batch Size: 64, Epoch: 13, Training Loss: 1.6054\n",
            "LR: 0.1, Batch Size: 64, Epoch: 14, Training Loss: 1.6066\n",
            "LR: 0.1, Batch Size: 64, Epoch: 15, Training Loss: 1.6052\n",
            "LR: 0.1, Batch Size: 64, Epoch: 16, Training Loss: 1.6048\n",
            "LR: 0.1, Batch Size: 64, Epoch: 17, Training Loss: 1.6360\n",
            "LR: 0.1, Batch Size: 64, Epoch: 18, Training Loss: 1.6039\n",
            "LR: 0.1, Batch Size: 64, Epoch: 19, Training Loss: 1.6062\n",
            "LR: 0.1, Batch Size: 64, Epoch: 20, Training Loss: 1.6065\n",
            "LR: 0.1, Batch Size: 64, Epoch: 21, Training Loss: 1.6078\n",
            "LR: 0.1, Batch Size: 64, Epoch: 22, Training Loss: 1.6084\n",
            "LR: 0.1, Batch Size: 64, Epoch: 23, Training Loss: 1.6060\n",
            "LR: 0.1, Batch Size: 64, Epoch: 24, Training Loss: 1.6076\n",
            "LR: 0.1, Batch Size: 64, Epoch: 25, Training Loss: 1.6070\n",
            "LR: 0.1, Batch Size: 64, Num Epochs: 25, Test Loss: 1.6034, Test Accuracy: 0.2202\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 64, Epoch: 1, Training Loss: 1.9825\n",
            "LR: 0.1, Batch Size: 64, Epoch: 2, Training Loss: 1.3316\n",
            "LR: 0.1, Batch Size: 64, Epoch: 3, Training Loss: 1.4648\n",
            "LR: 0.1, Batch Size: 64, Epoch: 4, Training Loss: 1.3493\n",
            "LR: 0.1, Batch Size: 64, Epoch: 5, Training Loss: 1.3656\n",
            "LR: 0.1, Batch Size: 64, Epoch: 6, Training Loss: 1.5215\n",
            "LR: 0.1, Batch Size: 64, Epoch: 7, Training Loss: 1.3779\n",
            "LR: 0.1, Batch Size: 64, Epoch: 8, Training Loss: 1.5595\n",
            "LR: 0.1, Batch Size: 64, Epoch: 9, Training Loss: 1.6550\n",
            "LR: 0.1, Batch Size: 64, Epoch: 10, Training Loss: 2.0480\n",
            "LR: 0.1, Batch Size: 64, Epoch: 11, Training Loss: 1.8301\n",
            "LR: 0.1, Batch Size: 64, Epoch: 12, Training Loss: 1.6009\n",
            "LR: 0.1, Batch Size: 64, Epoch: 13, Training Loss: 1.6028\n",
            "LR: 0.1, Batch Size: 64, Epoch: 14, Training Loss: 1.6040\n",
            "LR: 0.1, Batch Size: 64, Epoch: 15, Training Loss: 1.6013\n",
            "LR: 0.1, Batch Size: 64, Epoch: 16, Training Loss: 1.6054\n",
            "LR: 0.1, Batch Size: 64, Epoch: 17, Training Loss: 1.6661\n",
            "LR: 0.1, Batch Size: 64, Epoch: 18, Training Loss: 1.5924\n",
            "LR: 0.1, Batch Size: 64, Epoch: 19, Training Loss: 8.2864\n",
            "LR: 0.1, Batch Size: 64, Epoch: 20, Training Loss: 1.6371\n",
            "LR: 0.1, Batch Size: 64, Epoch: 21, Training Loss: 1.6071\n",
            "LR: 0.1, Batch Size: 64, Epoch: 22, Training Loss: 1.6128\n",
            "LR: 0.1, Batch Size: 64, Epoch: 23, Training Loss: 1.6074\n",
            "LR: 0.1, Batch Size: 64, Epoch: 24, Training Loss: 1.6054\n",
            "LR: 0.1, Batch Size: 64, Epoch: 25, Training Loss: 1.6082\n",
            "LR: 0.1, Batch Size: 64, Epoch: 26, Training Loss: 1.6077\n",
            "LR: 0.1, Batch Size: 64, Epoch: 27, Training Loss: 1.6051\n",
            "LR: 0.1, Batch Size: 64, Epoch: 28, Training Loss: 1.6075\n",
            "LR: 0.1, Batch Size: 64, Epoch: 29, Training Loss: 1.6144\n",
            "LR: 0.1, Batch Size: 64, Epoch: 30, Training Loss: 1.6081\n",
            "LR: 0.1, Batch Size: 64, Num Epochs: 30, Test Loss: 1.6060, Test Accuracy: 0.2270\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 128, Epoch: 1, Training Loss: 2.6738\n",
            "LR: 0.1, Batch Size: 128, Epoch: 2, Training Loss: 1.4616\n",
            "LR: 0.1, Batch Size: 128, Epoch: 3, Training Loss: 1.4099\n",
            "LR: 0.1, Batch Size: 128, Epoch: 4, Training Loss: 1.5130\n",
            "LR: 0.1, Batch Size: 128, Epoch: 5, Training Loss: 1.6235\n",
            "LR: 0.1, Batch Size: 128, Num Epochs: 5, Test Loss: 1.6108, Test Accuracy: 0.2180\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 128, Epoch: 1, Training Loss: 4.6624\n",
            "LR: 0.1, Batch Size: 128, Epoch: 2, Training Loss: 1.4895\n",
            "LR: 0.1, Batch Size: 128, Epoch: 3, Training Loss: 1.4200\n",
            "LR: 0.1, Batch Size: 128, Epoch: 4, Training Loss: 1.5998\n",
            "LR: 0.1, Batch Size: 128, Epoch: 5, Training Loss: 1.5031\n",
            "LR: 0.1, Batch Size: 128, Epoch: 6, Training Loss: 1.5944\n",
            "LR: 0.1, Batch Size: 128, Epoch: 7, Training Loss: 1.5754\n",
            "LR: 0.1, Batch Size: 128, Epoch: 8, Training Loss: 1.5709\n",
            "LR: 0.1, Batch Size: 128, Epoch: 9, Training Loss: 1.5466\n",
            "LR: 0.1, Batch Size: 128, Epoch: 10, Training Loss: 1.5631\n",
            "LR: 0.1, Batch Size: 128, Num Epochs: 10, Test Loss: 1.4884, Test Accuracy: 0.2899\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 128, Epoch: 1, Training Loss: 1.8699\n",
            "LR: 0.1, Batch Size: 128, Epoch: 2, Training Loss: 1.3008\n",
            "LR: 0.1, Batch Size: 128, Epoch: 3, Training Loss: 1.2058\n",
            "LR: 0.1, Batch Size: 128, Epoch: 4, Training Loss: 1.2383\n",
            "LR: 0.1, Batch Size: 128, Epoch: 5, Training Loss: 1.4998\n",
            "LR: 0.1, Batch Size: 128, Epoch: 6, Training Loss: 1.4758\n",
            "LR: 0.1, Batch Size: 128, Epoch: 7, Training Loss: 1.3512\n",
            "LR: 0.1, Batch Size: 128, Epoch: 8, Training Loss: 1.4353\n",
            "LR: 0.1, Batch Size: 128, Epoch: 9, Training Loss: 1.3797\n",
            "LR: 0.1, Batch Size: 128, Epoch: 10, Training Loss: 1.5334\n",
            "LR: 0.1, Batch Size: 128, Epoch: 11, Training Loss: 1.5061\n",
            "LR: 0.1, Batch Size: 128, Epoch: 12, Training Loss: 1.4730\n",
            "LR: 0.1, Batch Size: 128, Epoch: 13, Training Loss: 1.5170\n",
            "LR: 0.1, Batch Size: 128, Epoch: 14, Training Loss: 1.5339\n",
            "LR: 0.1, Batch Size: 128, Epoch: 15, Training Loss: 1.5045\n",
            "LR: 0.1, Batch Size: 128, Num Epochs: 15, Test Loss: 1.4220, Test Accuracy: 0.3169\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 128, Epoch: 1, Training Loss: 5.7785\n",
            "LR: 0.1, Batch Size: 128, Epoch: 2, Training Loss: 1.4894\n",
            "LR: 0.1, Batch Size: 128, Epoch: 3, Training Loss: 1.4318\n",
            "LR: 0.1, Batch Size: 128, Epoch: 4, Training Loss: 1.5523\n",
            "LR: 0.1, Batch Size: 128, Epoch: 5, Training Loss: 1.5612\n",
            "LR: 0.1, Batch Size: 128, Epoch: 6, Training Loss: 1.4656\n",
            "LR: 0.1, Batch Size: 128, Epoch: 7, Training Loss: 1.5292\n",
            "LR: 0.1, Batch Size: 128, Epoch: 8, Training Loss: 1.4669\n",
            "LR: 0.1, Batch Size: 128, Epoch: 9, Training Loss: 1.5557\n",
            "LR: 0.1, Batch Size: 128, Epoch: 10, Training Loss: 1.4291\n",
            "LR: 0.1, Batch Size: 128, Epoch: 11, Training Loss: 1.8455\n",
            "LR: 0.1, Batch Size: 128, Epoch: 12, Training Loss: 1.5500\n",
            "LR: 0.1, Batch Size: 128, Epoch: 13, Training Loss: 1.5895\n",
            "LR: 0.1, Batch Size: 128, Epoch: 14, Training Loss: 1.5941\n",
            "LR: 0.1, Batch Size: 128, Epoch: 15, Training Loss: 1.5898\n",
            "LR: 0.1, Batch Size: 128, Epoch: 16, Training Loss: 1.5854\n",
            "LR: 0.1, Batch Size: 128, Epoch: 17, Training Loss: 1.5564\n",
            "LR: 0.1, Batch Size: 128, Epoch: 18, Training Loss: 1.5799\n",
            "LR: 0.1, Batch Size: 128, Epoch: 19, Training Loss: 1.5499\n",
            "LR: 0.1, Batch Size: 128, Epoch: 20, Training Loss: 1.5145\n",
            "LR: 0.1, Batch Size: 128, Num Epochs: 20, Test Loss: 1.4401, Test Accuracy: 0.3258\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 128, Epoch: 1, Training Loss: 3.8321\n",
            "LR: 0.1, Batch Size: 128, Epoch: 2, Training Loss: 1.5395\n",
            "LR: 0.1, Batch Size: 128, Epoch: 3, Training Loss: 1.4619\n",
            "LR: 0.1, Batch Size: 128, Epoch: 4, Training Loss: 1.3705\n",
            "LR: 0.1, Batch Size: 128, Epoch: 5, Training Loss: 1.4481\n",
            "LR: 0.1, Batch Size: 128, Epoch: 6, Training Loss: 1.3787\n",
            "LR: 0.1, Batch Size: 128, Epoch: 7, Training Loss: 1.3531\n",
            "LR: 0.1, Batch Size: 128, Epoch: 8, Training Loss: 1.3211\n",
            "LR: 0.1, Batch Size: 128, Epoch: 9, Training Loss: 1.3142\n",
            "LR: 0.1, Batch Size: 128, Epoch: 10, Training Loss: 1.3040\n",
            "LR: 0.1, Batch Size: 128, Epoch: 11, Training Loss: 1.3307\n",
            "LR: 0.1, Batch Size: 128, Epoch: 12, Training Loss: 1.2536\n",
            "LR: 0.1, Batch Size: 128, Epoch: 13, Training Loss: 1.3743\n",
            "LR: 0.1, Batch Size: 128, Epoch: 14, Training Loss: 1.4190\n",
            "LR: 0.1, Batch Size: 128, Epoch: 15, Training Loss: 1.3751\n",
            "LR: 0.1, Batch Size: 128, Epoch: 16, Training Loss: 1.3545\n",
            "LR: 0.1, Batch Size: 128, Epoch: 17, Training Loss: 1.4112\n",
            "LR: 0.1, Batch Size: 128, Epoch: 18, Training Loss: 1.4537\n",
            "LR: 0.1, Batch Size: 128, Epoch: 19, Training Loss: 1.4162\n",
            "LR: 0.1, Batch Size: 128, Epoch: 20, Training Loss: 1.4372\n",
            "LR: 0.1, Batch Size: 128, Epoch: 21, Training Loss: 1.4138\n",
            "LR: 0.1, Batch Size: 128, Epoch: 22, Training Loss: 1.3655\n",
            "LR: 0.1, Batch Size: 128, Epoch: 23, Training Loss: 1.3883\n",
            "LR: 0.1, Batch Size: 128, Epoch: 24, Training Loss: 1.3542\n",
            "LR: 0.1, Batch Size: 128, Epoch: 25, Training Loss: 1.4390\n",
            "LR: 0.1, Batch Size: 128, Num Epochs: 25, Test Loss: 1.3035, Test Accuracy: 0.3663\n",
            "\n",
            "\n",
            "LR: 0.1, Batch Size: 128, Epoch: 1, Training Loss: 2.4515\n",
            "LR: 0.1, Batch Size: 128, Epoch: 2, Training Loss: 1.5499\n",
            "LR: 0.1, Batch Size: 128, Epoch: 3, Training Loss: 1.7481\n",
            "LR: 0.1, Batch Size: 128, Epoch: 4, Training Loss: 1.2851\n",
            "LR: 0.1, Batch Size: 128, Epoch: 5, Training Loss: 1.3254\n",
            "LR: 0.1, Batch Size: 128, Epoch: 6, Training Loss: 1.3812\n",
            "LR: 0.1, Batch Size: 128, Epoch: 7, Training Loss: 1.4901\n",
            "LR: 0.1, Batch Size: 128, Epoch: 8, Training Loss: 1.4061\n",
            "LR: 0.1, Batch Size: 128, Epoch: 9, Training Loss: 1.3606\n",
            "LR: 0.1, Batch Size: 128, Epoch: 10, Training Loss: 1.2610\n",
            "LR: 0.1, Batch Size: 128, Epoch: 11, Training Loss: 1.3575\n",
            "LR: 0.1, Batch Size: 128, Epoch: 12, Training Loss: 1.2969\n",
            "LR: 0.1, Batch Size: 128, Epoch: 13, Training Loss: 1.6889\n",
            "LR: 0.1, Batch Size: 128, Epoch: 14, Training Loss: 1.7940\n",
            "LR: 0.1, Batch Size: 128, Epoch: 15, Training Loss: 1.9946\n",
            "LR: 0.1, Batch Size: 128, Epoch: 16, Training Loss: 1.5581\n",
            "LR: 0.1, Batch Size: 128, Epoch: 17, Training Loss: 1.5615\n",
            "LR: 0.1, Batch Size: 128, Epoch: 18, Training Loss: 1.4855\n",
            "LR: 0.1, Batch Size: 128, Epoch: 19, Training Loss: 1.4411\n",
            "LR: 0.1, Batch Size: 128, Epoch: 20, Training Loss: 1.4193\n",
            "LR: 0.1, Batch Size: 128, Epoch: 21, Training Loss: 1.3671\n",
            "LR: 0.1, Batch Size: 128, Epoch: 22, Training Loss: 1.6151\n",
            "LR: 0.1, Batch Size: 128, Epoch: 23, Training Loss: 1.5479\n",
            "LR: 0.1, Batch Size: 128, Epoch: 24, Training Loss: 1.5717\n",
            "LR: 0.1, Batch Size: 128, Epoch: 25, Training Loss: 1.5424\n",
            "LR: 0.1, Batch Size: 128, Epoch: 26, Training Loss: 1.5227\n",
            "LR: 0.1, Batch Size: 128, Epoch: 27, Training Loss: 1.6700\n",
            "LR: 0.1, Batch Size: 128, Epoch: 28, Training Loss: 1.5960\n",
            "LR: 0.1, Batch Size: 128, Epoch: 29, Training Loss: 1.5934\n",
            "LR: 0.1, Batch Size: 128, Epoch: 30, Training Loss: 1.5960\n",
            "LR: 0.1, Batch Size: 128, Num Epochs: 30, Test Loss: 1.5965, Test Accuracy: 0.2202\n",
            "\n",
            "\n",
            "Best Accuracy: 0.9640\n",
            "Best Hyperparameters: {'learning_rate': 0.001, 'batch_size': 8, 'num_epochs': 20}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YVY8PYnZ_l38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bayesian Optimization"
      ],
      "metadata": {
        "id": "IC2cmvzS24cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/"
      ],
      "metadata": {
        "id": "YzQZLBNbUe2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model architecture\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate, activation_fn):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        layers = []\n",
        "        current_dim = input_dim\n",
        "        # Loop through hidden dimensions and create layers\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(current_dim, h_dim))\n",
        "            layers.append(activation_fn)\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            current_dim = h_dim\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(current_dim, output_dim))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "EK4E_DYG_L8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining function to train the model\n",
        "\n",
        "def train_model(hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4, dropout_rate, learning_rate, batch_size, num_epochs, activation, optimizer_name):\n",
        "\n",
        "    # type coersion! - inputs from bayesian optimization will be in float format\n",
        "    # but hidden dimensions and epochs in nn are always int\n",
        "    # so conversion is required!\n",
        "    # after all, you can't have 107.374 layers :)\n",
        "    hidden_dim1 = int(hidden_dim1)\n",
        "    hidden_dim2 = int(hidden_dim2)\n",
        "    hidden_dim3 = int(hidden_dim3)\n",
        "    hidden_dim4 = int(hidden_dim4)\n",
        "    batch_size = int(batch_size)\n",
        "    num_epochs = int(num_epochs)\n",
        "\n",
        "    #activation_fn = {'relu': nn.ReLU(), 'sigmoid': nn.Sigmoid(), 'tanh': nn.Tanh()}[activation]\n",
        "    activation_fn = {\n",
        "        'relu': nn.ReLU(),\n",
        "        'sigmoid': nn.Sigmoid(),\n",
        "        'tanh': nn.Tanh(),\n",
        "        'leaky_relu': nn.LeakyReLU(0.1),\n",
        "        'elu': nn.ELU(),\n",
        "        'selu': nn.SELU(),\n",
        "        'softplus': nn.Softplus()\n",
        "    }[activation]\n",
        "    optimizer_class = {'Adam': optim.Adam, 'SGD': optim.SGD}[optimizer_name]\n",
        "\n",
        "    input_dim = 128\n",
        "    output_dim = 5\n",
        "    model = SimpleNN(input_dim, [hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4], output_dim, dropout_rate, activation_fn)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for sequences, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(sequences)\n",
        "            loss = loss_fn(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in test_loader:\n",
        "            predictions = model(sequences)\n",
        "            _, predicted_labels = torch.max(predictions, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted_labels == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "jAB_gKfsAssS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter bounds for bayesian optimization\n",
        "pbounds = {\n",
        "    'hidden_dim1': (64, 256),\n",
        "    'hidden_dim2': (64, 256),\n",
        "    'hidden_dim3': (32, 128),\n",
        "    'hidden_dim4': (16, 64),\n",
        "    'dropout_rate': (0.1, 0.5),\n",
        "    'learning_rate': (1e-4, 1e-2),\n",
        "    'batch_size': (32, 128),\n",
        "    'num_epochs': (10, 50),\n",
        "    'activation': (0, 6), # 0: relu, 1: sigmoid, 2: tanh, ...\n",
        "    'optimizer_name': (0, 1) # 0: Adam, 1: SGD\n",
        "}\n",
        "\n",
        "# mapping activation and optimizer_name from numeric values to corresponsing string values\n",
        "def map_params(params):\n",
        "    params['activation'] = ['relu', 'sigmoid', 'tanh', 'leaky_relu', 'elu', 'selu', 'softplus',][int(params['activation'])]\n",
        "    params['optimizer_name'] = ['Adam', 'SGD'][int(params['optimizer_name'])]\n",
        "    return params\n",
        "\n",
        "# objective function to maximize or minimise\n",
        "def objective(**params):\n",
        "    params = map_params(params)\n",
        "    return train_model(**params)\n",
        "\n",
        "# Bayesian Optimization setup\n",
        "optimizer = BayesianOptimization(\n",
        "    f=objective,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Running optimizer for 10 inital random points and 50 optimization iterations\n",
        "# Total - 60\n",
        "optimizer.maximize(init_points=10, n_iter=50)\n",
        "\n",
        "# Extracting best hyperparameters\n",
        "best_params = optimizer.max['params']\n",
        "best_params = map_params(best_params)\n",
        "print(\"Best parameters:\", best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrIBSEOcELsL",
        "outputId": "fbce5e99-a999-4ab8-9e7c-ef90f1465028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | activa... | batch_... | dropou... | hidden... | hidden... | hidden... | hidden... | learni... | num_ep... | optimi... |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[30m1         | \u001b[30m0.9461    | \u001b[30m2.247     | \u001b[30m123.3     | \u001b[30m0.3928    | \u001b[30m178.9     | \u001b[30m93.96     | \u001b[30m46.98     | \u001b[30m18.79     | \u001b[30m0.008675  | \u001b[30m34.04     | \u001b[30m0.7081    |\n",
            "| \u001b[35m2         | \u001b[35m0.9551    | \u001b[35m0.1235    | \u001b[35m125.1     | \u001b[35m0.433     | \u001b[35m104.8     | \u001b[35m98.91     | \u001b[35m49.61     | \u001b[35m30.6      | \u001b[35m0.005295  | \u001b[35m27.28     | \u001b[35m0.2912    |\n",
            "| \u001b[35m3         | \u001b[35m0.9573    | \u001b[35m3.671     | \u001b[35m45.39     | \u001b[35m0.2169    | \u001b[35m134.3     | \u001b[35m151.6     | \u001b[35m107.4     | \u001b[35m25.58     | \u001b[35m0.005191  | \u001b[35m33.7      | \u001b[35m0.04645   |\n",
            "| \u001b[30m4         | \u001b[30m0.9483    | \u001b[30m3.645     | \u001b[30m48.37     | \u001b[30m0.126     | \u001b[30m246.2     | \u001b[30m249.4     | \u001b[30m109.6     | \u001b[30m30.62     | \u001b[30m0.001067  | \u001b[30m37.37     | \u001b[30m0.4402    |\n",
            "| \u001b[35m5         | \u001b[35m0.9596    | \u001b[35m0.7322    | \u001b[35m79.54     | \u001b[35m0.1138    | \u001b[35m238.6     | \u001b[35m113.7     | \u001b[35m95.6      | \u001b[35m30.96     | \u001b[35m0.005249  | \u001b[35m31.87     | \u001b[35m0.1849    |\n",
            "| \u001b[30m6         | \u001b[30m0.9551    | \u001b[30m5.818     | \u001b[30m106.4     | \u001b[30m0.4758    | \u001b[30m235.8     | \u001b[30m178.8     | \u001b[30m120.5     | \u001b[30m20.25     | \u001b[30m0.00204   | \u001b[30m11.81     | \u001b[30m0.3253    |\n",
            "| \u001b[30m7         | \u001b[30m0.9551    | \u001b[30m2.332     | \u001b[30m58.05     | \u001b[30m0.4315    | \u001b[30m132.5     | \u001b[30m117.9     | \u001b[30m84.1      | \u001b[30m22.76     | \u001b[30m0.008042  | \u001b[30m12.98     | \u001b[30m0.9869    |\n",
            "| \u001b[30m8         | \u001b[30m0.9551    | \u001b[30m4.633     | \u001b[30m51.08     | \u001b[30m0.1022    | \u001b[30m220.6     | \u001b[30m199.7     | \u001b[30m102.0     | \u001b[30m53.02     | \u001b[30m0.000833  | \u001b[30m24.34     | \u001b[30m0.1159    |\n",
            "| \u001b[30m9         | \u001b[30m0.9551    | \u001b[30m5.179     | \u001b[30m91.84     | \u001b[30m0.2324    | \u001b[30m76.2      | \u001b[30m123.7     | \u001b[30m63.22     | \u001b[30m51.02     | \u001b[30m0.006412  | \u001b[30m45.49     | \u001b[30m0.4722    |\n",
            "| \u001b[30m10        | \u001b[30m0.9573    | \u001b[30m0.7176    | \u001b[30m100.5     | \u001b[30m0.4043    | \u001b[30m171.8     | \u001b[30m212.0     | \u001b[30m79.4      | \u001b[30m41.09     | \u001b[30m0.004333  | \u001b[30m11.02     | \u001b[30m0.1079    |\n",
            "| \u001b[30m11        | \u001b[30m0.9573    | \u001b[30m5.199     | \u001b[30m73.51     | \u001b[30m0.3239    | \u001b[30m242.9     | \u001b[30m113.0     | \u001b[30m105.3     | \u001b[30m38.58     | \u001b[30m0.002206  | \u001b[30m44.12     | \u001b[30m0.6799    |\n",
            "| \u001b[30m12        | \u001b[30m0.9506    | \u001b[30m4.936     | \u001b[30m79.63     | \u001b[30m0.2029    | \u001b[30m254.6     | \u001b[30m126.6     | \u001b[30m83.18     | \u001b[30m44.53     | \u001b[30m0.001985  | \u001b[30m17.68     | \u001b[30m0.9559    |\n",
            "| \u001b[30m13        | \u001b[30m0.9596    | \u001b[30m2.175     | \u001b[30m84.0      | \u001b[30m0.2338    | \u001b[30m238.0     | \u001b[30m114.3     | \u001b[30m90.46     | \u001b[30m36.47     | \u001b[30m0.0001121 | \u001b[30m38.24     | \u001b[30m0.02636   |\n",
            "| \u001b[30m14        | \u001b[30m0.9573    | \u001b[30m3.943     | \u001b[30m43.96     | \u001b[30m0.1216    | \u001b[30m124.6     | \u001b[30m156.7     | \u001b[30m105.4     | \u001b[30m21.28     | \u001b[30m0.009199  | \u001b[30m34.87     | \u001b[30m0.3902    |\n",
            "| \u001b[30m15        | \u001b[30m0.9506    | \u001b[30m5.385     | \u001b[30m92.62     | \u001b[30m0.1847    | \u001b[30m77.71     | \u001b[30m129.3     | \u001b[30m62.81     | \u001b[30m52.22     | \u001b[30m0.001399  | \u001b[30m45.75     | \u001b[30m0.3549    |\n",
            "| \u001b[30m16        | \u001b[30m0.9416    | \u001b[30m0.4585    | \u001b[30m85.27     | \u001b[30m0.3455    | \u001b[30m223.6     | \u001b[30m239.8     | \u001b[30m41.3      | \u001b[30m42.26     | \u001b[30m0.008665  | \u001b[30m41.6      | \u001b[30m0.07453   |\n",
            "| \u001b[30m17        | \u001b[30m0.9506    | \u001b[30m4.854     | \u001b[30m83.93     | \u001b[30m0.3148    | \u001b[30m233.4     | \u001b[30m103.8     | \u001b[30m98.13     | \u001b[30m31.77     | \u001b[30m0.0001    | \u001b[30m39.54     | \u001b[30m0.395     |\n",
            "| \u001b[30m18        | \u001b[30m0.9573    | \u001b[30m0.1152    | \u001b[30m77.33     | \u001b[30m0.1421    | \u001b[30m241.8     | \u001b[30m119.6     | \u001b[30m95.15     | \u001b[30m35.94     | \u001b[30m0.008404  | \u001b[30m37.42     | \u001b[30m0.01328   |\n",
            "| \u001b[30m19        | \u001b[30m0.9416    | \u001b[30m2.872     | \u001b[30m83.47     | \u001b[30m0.5       | \u001b[30m238.0     | \u001b[30m118.6     | \u001b[30m87.03     | \u001b[30m30.85     | \u001b[30m0.0001    | \u001b[30m31.08     | \u001b[30m0.8901    |\n",
            "| \u001b[35m20        | \u001b[35m0.964     | \u001b[35m1.488     | \u001b[35m97.1      | \u001b[35m0.1006    | \u001b[35m82.4      | \u001b[35m160.3     | \u001b[35m48.39     | \u001b[35m22.76     | \u001b[35m0.007737  | \u001b[35m32.69     | \u001b[35m0.05067   |\n",
            "| \u001b[30m21        | \u001b[30m0.9393    | \u001b[30m1.376     | \u001b[30m51.71     | \u001b[30m0.1344    | \u001b[30m193.1     | \u001b[30m78.21     | \u001b[30m124.1     | \u001b[30m38.8      | \u001b[30m0.004655  | \u001b[30m10.98     | \u001b[30m0.7805    |\n",
            "| \u001b[30m22        | \u001b[30m0.9573    | \u001b[30m0.4593    | \u001b[30m33.26     | \u001b[30m0.4116    | \u001b[30m225.9     | \u001b[30m93.93     | \u001b[30m84.7      | \u001b[30m41.52     | \u001b[30m0.002627  | \u001b[30m16.09     | \u001b[30m0.549     |\n",
            "| \u001b[30m23        | \u001b[30m0.9461    | \u001b[30m4.742     | \u001b[30m67.07     | \u001b[30m0.1984    | \u001b[30m92.28     | \u001b[30m238.2     | \u001b[30m48.9      | \u001b[30m53.82     | \u001b[30m0.006934  | \u001b[30m36.4      | \u001b[30m0.6496    |\n",
            "| \u001b[30m24        | \u001b[30m0.9618    | \u001b[30m2.291     | \u001b[30m71.62     | \u001b[30m0.2937    | \u001b[30m225.7     | \u001b[30m170.0     | \u001b[30m62.84     | \u001b[30m19.68     | \u001b[30m0.001135  | \u001b[30m26.11     | \u001b[30m0.5276    |\n",
            "| \u001b[30m25        | \u001b[30m0.9506    | \u001b[30m3.406     | \u001b[30m113.7     | \u001b[30m0.3366    | \u001b[30m144.4     | \u001b[30m112.8     | \u001b[30m82.79     | \u001b[30m22.69     | \u001b[30m0.001056  | \u001b[30m48.29     | \u001b[30m0.04819   |\n",
            "| \u001b[30m26        | \u001b[30m0.9551    | \u001b[30m5.541     | \u001b[30m32.3      | \u001b[30m0.2968    | \u001b[30m208.6     | \u001b[30m85.32     | \u001b[30m88.07     | \u001b[30m60.46     | \u001b[30m0.001651  | \u001b[30m45.06     | \u001b[30m0.8801    |\n",
            "| \u001b[30m27        | \u001b[30m0.9506    | \u001b[30m1.5       | \u001b[30m97.11     | \u001b[30m0.1124    | \u001b[30m82.41     | \u001b[30m160.3     | \u001b[30m48.4      | \u001b[30m22.77     | \u001b[30m0.01      | \u001b[30m32.7      | \u001b[30m0.06247   |\n",
            "| \u001b[30m28        | \u001b[30m0.9506    | \u001b[30m0.6416    | \u001b[30m96.0      | \u001b[30m0.2043    | \u001b[30m184.6     | \u001b[30m113.2     | \u001b[30m86.81     | \u001b[30m36.82     | \u001b[30m0.006475  | \u001b[30m11.4      | \u001b[30m0.1291    |\n",
            "| \u001b[30m29        | \u001b[30m0.9528    | \u001b[30m4.567     | \u001b[30m120.7     | \u001b[30m0.375     | \u001b[30m254.0     | \u001b[30m118.7     | \u001b[30m76.37     | \u001b[30m25.41     | \u001b[30m0.005462  | \u001b[30m45.05     | \u001b[30m0.00478   |\n",
            "| \u001b[30m30        | \u001b[30m0.9573    | \u001b[30m5.161     | \u001b[30m42.6      | \u001b[30m0.4082    | \u001b[30m65.64     | \u001b[30m97.13     | \u001b[30m121.2     | \u001b[30m39.92     | \u001b[30m0.002241  | \u001b[30m43.75     | \u001b[30m0.1711    |\n",
            "| \u001b[30m31        | \u001b[30m0.9461    | \u001b[30m4.426     | \u001b[30m56.47     | \u001b[30m0.1345    | \u001b[30m241.8     | \u001b[30m239.6     | \u001b[30m117.7     | \u001b[30m55.05     | \u001b[30m0.006938  | \u001b[30m38.22     | \u001b[30m0.3901    |\n",
            "| \u001b[30m32        | \u001b[30m0.9528    | \u001b[30m1.367     | \u001b[30m43.36     | \u001b[30m0.3978    | \u001b[30m118.1     | \u001b[30m143.1     | \u001b[30m100.2     | \u001b[30m50.87     | \u001b[30m0.002647  | \u001b[30m47.77     | \u001b[30m0.8703    |\n",
            "| \u001b[30m33        | \u001b[30m0.5933    | \u001b[30m1.184     | \u001b[30m61.75     | \u001b[30m0.4458    | \u001b[30m137.2     | \u001b[30m228.8     | \u001b[30m51.89     | \u001b[30m21.92     | \u001b[30m0.005841  | \u001b[30m22.75     | \u001b[30m0.9444    |\n",
            "| \u001b[30m34        | \u001b[30m0.9483    | \u001b[30m1.94      | \u001b[30m113.7     | \u001b[30m0.3869    | \u001b[30m79.03     | \u001b[30m175.0     | \u001b[30m51.72     | \u001b[30m54.3      | \u001b[30m0.008159  | \u001b[30m37.0      | \u001b[30m0.04177   |\n",
            "| \u001b[30m35        | \u001b[30m0.227     | \u001b[30m1.366     | \u001b[30m96.97     | \u001b[30m0.1       | \u001b[30m82.28     | \u001b[30m160.1     | \u001b[30m48.27     | \u001b[30m22.63     | \u001b[30m0.0001    | \u001b[30m32.57     | \u001b[30m0.0       |\n",
            "| \u001b[30m36        | \u001b[30m0.8045    | \u001b[30m1.725     | \u001b[30m106.2     | \u001b[30m0.1591    | \u001b[30m150.0     | \u001b[30m158.0     | \u001b[30m44.1      | \u001b[30m47.33     | \u001b[30m0.0007869 | \u001b[30m24.79     | \u001b[30m0.7747    |\n",
            "| \u001b[30m37        | \u001b[30m0.7618    | \u001b[30m1.922     | \u001b[30m87.22     | \u001b[30m0.2792    | \u001b[30m170.8     | \u001b[30m249.6     | \u001b[30m41.2      | \u001b[30m25.58     | \u001b[30m0.00204   | \u001b[30m12.72     | \u001b[30m0.2632    |\n",
            "| \u001b[30m38        | \u001b[30m0.9551    | \u001b[30m1.534     | \u001b[30m62.73     | \u001b[30m0.2897    | \u001b[30m170.8     | \u001b[30m82.82     | \u001b[30m78.09     | \u001b[30m26.9      | \u001b[30m0.003049  | \u001b[30m25.43     | \u001b[30m0.2598    |\n",
            "| \u001b[30m39        | \u001b[30m0.9461    | \u001b[30m2.495     | \u001b[30m51.66     | \u001b[30m0.4647    | \u001b[30m98.73     | \u001b[30m182.3     | \u001b[30m107.1     | \u001b[30m53.83     | \u001b[30m0.009831  | \u001b[30m25.4      | \u001b[30m0.6908    |\n",
            "| \u001b[30m40        | \u001b[30m0.8921    | \u001b[30m3.65      | \u001b[30m66.5      | \u001b[30m0.3595    | \u001b[30m243.5     | \u001b[30m144.6     | \u001b[30m74.99     | \u001b[30m28.22     | \u001b[30m0.00986   | \u001b[30m44.99     | \u001b[30m0.4465    |\n",
            "| \u001b[30m41        | \u001b[30m0.964     | \u001b[30m0.5957    | \u001b[30m116.0     | \u001b[30m0.2956    | \u001b[30m169.0     | \u001b[30m114.2     | \u001b[30m118.6     | \u001b[30m19.89     | \u001b[30m0.005095  | \u001b[30m39.65     | \u001b[30m0.8056    |\n",
            "| \u001b[30m42        | \u001b[30m0.9573    | \u001b[30m1.217     | \u001b[30m73.55     | \u001b[30m0.3324    | \u001b[30m66.4      | \u001b[30m209.6     | \u001b[30m97.82     | \u001b[30m18.88     | \u001b[30m0.009934  | \u001b[30m43.65     | \u001b[30m0.7952    |\n",
            "| \u001b[30m43        | \u001b[30m0.9506    | \u001b[30m1.844     | \u001b[30m65.23     | \u001b[30m0.2647    | \u001b[30m156.4     | \u001b[30m173.0     | \u001b[30m36.35     | \u001b[30m23.29     | \u001b[30m0.003703  | \u001b[30m22.17     | \u001b[30m0.9283    |\n",
            "| \u001b[30m44        | \u001b[30m0.9528    | \u001b[30m0.9563    | \u001b[30m103.3     | \u001b[30m0.1575    | \u001b[30m91.19     | \u001b[30m210.2     | \u001b[30m86.29     | \u001b[30m55.11     | \u001b[30m0.007709  | \u001b[30m48.51     | \u001b[30m0.2828    |\n",
            "| \u001b[30m45        | \u001b[30m0.9551    | \u001b[30m2.069     | \u001b[30m126.7     | \u001b[30m0.4947    | \u001b[30m123.0     | \u001b[30m226.2     | \u001b[30m45.11     | \u001b[30m52.49     | \u001b[30m0.009406  | \u001b[30m42.03     | \u001b[30m0.7007    |\n",
            "| \u001b[30m46        | \u001b[30m0.9573    | \u001b[30m2.339     | \u001b[30m44.49     | \u001b[30m0.3021    | \u001b[30m223.2     | \u001b[30m146.4     | \u001b[30m36.53     | \u001b[30m16.28     | \u001b[30m0.003337  | \u001b[30m36.65     | \u001b[30m0.7331    |\n",
            "| \u001b[30m47        | \u001b[30m0.9506    | \u001b[30m2.919     | \u001b[30m68.89     | \u001b[30m0.4405    | \u001b[30m183.0     | \u001b[30m102.1     | \u001b[30m105.8     | \u001b[30m31.94     | \u001b[30m0.003473  | \u001b[30m45.11     | \u001b[30m0.3876    |\n",
            "| \u001b[30m48        | \u001b[30m0.964     | \u001b[30m0.4755    | \u001b[30m97.05     | \u001b[30m0.3707    | \u001b[30m137.2     | \u001b[30m250.5     | \u001b[30m66.42     | \u001b[30m23.06     | \u001b[30m0.002596  | \u001b[30m27.17     | \u001b[30m0.08437   |\n",
            "| \u001b[30m49        | \u001b[30m0.9573    | \u001b[30m0.663     | \u001b[30m119.4     | \u001b[30m0.2317    | \u001b[30m143.1     | \u001b[30m105.9     | \u001b[30m77.02     | \u001b[30m37.18     | \u001b[30m0.008685  | \u001b[30m11.51     | \u001b[30m0.9621    |\n",
            "| \u001b[30m50        | \u001b[30m0.9371    | \u001b[30m2.625     | \u001b[30m89.49     | \u001b[30m0.2706    | \u001b[30m121.7     | \u001b[30m119.4     | \u001b[30m66.65     | \u001b[30m39.07     | \u001b[30m0.008183  | \u001b[30m10.75     | \u001b[30m0.08743   |\n",
            "| \u001b[30m51        | \u001b[30m0.9573    | \u001b[30m0.8333    | \u001b[30m53.36     | \u001b[30m0.2615    | \u001b[30m227.1     | \u001b[30m115.3     | \u001b[30m36.33     | \u001b[30m50.36     | \u001b[30m0.009165  | \u001b[30m39.33     | \u001b[30m0.2303    |\n",
            "| \u001b[30m52        | \u001b[30m0.9461    | \u001b[30m3.415     | \u001b[30m42.21     | \u001b[30m0.4085    | \u001b[30m201.2     | \u001b[30m123.8     | \u001b[30m53.93     | \u001b[30m16.58     | \u001b[30m0.004528  | \u001b[30m31.68     | \u001b[30m0.2745    |\n",
            "| \u001b[30m53        | \u001b[30m0.9371    | \u001b[30m1.318     | \u001b[30m60.53     | \u001b[30m0.1055    | \u001b[30m235.1     | \u001b[30m247.2     | \u001b[30m98.68     | \u001b[30m17.68     | \u001b[30m0.001645  | \u001b[30m36.7      | \u001b[30m0.7722    |\n",
            "| \u001b[30m54        | \u001b[30m0.9596    | \u001b[30m4.211     | \u001b[30m94.23     | \u001b[30m0.3496    | \u001b[30m88.65     | \u001b[30m225.6     | \u001b[30m99.63     | \u001b[30m25.47     | \u001b[30m0.001676  | \u001b[30m14.55     | \u001b[30m0.4369    |\n",
            "| \u001b[30m55        | \u001b[30m0.9506    | \u001b[30m5.933     | \u001b[30m106.8     | \u001b[30m0.2258    | \u001b[30m194.0     | \u001b[30m162.9     | \u001b[30m39.93     | \u001b[30m26.44     | \u001b[30m0.007086  | \u001b[30m14.24     | \u001b[30m0.6547    |\n",
            "| \u001b[30m56        | \u001b[30m0.9461    | \u001b[30m5.526     | \u001b[30m70.14     | \u001b[30m0.3323    | \u001b[30m185.4     | \u001b[30m124.7     | \u001b[30m113.3     | \u001b[30m27.72     | \u001b[30m0.007152  | \u001b[30m12.37     | \u001b[30m0.8824    |\n",
            "| \u001b[30m57        | \u001b[30m0.9461    | \u001b[30m3.808     | \u001b[30m68.34     | \u001b[30m0.1732    | \u001b[30m136.3     | \u001b[30m127.9     | \u001b[30m46.26     | \u001b[30m57.6      | \u001b[30m0.0091    | \u001b[30m31.3      | \u001b[30m0.2515    |\n",
            "| \u001b[30m58        | \u001b[30m0.9573    | \u001b[30m4.218     | \u001b[30m88.29     | \u001b[30m0.335     | \u001b[30m82.84     | \u001b[30m99.0      | \u001b[30m45.12     | \u001b[30m20.41     | \u001b[30m0.00793   | \u001b[30m35.98     | \u001b[30m0.2331    |\n",
            "| \u001b[30m59        | \u001b[30m0.9596    | \u001b[30m3.796     | \u001b[30m40.3      | \u001b[30m0.2093    | \u001b[30m114.8     | \u001b[30m150.0     | \u001b[30m108.1     | \u001b[30m41.3      | \u001b[30m0.0009356 | \u001b[30m20.96     | \u001b[30m0.5244    |\n",
            "| \u001b[30m60        | \u001b[30m0.9573    | \u001b[30m3.801     | \u001b[30m109.8     | \u001b[30m0.1164    | \u001b[30m236.5     | \u001b[30m91.22     | \u001b[30m99.82     | \u001b[30m16.09     | \u001b[30m0.002467  | \u001b[30m32.61     | \u001b[30m0.7247    |\n",
            "=================================================================================================================================================\n",
            "Best parameters: {'activation': 'sigmoid', 'batch_size': 97.09737770516348, 'dropout_rate': 0.10060681128805556, 'hidden_dim1': 82.39797822195068, 'hidden_dim2': 160.25022270683652, 'hidden_dim3': 48.3913953499718, 'hidden_dim4': 22.75588405085431, 'learning_rate': 0.007737275760572966, 'num_epochs': 32.688044217288315, 'optimizer_name': 'Adam'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqpfey61FFHR",
        "outputId": "7695eac3-1488-47cc-8f8c-78eb556a38af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'sigmoid',\n",
              " 'batch_size': 97.09737770516348,\n",
              " 'dropout_rate': 0.10060681128805556,\n",
              " 'hidden_dim1': 82.39797822195068,\n",
              " 'hidden_dim2': 160.25022270683652,\n",
              " 'hidden_dim3': 48.3913953499718,\n",
              " 'hidden_dim4': 22.75588405085431,\n",
              " 'learning_rate': 0.007737275760572966,\n",
              " 'num_epochs': 32.688044217288315,\n",
              " 'optimizer_name': 'Adam'}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = map_params(optimizer.max['params'])"
      ],
      "metadata": {
        "id": "0YngaN84O3YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the model with best paramaters across multiple seeds and calculating average test accuracy\n",
        "import random\n",
        "\n",
        "seeds = [42, 101, 2024]\n",
        "accuracies = []\n",
        "\n",
        "for seed in seeds:\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    final_model = SimpleNN(\n",
        "        input_dim = 128,\n",
        "        hidden_dims = [round(best_params['hidden_dim1']), round(best_params['hidden_dim2']), round(best_params['hidden_dim3']), round(best_params['hidden_dim4'])],\n",
        "        output_dim = 5,\n",
        "        dropout_rate = best_params['dropout_rate'],\n",
        "        activation_fn = {\n",
        "            'relu': nn.ReLU(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'tanh': nn.Tanh(),\n",
        "            'leaky_relu': nn.LeakyReLU(0.1),\n",
        "            'elu': nn.ELU(),\n",
        "            'selu': nn.SELU(),\n",
        "            'softplus': nn.Softplus()\n",
        "        }[best_params['activation']]\n",
        "    )\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer_class = {'Adam': optim.Adam, 'SGD': optim.SGD}[best_params['optimizer_name']]\n",
        "    optimizer = optimizer_class(final_model.parameters(), lr=best_params['learning_rate'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=round(best_params['batch_size']), shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=round(best_params['batch_size']), shuffle=False)\n",
        "\n",
        "    for epoch in range(int(best_params['num_epochs'])):\n",
        "        final_model.train()\n",
        "        for sequences, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = final_model(sequences)\n",
        "            loss = loss_fn(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    final_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in test_loader:\n",
        "            predictions = final_model(sequences)\n",
        "            _, predicted_labels = torch.max(predictions, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted_labels == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    accuracies.append(accuracy)\n",
        "    print(f'Seed {seed} - Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "\n",
        "average_accuracy = sum(accuracies) / len(accuracies)\n",
        "print(f'Average Test Accuracy: {average_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4YNVUA8G6_N",
        "outputId": "90391572-cf37-4a76-fb5e-1b903e98d23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 - Test Accuracy: 0.9528\n",
            "Seed 101 - Test Accuracy: 0.9416\n",
            "Seed 2024 - Test Accuracy: 0.9528\n",
            "Average Test Accuracy: 0.9491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model with best parameters"
      ],
      "metadata": {
        "id": "LsQMWfRiK0oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer, best_params, filepath):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_params': best_params\n",
        "    }, filepath)\n",
        "\n",
        "save_path = '/content/drive/My Drive/BBC_News_Articles_Classification_Exploration/final_model.pth'\n",
        "save_model(final_model, optimizer, best_params, save_path)"
      ],
      "metadata": {
        "id": "O_i5j7OhQ0o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model & Testing on some examples"
      ],
      "metadata": {
        "id": "Ft-2gW5_Tk_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    best_params = checkpoint['best_params']\n",
        "\n",
        "    model = SimpleNN(\n",
        "        input_dim=128,\n",
        "        hidden_dims=[round(best_params['hidden_dim1']), round(best_params['hidden_dim2']), round(best_params['hidden_dim3']), round(best_params['hidden_dim4'])],\n",
        "        output_dim=5,\n",
        "        dropout_rate=best_params['dropout_rate'],\n",
        "        activation_fn={\n",
        "            'relu': nn.ReLU(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'tanh': nn.Tanh(),\n",
        "            'leaky_relu': nn.LeakyReLU(0.1),\n",
        "            'elu': nn.ELU(),\n",
        "            'selu': nn.SELU(),\n",
        "            'softplus': nn.Softplus()\n",
        "        }[best_params['activation']]\n",
        "    )\n",
        "\n",
        "    optimizer_class = {'Adam': optim.Adam, 'SGD': optim.SGD}[best_params['optimizer_name']]\n",
        "    optimizer = optimizer_class(model.parameters(), lr=best_params['learning_rate'])\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    return model, optimizer, best_params"
      ],
      "metadata": {
        "id": "895E5pB6SSTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model, loaded_optimizer, loaded_best_params = load_model(save_path)\n",
        "\n",
        "# Example usage of the loaded model for evaluation\n",
        "loaded_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for sequences, labels in test_loader:\n",
        "        predictions = loaded_model(sequences)\n",
        "        _, predicted_labels = torch.max(predictions, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted_labels == labels).sum().item()\n",
        "\n",
        "loaded_accuracy = correct / total\n",
        "print(f'Loaded Model Test Accuracy: {loaded_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thbXsDZLS8w5",
        "outputId": "d7153d11-6603-4654-e167-a13365d2449c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Model Test Accuracy: 0.9528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles = [\n",
        "    \"\"\"How much is Elon Musk, the mercurial multibillionaire, worth to Tesla, the carmaker he runs? In 2018 the company’s board put in place a plan to award Mr Musk shares over ten years worth $46bn, at their current price, provided the business cleared a series of hurdles. In January a Delaware judge struck down the package, calling it “unfathomable”, after a shareholder sued to have it rescinded. The company has asked its investors to reaffirm their support for the award ahead of an annual general meeting on June 13th. Mr Musk’s monster pay package is worth nearly 300 times what America’s best-paid chief executive, Hock Tan of Broadcom, a chipmaker, made last year. It is also equivalent to 8% of Tesla’s current market value—which is down by roughly a fifth over the past year.\"\"\",\n",
        "    \"\"\"macy s owner buys rival for $11bn us retail giant federated department stores is to buy rival may department stores for $11bn (Â£5.7bn).  the deal will bring together famous stores like macy s  bloomingdale s and marshall field s  creating the largest department store chain in the us. the combined firm will operate about 1 000 stores across the us  with combined annual sales of $30bn. the two companies  facing competition\"\"\",\n",
        "    \"\"\"A Scottish woman who allegedly inspired the character Martha in the hit Netflix drama Baby Reindeer is suing the streamer for defamation, negligence and privacy violations. Fiona Harvey, who says Martha is based on her, argued in a lawsuit filed in a California court on Thursday that Netflix told \"brutal lies\" about her to over 50 million viewers around the world. The lawsuit seeks over $170m (£132m) in damages for Ms Harvey, who claims the Baby Reindeer series falsely depicted her as a convicted criminal who spent time in prison for stalking. Netflix did not immediately respond to BBC's request for comment. Ms Harvey also denies that she sexually assaulted the show's creator, according to the court documents, which allege that Netflix “told these lies, and never stopped, because it was a better story than the truth, and better stories made money”. In one scene in the series, the Martha character is depicted as sexually assaulting the show's protagonist along a canal one night. Speaking to BBC News on Thursday, Ms Harvey said she was certain that Netflix would lose the case. \"I have no doubt about that. Otherwise, we wouldn't be doing it. We think we are going to win,\" she said. The first episode of the hit mini-series claims that \"this is a true story\". The show's end credits say that the programme \"is based on real events: however certain characters, names, incidents, locations, and dialogue have been fictionalized for dramatic purposes”. While giving evidence before the Culture Media and Sport Committee in Parliament last month, Netflix executive Benjamin King said the show was \"obviously a true story of the horrific abuse that the writer and protagonist Richard Gadd suffered at the hands of a convicted stalker\". Mr Gadd, a comedian, wrote and stars in the series about his alleged experience of being stalked by a woman he met at the pub where he worked. He is not named as a defendant in Ms Harvey's lawsuit. Neither Mr Gadd nor Ms Harvey's real names are used in the series. On social media, Mr Gadd has previously appealed to fans to refrain from trying to identify Martha, the stalker character he first described in a stand-up comedy routine. Ms Harvey has identified herself as the woman portrayed as Martha in the series. Netflix and Mr Gadd have not confirmed this. Ms Harvey's lawsuit alleges that Netflix \"did literally nothing\" to confirm that Mr Gadd's story was true before creating the series.” “It never investigated whether Harvey was convicted, a very serious misrepresentation of the facts,” the complaint states, referring to the character Martha's prior conviction for stalking. “It did nothing to understand the relationship between Gadd and Harvey, if any. It did nothing to determine whether other facts, including an assault, the alleged stalking or the conviction was accurate.” Richard Roth, a New York-based lawyer representing Ms Harvey, told BBC News on Thursday that he has \"incontrovertible documentary evidence” proving that his client has never been convicted of a crime. The lawsuit includes a photo of a background check and a certificate that claims that Ms Harvey has no criminal convictions on her record. Martha, the Baby Reindeer character, is a convicted stalker who is later arrested after Mr Gadd's character reports her to police. Mr Roth added that there is \"no doubt\" whatsoever Ms Harvey's identity was used for Baby Reindeer's plot. Ms Harvey, who lives in the UK, says that since the series was released in April she has received numerous death threats. The experience has left her \"fearful of leaving her home or checking the news\", the lawsuits says, adding that she has \"become extremely secluded and isolated, in fear of the public, going days without leaving her home\". In a nearly hour-long interview with Piers Morgan last month, Ms Harvey confirmed that she had known Mr Gadd during his time working at a pub in London. But she denied that she had acted like the character Martha, who sends Mr Gadd's character 41,000 emails and leaves 350 hours of voicemail messages in the show. \"None of that's true. I don't think I sent him anything,\" she said. \"No, I think there may have been a couple of emails exchanged, but that was it. Just jokey banter emails.\" The lawsuit does allege, however, that real comments that she made to Mr Gadd - such as a tweet she sent him in 2014 - are used in the show's dialogue.\"\"\",\n",
        "    \"\"\"Narendra Modi is set to be India's prime minister for a third time, a day after humbling election results which saw his majority slashed by a resurgent opposition. Mr Modi was backed to be prime minister again following a meeting with his National Democratic Alliance (NDA) on Wednesday. The 73-year-old had found himself unexpectedly reliant on the NDA's smaller parties to reach a parliamentary majority after his own party fell short of the 272 needed to form the next government. However, the opposition - which won 232 seats to the NDA's 293 - has yet to formally concede. It was holding its own meeting on Wednesday in the capital, Delhi, to discuss next steps. Mr Modi is likely to be sworn in for a record-equalling third term later this week. Mr Modi and his Hindu-nationalist Bharatiya Janata Party (BJP) won 240 seats following the weeks-long, seven-stage election, making them the largest party in the Lok Sabha, India's lower house. But it is a significantly reduced number for the prime minister: in 2019, the BJP won 303 seats, and Mr Modi had said he was aiming for 370 seats this time round. Instead, they have had to rely on NDA partners to secure Mr Modi's third term. According to an NDA release, he was \"unanimously\" chosen as their leader at the meeting at his Delhi residence, adding they were \"committed to serving the poor, women, youth, farmers and exploited, deprived and oppressed citizens of India\". Exactly what concessions its partners may have elicited from the BJP remains to be seen. Ahead of the meeting, there was speculation that demands from more powerful groups may have included ministerial positions in return for their support. It is the first time Mr Modi will have governed in coalition without his party having an outright majority, and it is unclear what the next five years will look like. Nilanajan Mukhopadhyay, who has written a biography of Mr Modi, told AFP news agency it would \"force Modi to take the point of view of others\". \"We shall see more democracy and a healthy parliament,\" he added. \"He will have to be a leader that he has never been; we will have to see a new Modi.\" Meanwhile, the opposition INDIA coalition has been celebrating the results - despite not winning. Congress president Mallikarjun Kharge hailed the \"overwhelming support received by our alliance\" and said voters had sent a message opposing the BJP's \"politics of hate, corruption and deprivation\". \"This is a mandate in defence of the Constitution of India and against price rise, unemployment, and crony capitalism and also to save democracy,\" his statement on social media added. Following the NDA's declaration of victory, the White House congratulated Mr Modi and said the US was hoping to work with India to \"ensure a free and open\" Asia. This year's Indian election was the largest the world has ever seen. More than 600m people took part - or 66% of the country's eligible voters. Nearly a billion people had registered to vote in total – about one in eight of the global population. Voting was staggered over seven rounds between 19 April and 1 June for security and logistical reasons. Much of the election took place in extreme and deadly heat as temperatures in parts of India soared to nearly 50C.\"\"\",\n",
        "    \"\"\"President Joe Biden said in an interview Thursday he would not pardon his son, Hunter Biden, if he’s found guilty of criminal federal gun charges. When asked by interviewer David Muir of ABC if he would rule out pardoning Hunter Biden, the president said, “Yes.” He also affirmed he would accept the outcome of the trial, currently underway in Delaware. The courtroom proceedings have delved into a painful moment for the Biden family, as Hunter was struggling with drug addiction in the aftermath of his brother Beau’s death. First lady Dr. Jill Biden attended the trial before traveling to France to join her husband for D-Day commemoration activities. The White House had said previously that Biden would not pardon his son. “I’ve been very clear; the president is not going to pardon his son,” press secretary Karine Jean-Pierre said in December. The president’s son is accused of illegally purchasing and possessing a gun while abusing or being addicted to drugs, a violation of federal law. He pleaded not guilty to the three charges, though he’s been open about his struggles with alcohol and crack cocaine addiction. The charges were brought by special counsel David Weiss. It’s the first time in US history that the child of a sitting president has been on trial. Biden has previously voiced support for his son and said he was proud of his recovery from addiction. “I am the President, but I am also a Dad. Jill and I love our son, and we are so proud of the man he is today,” the president said in a statement Monday as the trial was getting underway. “Hunter’s resilience in the face of adversity and the strength he has brought to his recovery are inspiring to us. A lot of families have loved ones who have overcome addiction and know what we mean,” he went on. “As the President, I don’t and won’t comment on pending federal cases, but as a Dad, I have boundless love for my son, confidence in him, and respect for his strength. Our family has been through a lot together, and Jill and I are going to continue to be there for Hunter and our family with our love and support.” The trial is underway the week after a New York jury returned 34 guilty counts against former President Donald Trump for falsifying business records in an attempt to cover up hush money payments to a porn actress. Trump has decried the verdict and falsely accused Biden of orchestrating the criminal charges. In the interview Thursday, Biden accused Trump of trying to subvert the rule of law by questioning the verdict in his own criminal trial. “He’s trying to undermine it,” Biden told Muir. “He got a fair trial. The jury spoke.” This story has been updated with additional reporting.\"\"\",\n",
        "    \"\"\"The Federal Trade Commission is investigating a recent Microsoft deal with artificial intelligence startup Inflection, according to a person familiar with the matter, as US antitrust regulators ramp up scrutiny of the red-hot AI industry. Microsoft announced in March that it had hired Inflection’s co-founders and a number of its staff to lead its Copilot program, and Inflection said its AI model would be hosted on Microsoft’s cloud platform. As part of that deal, Microsoft was said to have paid Inflection $650 million. In its announcement at the time, Microsoft described the move as merely a hiring decision, not as an acquisition. The FTC probe into Microsoft concerns whether the company’s investment in Inflection constituted an acquisition that Microsoft failed to disclose to the government, one of the people said. The investigation comes as antitrust officials at the FTC and the Justice Department are nearing a final agreement this week on how to jointly oversee AI giants such as Microsoft, Google, Nvidia, OpenAI and others, two people familiar with the matter told CNN. That agreement, which could be finalized within days, would appoint DOJ as the lead investigator of Nvidia, while the FTC would take responsibility for investigating Microsoft and OpenAI, the people said. DOJ will likely continue its role in overseeing Google, one of the people indicated. Any investigations would focus on whether the companies have used their dominant positions in the AI industry to harm competition through abusive and illegal behavior. The agreement shows enforcers are poised for a broad crackdown on some of the most well-known players in the AI sector, said Sarah Myers West, managing director of the AI Now Institute and a former AI advisor to the FTC. “Clearance processes like this are usually a key step before advancing an investigation,” West said. “This is a clear sign they’re moving quickly here.” Microsoft declined to comment on the DOJ-FTC agreement but, in a statement, defended its partnership with Inflection. “Our agreements with Inflection gave us the opportunity to recruit individuals at Inflection AI and build a team capable of accelerating Microsoft Copilot, while enabling Inflection to continue pursuing its independent business and ambition as an AI studio,” a Microsoft spokesperson said, adding that the company is “confident” it has complied with its reporting obligations. Inflection and Google didn’t immediately respond to a request for comment; Nvidia and OpenAI declined to comment. The DOJ and FTC declined to comment. The FTC-DOJ agreement was earlier reported by The New York Times; the FTC’s Microsoft probe was earlier reported by The Wall Street Journal. FTC Chair Lina Khan has warned in op-eds and congressional testimony that, left unchecked, artificial intelligence could “turbocharge” fraud and scams. The agency has published numerous reminders and warnings that businesses can be held liable for making misleading claims about their AI tools or for covertly using consumer data to train AI models. The FTC is currently investigating Reddit’s AI content licensing practices, and is separately investigating OpenAI for possible violations of consumer protection law. The US agencies’ division-of-labor agreement opens the door to more intensive probes of a sector that has energized investors, enthralled consumers and raised alarm bells among critics who say AI urgently needs regulation to forestall widespread job displacement, discrimination and fraud. Specifically, it carves out roles for the FTC and DOJ to review whether tech giants and AI companies are behaving in anticompetitive ways. And it highlights how enforcers are increasingly trying to bring existing laws to bear on the industry as prospects for new US laws governing AI have dimmed. The United States is widely viewed as a laggard on AI regulation as others such as the European Union have leapfrogged it with tough rules about how the technology can be used in high-risk contexts. The EU AI Act, for example, outlaws social scoring systems powered by AI and any biometric-based tools used to guess a person’s race, political leanings or sexual orientation. It also bans the use of AI to interpret the emotions of people in schools and workplaces, as well as some types of automated profiling intended to predict a person’s likelihood of committing future crimes. For years, technology critics and regulators have worried that major tech companies may be monopolizing entire sectors of the economy. That has led to high-profile US government antitrust suits targeting Amazon, Apple, Google, Meta and Microsoft. Some fear that tech companies could abuse their powerful roles in business and society to extend their dominance into the fast-growing field of generative AI, which exploded onto the scene in 2022 when OpenAI released ChatGPT. Nvidia’s soaring stock prices have served as a barometer of the AI frenzy, underscoring the company’s position as a leading supplier of computing chips necessary for training advanced AI models. On Wednesday, Nvidia became the second-largest publicly traded company in the United States, ending the day with a market capitalization of more than $3 trillion and edging out Apple. “AI relies on massive amounts of data and computing power, which can give already-dominant firms a substantial advantage,” DOJ antitrust chief Jonathan Kanter said last week in a speech at Stanford University, adding that Americans’ reliance on just a handful of technology giants could allow them to “control these new markets.” One way for tech giants to wield anticompetitive influence in the AI sector, critics say, is through exclusive partnerships with AI startups. The agreements can potentially “lock in” AI developers as customers of large cloud computing services and give the tech giants significant stakes or influence over the development of AI. Those types of deals, including Microsoft’s relationship with OpenAI, are the subject of an ongoing study by the FTC announced in January. The DOJ has also become increasingly vocal on AI issues. In 2022, the agency’s antitrust division hired Susan Athey, a Stanford University professor and AI expert, to be its chief economist. In looking at competition in the AI industry, antitrust enforcers should take lessons on how technology giants have behaved anticompetitively in the past, Athey told CNN at a recent event in Washington hosted by Bloomberg News. That could include gatekeeping or bottlenecking tactics, making it harder for consumers or customers to switch providers, or becoming the biggest buyer of key supplies — such as AI chips, for example — and denying those necessary supplies to competing rivals. “We should look back to historical analogues and see where sources of market power have been and how people have preserved them, and those are the kinds of tactics we might worry about going forward,” Athey said. That the DOJ is picking up oversight of Nvidia from the FTC is particularly notable, West told CNN. “It’s possible that means criminal penalties are now on the table, because that’s one of the tools DOJ uniquely carries.\"\"\",\n",
        "    \"\"\"Drones have changed war. Small, cheap, and deadly robots buzz in the skies high above the world’s battlefields, taking pictures and dropping explosives. They’re hard to counter. ZeroMark, a defense startup based in the United States, thinks it has a solution. It wants to turn the rifles of frontline soldiers into “handheld Iron Domes.” The idea is simple: Make it easier to shoot a drone out of the sky with a bullet. The problem is that drones are fast and maneuverable, making them hard for even a skilled marksman to hit. ZeroMark’s system would add aim assistance to existing rifles, ostensibly helping soldiers put a bullet in just the right place. “We’re mostly a software company,” ZeroMark CEO Joel Anderson tells WIRED. He says that the way it works is by placing a sensor on the rail mount at the front of a rifle, the same place you might put a scope. The sensor interacts with an actuator either in the stock or the foregrip of the rifle that makes adjustments to the soldier’s aim while they’re pointing the rifle at a target. A soldier beset by a drone would point their rifle at the target, turn on the system, and let the actuators solidify their aim before pulling the trigger. “So there’s a machine perception, computer vision component. We use lidar and electro-optical sensors to detect drones, classify them, and determine what they’re doing,” Anderson says. “The part that is ballistics is actually quite trivial … It’s numerical regression, it’s ballistic physics.” According to Anderson, ZeroMarks’ system is able to do things a human can’t. “For them to be able to calculate things like the bullet drop and trajectory and windage … It’s a very difficult thing to do for a person, but for a computer, it’s pretty easy,” he says. “And so we predetermined where the shot needs to land so that when they pull the trigger, it’s going to have a high likelihood of intersecting the path of the drone.” ZeroMark makes a tantalizing pitch—one so attractive that venture capital firm Andreesen Horowitz invested $7 million in the project. The reasons why are obvious for anyone paying attention to modern war. Cheap and deadly flying robots define the conflict between Russia and Ukraine. Every month, both sides send thousands of small drones to drop explosives, take pictures, and generate propaganda. With the world’s militaries looking for a way to fight back, counter-drone systems are a growth industry. There are hundreds of solutions, many of them not worth the PowerPoint slide they’re pitched from. Can a machine-learning aim-assist system like what ZeroMark is pitching work? It remains to be seen. According to Anderson, ZeroMark isn’t on the battlefield anywhere, but the company has “partners in Ukraine that are doing evaluations. We’re hoping to change that by the end of the summer.” There’s good reason to be skeptical. “I’d love a demonstration. If it works, show us. Till that happens, there are a lot of question marks around a technology like this,” Arthur Holland Michel, a counter-drone expert and senior fellow at the Carnegie Council for Ethics in International Affairs, tells WIRED. “There’s the question of the inherent unpredictability and brittleness of machine-learning-based systems that are trained on data that is, at best, only a small slice of what the system is likely to encounter in the field.” Anderson says that ZeroMark’s training data is built from “a variety of videos and drone behaviors that have been synthesized into different kinds of data sets and structures. But it’s mostly empirical information that’s coming out of places like Ukraine.” Michel also contends that the physics, which Anderson says are simple, are actually quite hard. ZeroMark’s pitch is that it will help soldiers knock a fast-moving object out of the sky with a bullet. “And that is very difficult,” Michel says. “It’s a very difficult equation. People have been trying to shoot drones out of the sky [for] as long as there have been drones in the sky. And it’s difficult, even when you have a drone that is not trying to avoid small arms fire.” That doesn’t mean ZeroMark doesn’t work—just that it’s good to remain skeptical in the face of bold claims from a new company promising to save lives. “The only truly trustworthy metric of whether a counter-drone system works is if it gets used widely in the field—if militaries don’t just buy three of them, they buy thousands of them,” Michel says. “Until the Pentagon buys 10,000, or 5,000, or even 1,000, it’s hard to say, and a little skepticism is very much merited.”\"\"\",\n",
        "    \"\"\"It sounds like Jennifer Lopez is trying to keep it positive. Following her cancellation last week of her summer concert tour so she could spend more time with her family, Lopez has offered a note of gratitude to her fans. Visitors to her On The Jlo were greeted this week with a note from the singer/actress about the success of her latest film “Atlas,” which is steaming on Netflix. The message begins with “Thank you.” “Hi everybody,” it reads. “I just found out some great news and it’s all because of YOU!!” Lopez shares that “Atlas” recently reached No. 1 on the Netflix top 10 movie titles list and she credited her supporters. “It may seem like there’s a lot of negativity out in the world right now…but don’t let the voices of a few drown out there is soooo much love out there,” her note reads. “Thank you, thank you, thank you!! I love you all so much. Jennifer.” Live Nation and Lopez announced the cancelation of her “THIS IS ME…LIVE” tour last Friday, stating she would “taking time off to be with her children, family and close friends.” “I am completely heartsick and devastated about letting you down. Please know that I wouldn’t do this if I didn’t feel that it was absolutely necessary,” Lopez said at the time. “I promise I will make it up to you and we will all be together again. I love you all so much. Until next time.” The news came amid recent reports Lopez and her husband Ben Affleck have been living apart. CNN has reached out to representatives for both stars for comment. The couple married in July 2022. Lopez’s tour was scheduled to kickoff June 26 in Orlando.\"\"\",\n",
        "    \"\"\"Slim Shady is back, back again. But, apparently, not for long. Eminem appears to be killing off his alter ego in his latest project, an album titled, “The Death of Slim Shady (Coup de Grace).” He has dropped the album’s first single, “Houdini,” which samples the Steve Miller Band’s 1982 hit, “Abracadabra.” His lyrics take aim at several people, including Megan Thee Stallion and even himself. “If I was to ask for Megan Thee Stallion if she would collab with me, would I really have a shot at a feat?” he raps, referencing Tory Lanez shooting Megan Thee Stallion in the feet in 2020, a crime for which Lanez was sentenced to 10 years in prison. Eminem also released a music video for the song in which he gets a little help from some of his celebrity friends. It opens with the 51-year-old rapper’s music manager, Paul Rosenberg, leaving a seemingly less than positive voicemail about the new album before Eminem’s mentor and frequent collaborator, Dr. Dre, tells him, “We’ve got a problem.” The issue being a portal that allows time travel between the present and 2002. Cameos from 50 Cent, Snoop Dogg and Pete Davidson follow. Eminem battles it out with his younger-self in the video, which will remind fans of his 2002 “Without Me” video. Recently a faux obituary appeared for his Slim Shady alter ego in his hometown newspaper, the Detroit Free Press. It referred to Slim Shady a “a rogue splinter in the flourishing underground rap scene.” “The Death of Slim Shady (Coup de Grâce)” follows Eminem’s 2020 album, “Music to Be Murdered By.” The full collection of new music is set for release this summer.\"\"\",\n",
        "    \"\"\"Lenny Kravitz’s leather pants continue to stay on. In a recently published interview with The Guardian, Kravitz opened about discovering that his father was having an affair, a story recounted in his 2020 memoir. His father, he said, told him he too would end up cheating on his spouse. “He became right. After the marriage [to actress Lisa Bonet], I became more like him,” Kravitz told The Guardian. “I was becoming a player.” The Grammy-winning singer and guitarist ended up divorcing Bonet in 1993 and discussed how he felt about his cheating ways. “I didn’t like it. I didn’t want to be that guy,” he said. “So I had to tackle that and it took years.” That meant “taking responsibility,” he said, adding he had to have “discipline” and not let “my own desires take over.” Kravitz said he’s not been in a serious relationship in nine years. The reporter then asked if Kravitz was being genuine when he stated previously that he wanted to be celibate until he found the right person. Kravitz confirmed his stance. “Yes,” he said. “It’s a spiritual thing.” CNN has reached out to a representative for the star for comment. Kravitz had previously opened up in interviews about his decision to stay celibate, saying in an interview with CBS Sunday Morning in 2008 that he believed that “in the end, that’s going to help me to find the right person.” “So I’m not going to waver on that. That’s a promise I made to God three years ago,” he said at the time. Kravitz has been linked to stars like Nicole Kidman, Adriana Lima and Kylie Minogue in the past. He now says, however, “I have become very set in my ways, in the way I live.” That sound you are hearing is the many volunteers asking Kravitz, “Are you going go my way?”\"\"\",\n",
        "    \"\"\"Kristaps Porzingis didn’t want to make predictions about how his body would respond heading into the NBA Finals after he spent more than a month on the sideline with a calf injury. Just fine, it turned out. Jaylen Brown scored 22 points, Porzingis made an immediate impact off the bench and added 20 and the Boston Celtics powered past the Dallas Mavericks 107-89 on Thursday night in Game 1. Derrick White finished with 15 points for Boston, which led by 29 points in the first half and connected on 16 3-pointers in a powerful start to its quest for an 18th NBA title. Porzingis, a 7-footer who had been sidelined since April 29, added six rebounds and three blocks in 21 minutes. “Tonight was affirmation to myself that I’m pretty good,” Porzingis said. “I’m not perfect but I can play like this and I can add to this team.” The last Celtics player to enter the court for pregame warmups, he said he received a jolt of energy from a home crowd, which erupted when he emerged from the tunnel. “The adrenaline was pumping through my veins,” Porzingis said. Celtics coach Joe Mazzulla wasn’t concerned about the layoff affecting Porzingis’ aggressiveness.\"\"\",\n",
        "    \"\"\"FORT WORTH, Texas — What do you get for the nine-time national all-around champion who has everything? An engraved silver belt buckle, apparently. Simone Biles earned the Texas token as a trophy Sunday night after clinching the top spot at the U.S. Gymnastics Championships with a total score of 119.750 (60.450 on Day 1, 59.300 on Day 2). She also swept every individual event title. Over two nights, the 27-year-old’s biggest mistake was over-rotating her Yurchenko double pike vault and landing on her back. Even with the fall, she still earned a 15.000 because the vault is so difficult and Biles executed it with little flaw aside from the landing. She began on the balance beam with a solid routine for a 14.800 and added a 15.100 on floor exercise. Though she had a bit too much juice on her triple-twisting double back tuck and bounced out of bounds, she incurred only minor landing deductions for her other three über difficult tumbling passes. Biles capped her winning all-around performance on the uneven bars, coasting through in her typical speedy fashion to score a 14.400. Amidst her historic night, Biles found time to boost up a fellow Olympic all-around champion after Suni Lee opened the night with a fall on vault. Lee competed a double-twisting vault Friday and warmed it up Sunday but didn’t get enough height off the vault table to complete two twists due to her hand slipping when performing the vault in competition. She managed 1 1/2 twists and sat the vault down, similar to the incomplete vault Biles did during team finals in Tokyo when struggling with the twisties. Lee wasn’t injured but stepped off the competition floor to gather herself after the fluke vault, and Biles found her to offer some support. “After Suni vaulted, I knew exactly what was going through her head. I dealt with that in Tokyo so I just knew that she needed some encouragement and somebody to trust her gymnastics for her and to believe in her, so that’s exactly what I did,” Biles said, adding that Lee asked her to stand by the uneven bars during her next routine after vault. Lee righted the ship with a 14.500 on bars for a routine that does not yet include the full difficulty she plans to show at Olympic Trials and landed in fourth in her first all-around competition back since 2021. Following the meet, she credited Biles for helping her stay grounded after the vault fall. “She’s been one of my biggest inspirations for a long time. I know that we’re kinda teammates and competitors, but she’s somebody that I look up to so to hear those words coming from her means a lot,” Lee said. Skye Blakely held on to her second-place position with another strong showing, highlighted by a 14.450 on bars. The 19-year-old has struggled with consistency issues when competing internationally, but with the debut of an upgraded vault and eight hit routines over both nights of competition, she strengthened her case for making the Paris Olympic team. Tokyo silver medalist Jordan Chiles climbed to fifth in the all-around standings behind a big 14.100 floor routine while 2020 floor Olympic gold medalist Jade Carey finished seventh. Both recorded falls on beam, but they weren’t alone. The four-inch event gave multiple gymnasts trouble Sunday, as Paris contender Leanne Wong also fell. Sixteen gymnasts earned an invite to Olympic Trials, slated for June 27-30 in Minneapolis, where they will compete for a spot on the five-person squad headed to Paris. Among the field are Shilese Jones and Kaliya Lincoln, who withdrew from championships but successfully petitioned to Trials. In the men’s competition, Brody Malone won his third national all-around title and took first on high bar Saturday night in his first competition back after a devastating knee injury in March 2023 that required three surgeries. The 24-year-old, who had to relearn how to walk after suffering a tibial plateau fracture, a partially torn PCL and a fully torn LCL, is on track to make his second Olympic team this summer alongside Michigan’s Frederick Richard and Stanford’s Khoi Young. The two 2023 World Championship medalists finished second and third, respectively, in the men’s all-around.\"\"\",\n",
        "    \"\"\"Islam Makhachev made a third successful defense of his UFC lightweight championship on Saturday night, and his reward in the pound-for-pound rankings is ... nothing. Makhachev (26-1), who submitted Dustin Poirier in Round 5 of their UFC 302 main event, was already No. 1 in the ESPN rankings. There was nowhere for him to rise. However, Makhachev surely solidified his position at the top of the sport in the eyes of our voters and many fans, although not the person whose job it is to promote him. \"I don't think he's the pound-for-pound best fighter in the world,\" UFC CEO Dana White said after Saturday's fights. \"For anyone to call Islam the pound-for-pound best fighter in the world when Jon Jones is still f---ing fighting is nuts and shouldn't be ranking in the pound-for-pound or doing any of the f---ing rankings ever, if that's what you really think.\" One can argue for Jones, for sure, but there's a strong case to be made for Makhachev. Unlike the GOAT discussion, rankings are not a lifetime achievement award. Jones has fought only once in over four years. In that time, Makhachev is 8-0 with seven finishes. And when it comes to ESPN's rankings, our eligibility rules require a fighter to have competed in the past year or have a fight booked. Jones last fought on March 4, 2023 -- exactly 15 months ago, after a three-year absence -- and has no upcoming fight scheduled. He is ineligible. (Interestingly, Jones does appear in the UFC's official rankings -- at No. 2, behind Makhachev.) Eligibility rules account for the one (small) change in these ESPN rankings. One of our voters had Demetrious Johnson in his top 10 the last time we published rankings, but \"Mighty Mouse\" is now ineligible. Removing Johnson, whose last fight was 13 months ago, bumped up Israel Adesanya one spot on that voter's ballot, putting Adesanya in an overall tie with his teammate, Alexander Volkanovski.\"\"\"\n",
        "]\n",
        "\n",
        "actual_classes = ['business', 'business', 'entertainment', 'politics', 'politics', 'tech', 'tech', 'entertainment', 'entertainment', 'entertainment', 'sports', 'sports', 'sports']\n",
        "\n",
        "class_labels = ['business', 'entertainment', 'politics', 'sports', 'tech']\n",
        "\n",
        "all_predictions = []\n",
        "\n",
        "for article in articles:\n",
        "    embedding = embedding_model([article])\n",
        "    article_tensor = torch.tensor(embedding.numpy(), dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        output = loaded_model(article_tensor)\n",
        "    probabilities = F.softmax(output, dim=1)\n",
        "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "    all_predictions.append(predicted_class)\n",
        "\n",
        "\n",
        "for i, prediction in enumerate(all_predictions):\n",
        "    predicted_label = class_labels[prediction]\n",
        "    actual_label = actual_classes[i]\n",
        "    print(f\"Article {i+1} Prediction: {predicted_label}, Actual: {actual_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCeDiCLETD0D",
        "outputId": "738a5e57-2668-496f-9c4b-ae55e5f8ac64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1 Prediction: business, Actual: business\n",
            "Article 2 Prediction: business, Actual: business\n",
            "Article 3 Prediction: entertainment, Actual: entertainment\n",
            "Article 4 Prediction: politics, Actual: politics\n",
            "Article 5 Prediction: politics, Actual: politics\n",
            "Article 6 Prediction: tech, Actual: tech\n",
            "Article 7 Prediction: tech, Actual: tech\n",
            "Article 8 Prediction: entertainment, Actual: entertainment\n",
            "Article 9 Prediction: entertainment, Actual: entertainment\n",
            "Article 10 Prediction: entertainment, Actual: entertainment\n",
            "Article 11 Prediction: sports, Actual: sports\n",
            "Article 12 Prediction: sports, Actual: sports\n",
            "Article 13 Prediction: sports, Actual: sports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "all correct predictions ;)"
      ],
      "metadata": {
        "id": "Ck9y4HArygGd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vfJlLPvrypDz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}